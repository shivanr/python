{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.utils\n",
    "\n",
    "class LR(object):\n",
    "\n",
    "    def __init__(self, penalty='l2', C=100, tol=0.01, class_weight=None, max_iter=100):\n",
    "        \"\"\" The Invariants Mining model for anomaly detection\n",
    "        Attributes\n",
    "        ----------\n",
    "            classifier: object, the classifier for anomaly detection\n",
    "        \"\"\"\n",
    "        self.classifier = LogisticRegression(penalty=penalty, C=C, tol=tol, class_weight=class_weight,\n",
    "                                             max_iter=max_iter)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: ndarray, the event count matrix of shape num_instances-by-num_events\n",
    "        \"\"\"\n",
    "        print('====== Model summary ======')\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict anomalies with mined invariants\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: the input event count matrix\n",
    "        Returns\n",
    "        -------\n",
    "            y_pred: ndarray, the predicted label vector of shape (num_instances,)\n",
    "        \"\"\"\n",
    "        y_pred = self.classifier.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import sklearn.utils \n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, penalty='l1', tol=0.1, C=1, dual=False, class_weight=None, \n",
    "                 max_iter=100):\n",
    "        \"\"\" The Invariants Mining model for anomaly detection\n",
    "        Arguments\n",
    "        ---------\n",
    "        See SVM API: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "            classifier: object, the classifier for anomaly detection\n",
    "        \"\"\"\n",
    "        self.classifier = svm.LinearSVC(penalty=penalty, tol=tol, C=C, dual=dual, \n",
    "                                        class_weight=class_weight, max_iter=max_iter)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: ndarray, the event count matrix of shape num_instances-by-num_events\n",
    "        \"\"\"\n",
    "        print('====== Model summary ======')\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict anomalies with mined invariants\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: the input event count matrix\n",
    "        Returns\n",
    "        -------\n",
    "            y_pred: ndarray, the predicted label vector of shape (num_instances,)\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.classifier.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.utils\n",
    "\n",
    "class PCA(object):\n",
    "\n",
    "    def __init__(self, n_components=0.95, threshold=None, c_alpha=3.2905):\n",
    "        \"\"\" The PCA model for anomaly detection\n",
    "        Attributes\n",
    "        ----------\n",
    "            proj_C: The projection matrix for projecting feature vector to abnormal space\n",
    "            n_components: float/int, number of principal compnents or the variance ratio they cover\n",
    "            threshold: float, the anomaly detection threshold. When setting to None, the threshold \n",
    "                is automatically caculated using Q-statistics\n",
    "            c_alpha: float, the c_alpha parameter for caculating anomaly detection threshold using \n",
    "                Q-statistics. The following is lookup table for c_alpha:\n",
    "                c_alpha = 1.7507; # alpha = 0.08\n",
    "                c_alpha = 1.9600; # alpha = 0.05\n",
    "                c_alpha = 2.5758; # alpha = 0.01\n",
    "                c_alpha = 2.807; # alpha = 0.005\n",
    "                c_alpha = 2.9677;  # alpha = 0.003\n",
    "                c_alpha = 3.2905;  # alpha = 0.001\n",
    "                c_alpha = 3.4808;  # alpha = 0.0005\n",
    "                c_alpha = 3.8906;  # alpha = 0.0001\n",
    "                c_alpha = 4.4172;  # alpha = 0.00001\n",
    "        \"\"\"\n",
    "\n",
    "        self.proj_C = None\n",
    "        self.components = None\n",
    "        self.n_components = n_components\n",
    "        self.threshold = threshold\n",
    "        self.c_alpha = c_alpha\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Auguments\n",
    "        ---------\n",
    "            X: ndarray, the event count matrix of shape num_instances-by-num_events\n",
    "        \"\"\"\n",
    "\n",
    "        print('====== Model summary ======')\n",
    "        num_instances, num_events = X.shape\n",
    "        X_cov = np.dot(X.T, X) / float(num_instances)\n",
    "        U, sigma, V = np.linalg.svd(X_cov)\n",
    "        n_components = self.n_components\n",
    "        if n_components < 1:\n",
    "            total_variance = np.sum(sigma)\n",
    "            variance = 0\n",
    "            for i in range(num_events):\n",
    "                variance += sigma[i]\n",
    "                if variance / total_variance >= n_components:\n",
    "                    break\n",
    "            n_components = i + 1\n",
    "\n",
    "        P = U[:, :n_components]\n",
    "        I = np.identity(num_events, int)\n",
    "        self.components = P\n",
    "        self.proj_C = I - np.dot(P, P.T)\n",
    "        print('n_components: {}'.format(n_components))\n",
    "        print('Project matrix shape: {}-by-{}'.format(self.proj_C.shape[0], self.proj_C.shape[1]))\n",
    "\n",
    "        if not self.threshold:\n",
    "            # Calculate threshold using Q-statistic. Information can be found at:\n",
    "            # http://conferences.sigcomm.org/sigcomm/2004/papers/p405-lakhina111.pdf\n",
    "            phi = np.zeros(3)\n",
    "            for i in range(3):\n",
    "                for j in range(n_components, num_events):\n",
    "                    phi[i] += np.power(sigma[j], i + 1)\n",
    "            h0 = 1.0 - 2 * phi[0] * phi[2] / (3.0 * phi[1] * phi[1])\n",
    "            self.threshold = phi[0] * np.power(self.c_alpha * np.sqrt(2 * phi[1] * h0 * h0) / phi[0]\n",
    "                                               + 1.0 + phi[1] * h0 * (h0 - 1) / (phi[0] * phi[0]), \n",
    "                                               1.0 / h0)\n",
    "        print('SPE threshold: {}\\n'.format(self.threshold))\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert self.proj_C is not None, 'PCA model needs to be trained before prediction.'\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y_a = np.dot(self.proj_C, X[i, :])\n",
    "            SPE = np.dot(y_a, y_a)\n",
    "            if SPE > self.threshold:\n",
    "                y_pred[i] = 1\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The implementation of the decision tree model for anomaly detection.\n",
    "Authors: \n",
    "    LogPAI Team\n",
    "Reference: \n",
    "    [1] Mike Chen, Alice X. Zheng, Jim Lloyd, Michael I. Jordan, Eric Brewer. \n",
    "        Failure Diagnosis Using Decision Trees. IEEE International Conference \n",
    "        on Autonomic Computing (ICAC), 2004.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import sklearn.utils\n",
    "\n",
    "class DecisionTree(object):\n",
    "\n",
    "    def __init__(self, criterion='gini', max_depth=None, max_features=None, class_weight=None):\n",
    "        \"\"\" The Invariants Mining model for anomaly detection\n",
    "        Arguments\n",
    "        ---------\n",
    "        See DecisionTreeClassifier API: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "        Attributes\n",
    "        ----------\n",
    "            classifier: object, the classifier for anomaly detection\n",
    "        \"\"\"\n",
    "        self.classifier = tree.DecisionTreeClassifier(criterion=criterion, max_depth=max_depth,\n",
    "                          max_features=max_features, class_weight=class_weight)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: ndarray, the event count matrix of shape num_instances-by-num_events\n",
    "        \"\"\"\n",
    "        print('====== Model summary ======')\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict anomalies with mined invariants\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: the input event count matrix\n",
    "        Returns\n",
    "        -------\n",
    "            y_pred: ndarray, the predicted label vector of shape (num_instances,)\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.classifier.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['actual'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features= features.drop('actual', axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "Training Features Shape: (261, 14)\n",
    "Training Labels Shape: (261,)\n",
    "Testing Features Shape: (87, 14)\n",
    "Testing Labels Shape: (87,)\n",
    "    \n",
    "    \n",
    "# The baseline predictions are the historical averages\n",
    "baseline_preds = test_features[:, feature_list.index('average')]\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - test_labels)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2))\n",
    "Average baseline error:  5.06 degrees.\n",
    "    \n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "Mean Absolute Error: 3.83 degrees.\n",
    "    \n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "Accuracy: 93.99 %.\n",
    "    \n",
    "    \n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')\n",
    "\n",
    "\n",
    "# Limit depth of tree to 3 levels\n",
    "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "rf_small.fit(train_features, train_labels)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "graph.write_png('small_tree.png');\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "\n",
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "# Extract the two most important features\n",
    "important_indices = [feature_list.index('temp_1'), feature_list.index('average')]\n",
    "train_important = train_features[:, important_indices]\n",
    "test_important = test_features[:, important_indices]\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_labels)\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - test_labels)\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / test_labels))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "Mean Absolute Error: 3.9 degrees.\n",
    "Accuracy: 93.8 %\n",
    "    \n",
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n",
    "\n",
    "\n",
    "# Use datetime for creating date objects for plotting\n",
    "import datetime\n",
    "# Dates of training values\n",
    "months = features[:, feature_list.index('month')]\n",
    "days = features[:, feature_list.index('day')]\n",
    "years = features[:, feature_list.index('year')]\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})\n",
    "# Dates of predictions\n",
    "months = test_features[:, feature_list.index('month')]\n",
    "days = test_features[:, feature_list.index('day')]\n",
    "years = test_features[:, feature_list.index('year')]\n",
    "# Column of dates\n",
    "test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "# Convert to datetime objects\n",
    "test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n",
    "# Dataframe with predictions and dates\n",
    "predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})\n",
    "# Plot the actual values\n",
    "plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')\n",
    "# Plot the predicted values\n",
    "plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
    "plt.xticks(rotation = '60'); \n",
    "plt.legend()\n",
    "# Graph labels\n",
    "plt.xlabel('Date'); plt.ylabel('Maximum Temperature (F)'); plt.title('Actual and Predicted Values');\n",
    "\n",
    "\n",
    "\n",
    "# Make the data accessible for plotting\n",
    "true_data['temp_1'] = features[:, feature_list.index('temp_1')]\n",
    "true_data['average'] = features[:, feature_list.index('average')]\n",
    "true_data['friend'] = features[:, feature_list.index('friend')]\n",
    "# Plot all the data as lines\n",
    "plt.plot(true_data['date'], true_data['actual'], 'b-', label  = 'actual', alpha = 1.0)\n",
    "plt.plot(true_data['date'], true_data['temp_1'], 'y-', label  = 'temp_1', alpha = 1.0)\n",
    "plt.plot(true_data['date'], true_data['average'], 'k-', label = 'average', alpha = 0.8)\n",
    "plt.plot(true_data['date'], true_data['friend'], 'r-', label = 'friend', alpha = 0.3)\n",
    "# Formatting plot\n",
    "plt.legend(); plt.xticks(rotation = '60');\n",
    "# Lables and title\n",
    "plt.xlabel('Date'); plt.ylabel('Maximum Temperature (F)'); plt.title('Actual Max Temp and Variables');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Processed Yarn Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedfile=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\process_logs\\application_1580556634479_40389_processed.csv'\n",
    "header_list = [\"level\", \"message\", \"lable\"]\n",
    "df=pd.read_csv(processedfile,delimiter=\"|\",names=header_list)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodupdf=df.drop_duplicates(subset={\"level\",\"message\",\"lable\"}, keep='first', inplace=False)\n",
    "nodupdf.shape\n",
    "errordf=nodupdf[nodupdf['level']=='error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>message</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>error</td>\n",
       "      <td>importprocessstep sqoop import  abort!  custo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     level                                            message  lable\n",
       "123  error   importprocessstep sqoop import  abort!  custo...    NaN"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodupErrorDf=errordf.drop_duplicates(subset={\"message\"}, keep='first', inplace=False)\n",
    "#nodupErrorDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in nodupErrorDf[\"message\"]:\n",
    "#    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomDbfile=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\process_logs\\rdb.csv'\n",
    "header_list = [\"message\", \"solution\"]\n",
    "rdbDF=pd.read_csv(recomDbfile,delimiter=\"|\",names=header_list)\n",
    "rdbDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()\n",
    "#tfidf = vectorizer.fit_transform(nodupErrorDf.iloc[0],rdbDF.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf1 = vectorizer.fit_transform(nodupErrorDf.iloc[0],rdbDF.iloc[])\n",
    "#((tfidf1 * tfidf1.T).A)[0,1]\n",
    "#tfidf1=vectorizer.fit_transform(['applicationmaster user class threw exception invocationtargetexception caused by numberformatexception for input string'],['applicationmaster user class threw exception invocationtargetexception caused by numberformatexception for input string'])\n",
    "#((tfidf1 * tfidf1.T).A)[0,1]\n",
    "#(tfidf1 * tfidf1.T).A\n",
    "#dir(tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf_scores = []\n",
    "#score = cosine_similarity(errordf.iloc[0],rdbDF.iloc[0])\n",
    "#Tfidf_scores.append(score)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " ' Inserting string in place of number.Input file data is in wrong format.Please correct and reprocess',\n",
       " 'applicationmaster user class threw exception invocationtargetexception caused by numberformatexception for input string')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from Levenshtein import ratio\n",
    "def getApproximateAnswer2():\n",
    "    max_score = 0\n",
    "    answer = \"\"\n",
    "    prediction = \"\"\n",
    "    recomDbfile=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\process_logs\\rdb.csv'\n",
    "    header_list = [\"message\", \"solution\"]\n",
    "    rdbDF=pd.read_csv(recomDbfile,delimiter=\"|\",names=header_list)\n",
    "    for i,q in rdbDF.iterrows():\n",
    "        score = ratio(str(nodupErrorDf.iloc[0][\"message\"]),str(q[\"message\"]))\n",
    "        if score >= 0.9: # I'm sure, stop here\n",
    "            return score,q[\"solution\"], q[\"message\"]\n",
    "        elif score > max_score: # I'm unsure, continue\n",
    "            max_score = score\n",
    "            answer = q[\"solution\"]\n",
    "            prediction = q[\"solution\"]\n",
    "        if max_score > 0.3: # threshold is lowered\n",
    "            return answer, max_score, prediction\n",
    "        return \"Sorry, I didn't get you.\", max_score, prediction\n",
    "\n",
    "ans=getApproximateAnswer2()\n",
    "ans'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(binary=False,\n",
    "                     max_df=0.95, \n",
    "                     min_df=0.15,\n",
    "                     ngram_range = (1,10),use_idf = False, norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 41)\n",
      "['applicationmaster', 'applicationmaster user', 'applicationmaster user class', 'applicationmaster user class threw', 'applicationmaster user class threw exception', 'applicationmaster user class threw exception invocationtargetexception', 'applicationmaster user class threw exception invocationtargetexception caused', 'applicationmaster user class threw exception invocationtargetexception caused by', 'by', 'caused', 'caused by', 'class', 'class threw', 'class threw exception', 'class threw exception invocationtargetexception', 'class threw exception invocationtargetexception caused', 'class threw exception invocationtargetexception caused by', 'entity', 'error', 'exception', 'exception invocationtargetexception', 'exception invocationtargetexception caused', 'exception invocationtargetexception caused by', 'for', 'invocationtargetexception', 'invocationtargetexception caused', 'invocationtargetexception caused by', 'not', 'the', 'threw', 'threw exception', 'threw exception invocationtargetexception', 'threw exception invocationtargetexception caused', 'threw exception invocationtargetexception caused by', 'user', 'user class', 'user class threw', 'user class threw exception', 'user class threw exception invocationtargetexception', 'user class threw exception invocationtargetexception caused', 'user class threw exception invocationtargetexception caused by']\n"
     ]
    }
   ],
   "source": [
    "rdbMessage_vectors = vectorizer.fit_transform(rdbDF['message'])\n",
    "#rdbMessage_vectors = vectorizer.fit_transform(rdbDF['message'])\n",
    "print(rdbMessage_vectors.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=[]\n",
    "q_vector = vectorizer.transform([errordf.iloc[0][\"message\"]])\n",
    "results.append(cosine_similarity(q_vector, rdbMessage_vectors.toarray()))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_score = [item for sublist in results for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flat_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Similarity Found\n"
     ]
    }
   ],
   "source": [
    "is_all_zero = np.all((np.array(flat_score) == 0))\n",
    "if is_all_zero:\n",
    "    print(\"No Similarity Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.popen('cp '+ processedfile +' '+ source.txt destination.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedfile=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\process_logs\\1application_1568810042014_190726_processed.csv'\n",
    "header_list = [\"level\", \"message\", \"lable\"]\n",
    "df=pd.read_csv(processedfile,delimiter=\"|\",names=header_list)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodupdf=df.drop_duplicates(subset={\"level\",\"message\",\"lable\"}, keep='first', inplace=False)\n",
    "nodupdf.shape\n",
    "errordf=nodupdf[nodupdf['level']=='error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sreddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "\n",
    "\n",
    "dog=wn.synsets('dog', pos=wn.NOUN)[0] #get the first noun synonym of the word \"dog\"\n",
    "\n",
    "cat=wn.synsets('cat', pos=wn.NOUN)[0]\n",
    "\n",
    "rose=wn.synsets('rose', pos=wn.NOUN)[0]\n",
    "\n",
    "flower=wn.synsets('flower', pos=wn.NOUN)[0]\n",
    "\n",
    "\n",
    "\n",
    "#brown_ic = wordnet_ic.ic('ic-brown.dat') #load the brown corpus to compute the IC\n",
    "\n",
    "\n",
    "\n",
    "#rose.res_similarity(flower, brown_ic)\n",
    "\n",
    "#rose.res_similarity(dog, brown_ic)\n",
    "\n",
    "#cat.res_similarity(dog, brown_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\sreddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.911666509036577"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rose.res_similarity(flower, brown_ic)\n",
    "rose.res_similarity(dog, brown_ic)\n",
    "cat.res_similarity(dog, brown_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(sentence):\n",
    "    ignore = ['a', \"the\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))    \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(allsentences):\n",
    "    vocab = tokenize(allsentences)\n",
    "    print(\"Word List for Document \\n{0} \\n\".format(vocab));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsentences = [\"Joe waited for the train train\", \"The train was late\", \"Mary and Samantha took the bus\",\"I looked for Mary and Samantha at the bus station\",\"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe waited for the train train\n",
      "[0. 1. 0. 0.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[0. 1. 0. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 0. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "Joe waited for the train train\n",
      "[1. 1. 2. 1.]\n",
      "\n",
      "The train was late\n",
      "[0. 0. 1. 0.]\n",
      "\n",
      "I looked for Mary and Samantha at the bus station\n",
      "[1. 0. 0. 0.]\n",
      "\n",
      "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
      "[0. 0. 0. 1.]\n",
      "\n",
      "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
      "[1. 0. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "vocab = tokenize(allsentences)\n",
    "for sentence in allsentences:\n",
    "    words = word_extraction(sentence)\n",
    "    bag_vector = numpy.zeros(len(vocab))\n",
    "    for w in words:\n",
    "        for i,word in enumerate(vocab):\n",
    "            if word == w:\n",
    "                bag_vector[i] += 1\n",
    "                print(\"{0}\\n{1}\\n\".format(sentence,numpy.array(bag_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-84aeefeb6303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# execute the text here as :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# text = \"\"\" # place text here  \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sreddy/nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sreddy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-3cb0db802681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword2count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installed\\anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installed\\anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installed\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installed\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installed\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sreddy/nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\installed\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sreddy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model \n",
    "#dataset=\"We declare a dictionary to hold our bag of words\"\n",
    "\n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sreddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')  \n",
    "raw_html = raw_html.read()\n",
    "\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:  \n",
    "    article_text += para.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the 2010s representation learning and deep neural network style machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques 4 5 can achieve state of the art results in many natural language tasks for example in language modeling 6 parsing 7 8 and many others \n"
     ]
    }
   ],
   "source": [
    "print(corpus[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Recimendation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "processedFilePth = r\"C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\data\\processted_logs\"\n",
    "processedFileName = \"application_1568810042014_190726.csv\"\n",
    "\n",
    "fullprocessedFileName=os.path.join(processedFilePth,processedFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_logfile(fullLogFileName):\n",
    "#    filePath=filePath\n",
    "#    fileName=fileName\n",
    "#    fullLogFileName=os.path.join(filePath,fileName)\n",
    "    fullLogFileName=fullLogFileName\n",
    "    header_list = [\"level\", \"message\"]\n",
    "    log_df=pd.read_csv(fullLogFileName,delimiter=\"|\")\n",
    "    return log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupError(inputDf):\n",
    "    nodupdf=inputDf.drop_duplicates(subset={0,1}, keep='first', inplace=False)\n",
    "    errordf=nodupdf[nodupdf[1]=='error']\n",
    "    return errordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fullprocessedFileName=os.path.join(processedFilePth,processedFileName)\n",
    "\n",
    "logDf=read_logfile(fullprocessedFileName)\n",
    "kbDf=read_kb(DBFullPath)\n",
    "#dedupLogDf=dedupError(logDf)    \n",
    "vectorizer = TfidfVectorizer(binary=False,max_df=0.95,min_df=0.15,\n",
    "                             ngram_range = (1,10),use_idf = False, norm = None)\n",
    "\n",
    "kbVecDf = vectorizer.fit_transform(kbDf['message'])\n",
    "#logVecDf = vectorizer.transform([dedupLogDf.iloc[0][\"message\"]])    \n",
    "#similarityMatrix=findSimilarity(logVecDf,kbVecDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info                                                                                                                           info\n",
       " coarsegrainedexecutorbackend: registered signal handlers for [term, hup, int]     securitymanager: securitymanager: authenticat...\n",
       "                                                                                                                                   \n",
       " .1                                                                                                                                \n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dedupLogDf=dedupError(logDf)\n",
    "#nodupdf=logDf.drop_duplicates(subset={0,1}, keep='first', inplace=False)\n",
    "logDf.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from processYarnLog import preprocessLog\n",
    "from readYarnLog import read_yarnlog\n",
    "from recomendationEngin1 import read_logfile,read_kb,dedupError,findSimilarity,isSimilarityFound,recomendSolution\n",
    "DBFullPath=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\process_logs\\rdb.csv'\n",
    "toBeAnalyzed=r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\tobeanalyzed'\n",
    "logDf=read_logfile(fullprocessedFileName)\n",
    "kbDf=read_kb(DBFullPath)\n",
    "dedupLogDf=dedupError(logDf)    \n",
    "vectorizer = TfidfVectorizer(binary=False,max_df=0.95,min_df=0.15,ngram_range = (1,10),use_idf = False, norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbVecDf = vectorizer.fit_transform(kbDf['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fullLogFileName=fullprocessedFileName\n",
    "header_list = [\"level\", \"message\"]\n",
    "log_df=pd.read_csv(fullprocessedFileName,delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.columns =[\"level\", \"message\",\"d1\",\"d2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>info</td>\n",
       "      <td>slf0jlogger: slf0jlogger started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: starting remoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: remoting started; listening on addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'sparkexe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore started with capacit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: connecting to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: successfully re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>info</td>\n",
       "      <td>executor: starting executor id 0 on host awde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'org.apac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>info</td>\n",
       "      <td>nettyblocktransferservice: server created on 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: trying to register blockm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: registered blockmanager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver commande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore cleared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanager: blockmanager stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from awd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from TOK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>info</td>\n",
       "      <td>remoteactorrefprovider$remotingterminator: sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>info</td>\n",
       "      <td>applicationmaster: registered signal handlers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>info</td>\n",
       "      <td>applicationmaster: applicationattemptid: appa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>info</td>\n",
       "      <td>remoteactorrefprovider$remotingterminator: sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: registered sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>info</td>\n",
       "      <td>slf0jlogger: slf0jlogger started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: starting remoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: remoting started; listening on addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'sparkexe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore started with capacit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: connecting to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: successfully re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>info</td>\n",
       "      <td>executor: starting executor id 0 on host awde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'org.apac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>info</td>\n",
       "      <td>nettyblocktransferservice: server created on 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: trying to register blockm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: registered blockmanager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver commande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore cleared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanager: blockmanager stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from awd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from TOK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    level                                            message\n",
       "0    info    securitymanager: changing view acls to:TOK_SUID\n",
       "1    info   securitymanager: changing modify acls to:TOK_...\n",
       "2    info   securitymanager: securitymanager: authenticat...\n",
       "3    info    securitymanager: changing view acls to:TOK_SUID\n",
       "4    info   securitymanager: changing modify acls to:TOK_...\n",
       "5    info   securitymanager: securitymanager: authenticat...\n",
       "6    info                   slf0jlogger: slf0jlogger started\n",
       "7    info                        remoting: starting remoting\n",
       "8    info   remoting: remoting started; listening on addr...\n",
       "9    info   utils: successfully started service 'sparkexe...\n",
       "10   info   diskblockmanager: created local directory at ...\n",
       "11   info   diskblockmanager: created local directory at ...\n",
       "12   info   diskblockmanager: created local directory at ...\n",
       "13   info   memorystore: memorystore started with capacit...\n",
       "14   info   coarsegrainedexecutorbackend: connecting to d...\n",
       "15   info   coarsegrainedexecutorbackend: successfully re...\n",
       "16   info   executor: starting executor id 0 on host awde...\n",
       "17   info   utils: successfully started service 'org.apac...\n",
       "18   info     nettyblocktransferservice: server created on 0\n",
       "19   info   blockmanagermaster: trying to register blockm...\n",
       "20   info        blockmanagermaster: registered blockmanager\n",
       "21   info   coarsegrainedexecutorbackend: driver commande...\n",
       "22   info                   memorystore: memorystore cleared\n",
       "23   info                 blockmanager: blockmanager stopped\n",
       "24   info   coarsegrainedexecutorbackend: driver from awd...\n",
       "25   info   coarsegrainedexecutorbackend: driver from TOK...\n",
       "26   info   remoteactorrefprovider$remotingterminator: sh...\n",
       "27   info          shutdownhookmanager: shutdown hook called\n",
       "28   info   applicationmaster: registered signal handlers...\n",
       "29   info   applicationmaster: applicationattemptid: appa...\n",
       "..    ...                                                ...\n",
       "302  info   remoteactorrefprovider$remotingterminator: sh...\n",
       "303  info          shutdownhookmanager: shutdown hook called\n",
       "304  info   coarsegrainedexecutorbackend: registered sign...\n",
       "305  info    securitymanager: changing view acls to:TOK_SUID\n",
       "306  info   securitymanager: changing modify acls to:TOK_...\n",
       "307  info   securitymanager: securitymanager: authenticat...\n",
       "308  info    securitymanager: changing view acls to:TOK_SUID\n",
       "309  info   securitymanager: changing modify acls to:TOK_...\n",
       "310  info   securitymanager: securitymanager: authenticat...\n",
       "311  info                   slf0jlogger: slf0jlogger started\n",
       "312  info                        remoting: starting remoting\n",
       "313  info   remoting: remoting started; listening on addr...\n",
       "314  info   utils: successfully started service 'sparkexe...\n",
       "315  info   diskblockmanager: created local directory at ...\n",
       "316  info   diskblockmanager: created local directory at ...\n",
       "317  info   diskblockmanager: created local directory at ...\n",
       "318  info   memorystore: memorystore started with capacit...\n",
       "319  info   coarsegrainedexecutorbackend: connecting to d...\n",
       "320  info   coarsegrainedexecutorbackend: successfully re...\n",
       "321  info   executor: starting executor id 0 on host awde...\n",
       "322  info   utils: successfully started service 'org.apac...\n",
       "323  info     nettyblocktransferservice: server created on 0\n",
       "324  info   blockmanagermaster: trying to register blockm...\n",
       "325  info        blockmanagermaster: registered blockmanager\n",
       "326  info   coarsegrainedexecutorbackend: driver commande...\n",
       "327  info                   memorystore: memorystore cleared\n",
       "328  info                 blockmanager: blockmanager stopped\n",
       "329  info   coarsegrainedexecutorbackend: driver from awd...\n",
       "330  info   coarsegrainedexecutorbackend: driver from TOK...\n",
       "331  info          shutdownhookmanager: shutdown hook called\n",
       "\n",
       "[332 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df[[\"level\", \"message\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"0\",\"level\", \"message\"]\n",
    "data = pd.read_csv(fullprocessedFileName,delimiter=\"|\",usecols=[0,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>coarsegrainedexecutorbackend: registered signal handlers for [term, hup, int]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>info</td>\n",
       "      <td>slf0jlogger: slf0jlogger started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: starting remoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: remoting started; listening on addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'sparkexe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore started with capacit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: connecting to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: successfully re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>info</td>\n",
       "      <td>executor: starting executor id 0 on host awde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'org.apac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>info</td>\n",
       "      <td>nettyblocktransferservice: server created on 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: trying to register blockm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: registered blockmanager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver commande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore cleared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanager: blockmanager stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from awd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from TOK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>info</td>\n",
       "      <td>remoteactorrefprovider$remotingterminator: sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>info</td>\n",
       "      <td>applicationmaster: registered signal handlers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>info</td>\n",
       "      <td>applicationmaster: applicationattemptid: appa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>info</td>\n",
       "      <td>remoteactorrefprovider$remotingterminator: sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: registered sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing view acls to:TOK_SUID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: changing modify acls to:TOK_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>info</td>\n",
       "      <td>securitymanager: securitymanager: authenticat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>info</td>\n",
       "      <td>slf0jlogger: slf0jlogger started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: starting remoting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>info</td>\n",
       "      <td>remoting: remoting started; listening on addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'sparkexe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>info</td>\n",
       "      <td>diskblockmanager: created local directory at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore started with capacit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: connecting to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: successfully re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>info</td>\n",
       "      <td>executor: starting executor id 0 on host awde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>info</td>\n",
       "      <td>utils: successfully started service 'org.apac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>info</td>\n",
       "      <td>nettyblocktransferservice: server created on 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: trying to register blockm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanagermaster: registered blockmanager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver commande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>info</td>\n",
       "      <td>memorystore: memorystore cleared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>info</td>\n",
       "      <td>blockmanager: blockmanager stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from awd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>info</td>\n",
       "      <td>coarsegrainedexecutorbackend: driver from TOK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>info</td>\n",
       "      <td>shutdownhookmanager: shutdown hook called</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     info  \\\n",
       "0    info   \n",
       "1    info   \n",
       "2    info   \n",
       "3    info   \n",
       "4    info   \n",
       "5    info   \n",
       "6    info   \n",
       "7    info   \n",
       "8    info   \n",
       "9    info   \n",
       "10   info   \n",
       "11   info   \n",
       "12   info   \n",
       "13   info   \n",
       "14   info   \n",
       "15   info   \n",
       "16   info   \n",
       "17   info   \n",
       "18   info   \n",
       "19   info   \n",
       "20   info   \n",
       "21   info   \n",
       "22   info   \n",
       "23   info   \n",
       "24   info   \n",
       "25   info   \n",
       "26   info   \n",
       "27   info   \n",
       "28   info   \n",
       "29   info   \n",
       "..    ...   \n",
       "302  info   \n",
       "303  info   \n",
       "304  info   \n",
       "305  info   \n",
       "306  info   \n",
       "307  info   \n",
       "308  info   \n",
       "309  info   \n",
       "310  info   \n",
       "311  info   \n",
       "312  info   \n",
       "313  info   \n",
       "314  info   \n",
       "315  info   \n",
       "316  info   \n",
       "317  info   \n",
       "318  info   \n",
       "319  info   \n",
       "320  info   \n",
       "321  info   \n",
       "322  info   \n",
       "323  info   \n",
       "324  info   \n",
       "325  info   \n",
       "326  info   \n",
       "327  info   \n",
       "328  info   \n",
       "329  info   \n",
       "330  info   \n",
       "331  info   \n",
       "\n",
       "     coarsegrainedexecutorbackend: registered signal handlers for [term, hup, int]  \n",
       "0      securitymanager: changing view acls to:TOK_SUID                              \n",
       "1     securitymanager: changing modify acls to:TOK_...                              \n",
       "2     securitymanager: securitymanager: authenticat...                              \n",
       "3      securitymanager: changing view acls to:TOK_SUID                              \n",
       "4     securitymanager: changing modify acls to:TOK_...                              \n",
       "5     securitymanager: securitymanager: authenticat...                              \n",
       "6                     slf0jlogger: slf0jlogger started                              \n",
       "7                          remoting: starting remoting                              \n",
       "8     remoting: remoting started; listening on addr...                              \n",
       "9     utils: successfully started service 'sparkexe...                              \n",
       "10    diskblockmanager: created local directory at ...                              \n",
       "11    diskblockmanager: created local directory at ...                              \n",
       "12    diskblockmanager: created local directory at ...                              \n",
       "13    memorystore: memorystore started with capacit...                              \n",
       "14    coarsegrainedexecutorbackend: connecting to d...                              \n",
       "15    coarsegrainedexecutorbackend: successfully re...                              \n",
       "16    executor: starting executor id 0 on host awde...                              \n",
       "17    utils: successfully started service 'org.apac...                              \n",
       "18      nettyblocktransferservice: server created on 0                              \n",
       "19    blockmanagermaster: trying to register blockm...                              \n",
       "20         blockmanagermaster: registered blockmanager                              \n",
       "21    coarsegrainedexecutorbackend: driver commande...                              \n",
       "22                    memorystore: memorystore cleared                              \n",
       "23                  blockmanager: blockmanager stopped                              \n",
       "24    coarsegrainedexecutorbackend: driver from awd...                              \n",
       "25    coarsegrainedexecutorbackend: driver from TOK...                              \n",
       "26    remoteactorrefprovider$remotingterminator: sh...                              \n",
       "27           shutdownhookmanager: shutdown hook called                              \n",
       "28    applicationmaster: registered signal handlers...                              \n",
       "29    applicationmaster: applicationattemptid: appa...                              \n",
       "..                                                 ...                              \n",
       "302   remoteactorrefprovider$remotingterminator: sh...                              \n",
       "303          shutdownhookmanager: shutdown hook called                              \n",
       "304   coarsegrainedexecutorbackend: registered sign...                              \n",
       "305    securitymanager: changing view acls to:TOK_SUID                              \n",
       "306   securitymanager: changing modify acls to:TOK_...                              \n",
       "307   securitymanager: securitymanager: authenticat...                              \n",
       "308    securitymanager: changing view acls to:TOK_SUID                              \n",
       "309   securitymanager: changing modify acls to:TOK_...                              \n",
       "310   securitymanager: securitymanager: authenticat...                              \n",
       "311                   slf0jlogger: slf0jlogger started                              \n",
       "312                        remoting: starting remoting                              \n",
       "313   remoting: remoting started; listening on addr...                              \n",
       "314   utils: successfully started service 'sparkexe...                              \n",
       "315   diskblockmanager: created local directory at ...                              \n",
       "316   diskblockmanager: created local directory at ...                              \n",
       "317   diskblockmanager: created local directory at ...                              \n",
       "318   memorystore: memorystore started with capacit...                              \n",
       "319   coarsegrainedexecutorbackend: connecting to d...                              \n",
       "320   coarsegrainedexecutorbackend: successfully re...                              \n",
       "321   executor: starting executor id 0 on host awde...                              \n",
       "322   utils: successfully started service 'org.apac...                              \n",
       "323     nettyblocktransferservice: server created on 0                              \n",
       "324   blockmanagermaster: trying to register blockm...                              \n",
       "325        blockmanagermaster: registered blockmanager                              \n",
       "326   coarsegrainedexecutorbackend: driver commande...                              \n",
       "327                   memorystore: memorystore cleared                              \n",
       "328                 blockmanager: blockmanager stopped                              \n",
       "329   coarsegrainedexecutorbackend: driver from awd...                              \n",
       "330   coarsegrainedexecutorbackend: driver from TOK...                              \n",
       "331          shutdownhookmanager: shutdown hook called                              \n",
       "\n",
       "[332 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
