{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#from plotly import plotly\n",
    "#import plotly.offline as offline\n",
    "#import plotly.graph_objs as go\n",
    "#offline.init_notebook_mode()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_data = pd.read_csv(r'C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder/application1.log',sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName}%{GREEDYDATA:logdata}', pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestmp': '2020-03-12 20:09:02,847', 'prg': '[main]', 'level': 'INFO', 'ClassName': 'org.apache.spark.executor.CoarseGrainedExecutorBackend', 'logdata': '  - Started daemon with process name: 70733@deda1x3311'}\n"
     ]
    }
   ],
   "source": [
    "print(lg.grok('''2020-03-12 20:09:02,847 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend  - Started daemon with process name: 70733@deda1x3311'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestmp': '2020-03-12 20:09:17,951', 'prg': '[Executor task launch worker for task 0]', 'level': 'ERROR', 'ClassName': 'org.apache.spark.executor.Executor', 'logdata': '  - Exception in task 0.0 in stage 0.0 (TID 0)'}\n"
     ]
    }
   ],
   "source": [
    "lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName}%{GREEDYDATA:logdata}', pr)\n",
    "print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-5bd1b30ef165>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-5bd1b30ef165>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [%{CUSTOM_PROCESS_NAME:processName}\\]%{SPACE}%{JAVACLASS:class}%{SPACE}-%{SPACE}%{JAVALOGMESSAGE:logmessage}\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[%{CUSTOM_PROCESS_NAME:processName}\\]%{SPACE}%{JAVACLASS:class}%{SPACE}-%{SPACE}%{JAVALOGMESSAGE:logmessage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestmp': '2020-03-12 20:09:17,951', 'prg': '[Executor task launch worker for task 0]', 'level': 'ERROR', 'ClassName': 'org.apache.spark.executor.Executor', 'JavaMessage': ' - Exception in task 0.0 in stage 0.0 (TID', 'logdata': '0)'}\n"
     ]
    }
   ],
   "source": [
    "lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)\n",
    "java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'If' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8622e8783670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#print(line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrok\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mIf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'If' is not defined"
     ]
    }
   ],
   "source": [
    "filename = \"application1.log\"\n",
    "\n",
    "lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)\n",
    "#java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        #print(line)\n",
    "        print(lg.grok(line))\n",
    "        If \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClassName': 'java.lang.RuntimeException:', 'ClassName2': 'java.lang.NullPointerException:', 'JavaMessage': 'hive.llap.daemon.service.hosts must be', 'logdata': 'defined'}\n"
     ]
    }
   ],
   "source": [
    "lg = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "print(lg.grok('''java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JavaStackTrace': 'at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:66)', 'class': 'com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory', 'method': 'createDataReader', 'file': 'HiveWarehouseDataReaderFactory.java', 'line': '66', 'logdata': ''}\n"
     ]
    }
   ],
   "source": [
    "lg = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "print(lg.grok('''at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:66)'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClassMessage': '2020-01-07 13:06:03,146 [Driver] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef  - Registered StateStoreCoordinator endpoint', 'logdata': ''}\n"
     ]
    }
   ],
   "source": [
    "#%{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} \n",
    "lg = LineGrokker('%{JAVALOGMESSAGE:ClassMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "a=lg.grok('''2020-01-07 13:06:03,146 [Driver] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef  - Registered StateStoreCoordinator endpoint''')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-09ee9a29de93>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-09ee9a29de93>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    if cline !=None\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        #print(lg.grok(line))\n",
    "        if pline=='':\n",
    "            cline=lg.grok(line)\n",
    "            if cline !=None\n",
    "                pline=cline            \n",
    "        elseif pline['level']=='ERROR':\n",
    "            cline=lg2.grok(line)\n",
    "            if cline !=None\n",
    "                pline={**pline, **cline}\n",
    "            cline=lg3.grok(line)\n",
    "            if cline !=None\n",
    "                pline={**pline, **cline}\n",
    "            cline=lg4.grok(line)\n",
    "            if cline !=None\n",
    "                pline={**pline, **cline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestmp': '2020-03-12 20:09:17,951', 'prg': '[Executor task launch worker for task 0]', 'level': 'ERROR', 'ClassName': 'org.apache.spark.executor.Executor', 'JavaMessage': ' - Exception in task 0.0 in stage 0.0 (TID', 'logdata': '0)'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "filename = \"testlog.txt\"\n",
    "\n",
    "outF = open(\"testoutput\", \"w\")\n",
    "\n",
    "lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{JAVALOGMESSAGE:ClassMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)\n",
    "#java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "pline=[]\n",
    "cline={}\n",
    "\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        #print(lg.grok(line))\n",
    "        cline=lg.grok(line)\n",
    "        if (cline is not None) and len(pline)==0:\n",
    "            print(cline)\n",
    "#            outF.write(str(cline))\n",
    "#            outF.write(\"\\n\")\n",
    "            #pline=[]\n",
    "            print(1)\n",
    "            if cline['level']=='ERROR':\n",
    "                pline.append(cline)\n",
    "            else:\n",
    "                outF.write(str(cline))\n",
    "                outF.write(\"\\n\")\n",
    "            continue\n",
    "        elif (cline is None) and len(pline)!=0:\n",
    "            cline=lg2.grok(line)\n",
    "            if cline is not None:\n",
    "                #print(cline)\n",
    "                #print(2)\n",
    "                pline.append(cline)\n",
    "                continue\n",
    "            cline=lg3.grok(line)\n",
    "            if cline is not None:\n",
    "                #print(cline)\n",
    "                #print(3)\n",
    "                pline.append(cline)                \n",
    "#            cline=lg4.grok(line)\n",
    "#            if cline is not None:\n",
    "#                print(cline)\n",
    "#                print(4)\n",
    "#                pline.append(cline)\n",
    "        elif (cline is not None) and len(pline)!=0:        \n",
    "            outF.write(str(pline))\n",
    "            outF.write(\"\\n\")\n",
    "            outF.write(str(cline))\n",
    "            outF.write(\"\\n\")\n",
    "            pline=[]\n",
    "#        print(pline)\n",
    "        \n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-605f9c5eef56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'testoutput'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#        for key in line.keys():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#outcsv = open(\"testoutput.csv\", \"w\")\n",
    "with open('testoutput', 'r') as f:\n",
    "    for line in f:\n",
    "        a=dict(line)\n",
    "        print(type(a))\n",
    "#        for key in line.keys():\n",
    "#              print(line[key])\n",
    "#        outcsv.write(\"%s,%s\\n\"%(key,line[key]))\n",
    "#outcsv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_02_000003 on awdex01126.merckgroup.com_45454_1581509898363', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14170', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data5/hadoop/yarn/local/usercache/s112788/filecache/134/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 __spark__.jar -> /data2/hadoop/yarn/local/filecache/4063/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data4/hadoop/yarn/local/usercache/s112788/filecache/133/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/348/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data5/hadoop/yarn/local/filecache/351/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data2/hadoop/yarn/local/filecache/1708/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    69 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data3/hadoop/yarn/local/filecache/1709/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 azure-storage-4.2.0.jar -> /data2/hadoop/yarn/local/filecache/1715/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data5/hadoop/yarn/local/filecache/7455/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data2/hadoop/yarn/local/filecache/7454/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 commons-lang3-3.4.jar -> /data3/hadoop/yarn/local/filecache/1716/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop  1016 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data3/hadoop/yarn/local/filecache/370/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data3/hadoop/yarn/local/filecache/4061/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data4/hadoop/yarn/local/filecache/352/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 guava-11.0.2.jar -> /data4/hadoop/yarn/local/filecache/1712/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data4/hadoop/yarn/local/filecache/7447/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/360/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    74 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/1711/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    53 Feb 12 13:17 hive-site.xml -> /data3/hadoop/yarn/local/filecache/4062/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data5/hadoop/yarn/local/filecache/353/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data5/hadoop/yarn/local/filecache/354/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data3/hadoop/yarn/local/filecache/1710/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    57 Feb 12 13:17 joda-time-2.5.jar -> /data3/hadoop/yarn/local/filecache/1714/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 json-simple-1.1.jar -> /data1/hadoop/yarn/local/filecache/1718/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22409 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data2/hadoop/yarn/local/filecache/7458/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data1/hadoop/yarn/local/filecache/7456/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data3/hadoop/yarn/local/filecache/368/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    82 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data3/hadoop/yarn/local/filecache/1713/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data6/hadoop/yarn/local/filecache/349/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/7453/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data4/hadoop/yarn/local/filecache/7452/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data4/hadoop/yarn/local/filecache/7448/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 py4j-0.9-src.zip -> /data4/hadoop/yarn/local/filecache/347/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    51 Feb 12 13:17 pyspark.zip -> /data5/hadoop/yarn/local/filecache/1707/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data1/hadoop/yarn/local/filecache/7451/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data5/hadoop/yarn/local/filecache/356/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data6/hadoop/yarn/local/filecache/7445/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop   100 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/1703/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data5/hadoop/yarn/local/filecache/7457/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data2/hadoop/yarn/local/filecache/7450/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data2/hadoop/yarn/local/filecache/7449/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '49136351    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '17227854    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '17227915    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '17227903    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '17227894    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '17227912    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '17227904    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17228082    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '17227888    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227913    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227901    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '17227896    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '17227911    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '17228047    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '17227890    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227917    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227886    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '17227889    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '17228043    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '17227870    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227855   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227891    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227910    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '17227897    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '17227867    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '17227899    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227898   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227916    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '17227892    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227907    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '17227895    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '17227868   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17228085    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227885   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17228044    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227887   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '17227905    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '17227914    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '11904417   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 14  2018 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49062405   40 -r-xr-xr-x   1 yarn     hadoop      36888 Dec 16 20:15 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7250107  580 -r-xr-xr-x   1 yarn     hadoop     588001 Feb 25  2019 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49136352    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '38535303  740 -r-xr-xr-x   1 yarn     hadoop     751238 Dec 16 20:15 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17170565  348 -r-xr-xr-x   1 yarn     hadoop     349304 Dec 16 20:15 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': '55099635 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Dec 16 20:15 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49062376  152 -r-xr-xr-x   1 yarn     hadoop     148962 Dec 16 20:15 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38495218  356 -r-xr-xr-x   1 yarn     hadoop     357604 Feb 25  2019 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '49136354    4 -rw-------   1 s112788  hadoop       1016 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '17179781  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17179784 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17179777   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 14  2018 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '49062411  480 -r-xr-xr-x   1 yarn     hadoop     486892 Dec 16 20:15 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7250122  432 -r-xr-xr-x   1 yarn     hadoop     434678 Feb 25  2019 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17170582 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Dec 16 20:15 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '55058470   16 -r-xr-xr-x   1 yarn     hadoop      16046 Feb 25  2019 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '55099514  228 -r-xr-xr-x   1 yarn     hadoop     226987 Dec 16 20:15 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49062395 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Dec 16 20:15 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38527370  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38527359  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49029135  732 -r-xr-xr-x   1 yarn     hadoop     745325 Feb 25  2019 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7249946 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Feb 25  2019 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7276356   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7249941   12 -r-xr-xr-x   1 yarn     hadoop      10092 Feb 25  2019 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17170571 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Dec 16 20:15 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38495143 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Feb 25  2019 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38527356   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38552177  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49062372 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Dec 16 20:15 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '12025922  164 -r-xr-xr-x   1 yarn     hadoop     162717 Dec 16 20:15 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49029130  564 -r-xr-xr-x   1 yarn     hadoop     570101 Feb 25  2019 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7276359  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7249951   56 -r-xr-xr-x   1 yarn     hadoop      52413 Feb 25  2019 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17162385 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Feb 25  2019 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49422665 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Jun 13  2019 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38535306  168 -r-xr-xr-x   1 yarn     hadoop     165361 Dec 16 20:15 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '17162381  216 -r-xr-xr-x   1 yarn     hadoop     213154 Feb 25  2019 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '12025870 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Jun 13  2019 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38527367 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '12025873    4 -r-xr-xr-x   1 yarn     hadoop       1912 Jun 13  2019 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '38527352  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '49136353   24 -rwx------   1 s112788  hadoop      22409 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22409', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDOUT=\"https://awdex01126.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000003/s112788/stdout?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01126.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDERR=\"https://awdex01126.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000003/s112788/stderr?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_02_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7452/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/347/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/4063/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/1716/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7449/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/351/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/1715/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7453/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7457/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/353/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7451/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/usercache/s112788/filecache/134/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/1708/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/1712/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/7445/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7456/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/1718/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7448/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/4062/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/1711/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/usercache/s112788/filecache/133/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7450/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/1714/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/1709/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/348/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7447/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/4061/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/368/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/1710/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/1713/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/352/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/349/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7455/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/1703/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/1707/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/354/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/360/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/370/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7458/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7454/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/356/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError=\\'kill %p\\' -Xms1024m -Xmx1024m \\'-Dlog4j.configuration=spark-log4j.properties\\' -Djava.io.tmpdir=$PWD/tmp \\'-Dspark.ui.port=0\\' \\'-Dspark.driver.port=44088\\' -Dspark.yarn.app.container.log.dir=/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.112:44088 --executor-id 2 --hostname awdex01126.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/stdout 2> /data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000003/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:4500', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data5/hadoop/yarn/local/filecache/1703/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/4063/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:59 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:59 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:59 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01126.merckgroup.com:59600]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:18:03 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 59600.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-8409fcaf-2f36-430a-8009-671bb00056f6', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-934268e0-3604-4d3c-8af9-47520381e5b3', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-11c8cecf-fb99-4019-ab9b-0107c765dacc', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-56bbd797-63c8-464c-872c-958a49b28c43', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-9d070b23-a356-476f-88a1-ddab14bcc50e', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-887809c5-564e-4292-85f0-a0f21f33c7f7', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:03 INFO MemoryStore: MemoryStore started with capacity 511.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.151.112:44088', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO CoarseGrainedExecutorBackend: Successfully registered with driver', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO Executor: Starting executor ID 2 on host awdex01126.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:18:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36748.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO NettyBlockTransferService: Server created on 36748', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver from awdex01130.merckgroup.com:44088 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver from 172.31.151.112:44088 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_01_000002 on awdex01127.merckgroup.com_45454_1581509897681', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14165', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data2/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 __spark__.jar -> /data1/hadoop/yarn/local/filecache/5307/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data1/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/431/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/434/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data5/hadoop/yarn/local/filecache/442/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/444/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 azure-storage-4.2.0.jar -> /data4/hadoop/yarn/local/filecache/450/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data2/hadoop/yarn/local/filecache/3222/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data2/hadoop/yarn/local/filecache/3221/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 commons-lang3-3.4.jar -> /data2/hadoop/yarn/local/filecache/452/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop  1016 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data4/hadoop/yarn/local/filecache/453/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data3/hadoop/yarn/local/filecache/438/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data3/hadoop/yarn/local/filecache/435/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 guava-11.0.2.jar -> /data2/hadoop/yarn/local/filecache/448/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/4184/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/443/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    73 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/446/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 hive-site.xml -> /data2/hadoop/yarn/local/filecache/440/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data6/hadoop/yarn/local/filecache/436/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data6/hadoop/yarn/local/filecache/437/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/445/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 joda-time-2.5.jar -> /data5/hadoop/yarn/local/filecache/449/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 json-simple-1.1.jar -> /data5/hadoop/yarn/local/filecache/454/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22396 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/3225/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data6/hadoop/yarn/local/filecache/3223/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/451/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data3/hadoop/yarn/local/filecache/447/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    82 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/5306/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data4/hadoop/yarn/local/filecache/3220/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data1/hadoop/yarn/local/filecache/3219/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data5/hadoop/yarn/local/filecache/3215/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 py4j-0.9-src.zip -> /data2/hadoop/yarn/local/filecache/5305/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 pyspark.zip -> /data6/hadoop/yarn/local/filecache/441/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data2/hadoop/yarn/local/filecache/3218/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data1/hadoop/yarn/local/filecache/439/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data2/hadoop/yarn/local/filecache/3212/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    99 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/433/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data3/hadoop/yarn/local/filecache/3224/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/3217/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data3/hadoop/yarn/local/filecache/4185/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '29344092    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '59957372 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Aug 14  2018 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11927737 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 22 18:31 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29411061  740 -r-xr-xr-x   1 yarn     hadoop     751238 Apr 25  2019 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '63209485 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Jun 19  2019 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29344095    4 -rw-------   1 s112788  hadoop       1016 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '26550566 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Apr 25  2019 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29222893  432 -r-xr-xr-x   1 yarn     hadoop     434678 Aug 14  2018 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '35922094  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29344094   24 -rwx------   1 s112788  hadoop      22396 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '35922090   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '59957325 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29222886    4 -r-xr-xr-x   1 yarn     hadoop       1912 Aug 14  2018 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976823    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '11976868    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '11976829   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976834   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976886    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976863    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '11976882    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976828    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '11976849    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976839    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976880    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976875    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '11976865    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976864   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976884    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '11976848    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976835    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '11976873    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '11976876    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976867    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '11976860    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '11976877    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '11976872    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '11976878    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '11976879    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '11976885    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '11976881    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '11976858    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '11976857    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976833    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976826   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976869    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976836   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '11976862    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '11976870    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '11976871    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '11976841    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '11911730  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38668107  580 -r-xr-xr-x   1 yarn     hadoop     588001 Aug 14  2018 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38668102  564 -r-xr-xr-x   1 yarn     hadoop     570101 Aug 14  2018 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '60138356  168 -r-xr-xr-x   1 yarn     hadoop     165361 Apr 25  2019 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11911701 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29411058   40 -r-xr-xr-x   1 yarn     hadoop      36888 Apr 25  2019 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '59957386   56 -r-xr-xr-x   1 yarn     hadoop      52413 Aug 14  2018 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305791  732 -r-xr-xr-x   1 yarn     hadoop     745325 Aug 14  2018 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38668110   16 -r-xr-xr-x   1 yarn     hadoop      16046 Aug 14  2018 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29294718   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 22 18:15 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '29222890 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Aug 14  2018 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '35922036 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Apr 25  2019 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305784  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29491577  348 -r-xr-xr-x   1 yarn     hadoop     349304 Jun 19  2019 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11977075  152 -r-xr-xr-x   1 yarn     hadoop     148962 Apr 25  2019 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29411014  164 -r-xr-xr-x   1 yarn     hadoop     162717 Apr 25  2019 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11977081  480 -r-xr-xr-x   1 yarn     hadoop     486892 Apr 25  2019 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305780  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305787  216 -r-xr-xr-x   1 yarn     hadoop     213154 Aug 14  2018 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38667985 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 14  2018 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305795   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29294774   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 22 18:15 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29411054  228 -r-xr-xr-x   1 yarn     hadoop     226987 Apr 25  2019 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11953245 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Aug 14  2018 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '11977078 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Apr 25  2019 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '29344093    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '29230323  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26305801  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '38822104 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Apr 25  2019 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '35922096  356 -r-xr-xr-x   1 yarn     hadoop     357604 Aug 14  2018 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '11953121   12 -r-xr-xr-x   1 yarn     hadoop      10092 Aug 14  2018 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22396', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDOUT=\"https://awdex01127.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000002/s112788/stdout?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01127.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDERR=\"https://awdex01127.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000002/s112788/stderr?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_01_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/434/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/5306/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/439/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/452/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/443/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/444/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/3219/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/437/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/3215/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/3223/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/450/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3212/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/5307/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/4184/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/5305/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/436/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/440/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3221/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/447/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/438/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/3217/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/449/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/441/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/431/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/451/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/442/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/435/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/446/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/445/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/3220/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3222/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3218/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/433/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/453/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/454/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/4185/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/448/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/3224/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/3225/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError=\\'kill %p\\' -Xms1024m -Xmx1024m \\'-Dlog4j.configuration=spark-log4j.properties\\' -Djava.io.tmpdir=$PWD/tmp \\'-Dspark.ui.port=0\\' \\'-Dspark.driver.port=40353\\' -Dspark.yarn.app.container.log.dir=/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.21:40353 --executor-id 1 --hostname awdex01127.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/stdout 2> /data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000002/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:4497', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/5307/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data5/hadoop/yarn/local/filecache/433/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:35 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:35 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:35 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:35 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01127.merckgroup.com:57747]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:36 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 57747.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-6e8f6b5b-7db8-4861-9c7e-52dec1f27f59', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-e2e52f90-cac5-4585-855c-9f348783e1e3', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-03499702-2fa5-40fa-a37f-641286a03629', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-b098908e-f2d3-484b-ad2b-ff420004c1ce', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-e3a9429a-0e98-4fe4-bbe8-24c8a68a67f1', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-8366168d-b8f2-491e-b5d9-9929e92a56cb', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO MemoryStore: MemoryStore started with capacity 511.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.151.21:40353', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO CoarseGrainedExecutorBackend: Successfully registered with driver', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO Executor: Starting executor ID 1 on host awdex01127.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39444.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO NettyBlockTransferService: Server created on 39444', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver from awdex01129.merckgroup.com:40353 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver from 172.31.151.21:40353 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_01_000003 on awdex01129.merckgroup.com_45454_1581509897767', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14153', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data3/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 __spark__.jar -> /data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/331/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data3/hadoop/yarn/local/filecache/334/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/342/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/344/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 azure-storage-4.2.0.jar -> /data4/hadoop/yarn/local/filecache/350/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data3/hadoop/yarn/local/filecache/7290/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/7289/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 commons-lang3-3.4.jar -> /data4/hadoop/yarn/local/filecache/352/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop  1016 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data6/hadoop/yarn/local/filecache/353/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data4/hadoop/yarn/local/filecache/338/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data6/hadoop/yarn/local/filecache/335/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 guava-11.0.2.jar -> /data2/hadoop/yarn/local/filecache/348/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data5/hadoop/yarn/local/filecache/7282/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/343/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    73 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/346/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 hive-site.xml -> /data3/hadoop/yarn/local/filecache/340/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data3/hadoop/yarn/local/filecache/336/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/337/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/345/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 joda-time-2.5.jar -> /data6/hadoop/yarn/local/filecache/349/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 json-simple-1.1.jar -> /data5/hadoop/yarn/local/filecache/354/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22395 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data2/hadoop/yarn/local/filecache/7293/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data4/hadoop/yarn/local/filecache/7291/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/351/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data3/hadoop/yarn/local/filecache/347/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/332/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/7288/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data4/hadoop/yarn/local/filecache/7287/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/7283/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 py4j-0.9-src.zip -> /data4/hadoop/yarn/local/filecache/4944/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 pyspark.zip -> /data3/hadoop/yarn/local/filecache/341/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data3/hadoop/yarn/local/filecache/7286/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data2/hadoop/yarn/local/filecache/339/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data4/hadoop/yarn/local/filecache/7280/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    99 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data2/hadoop/yarn/local/filecache/7292/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/7285/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data5/hadoop/yarn/local/filecache/7284/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '40042685    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '6406685 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Dec 16 22:00 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40042688   24 -rwx------   1 s112788  hadoop      22395 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '6160778  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512022  168 -r-xr-xr-x   1 yarn     hadoop     165361 Dec 16 22:00 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36503841    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '36503851    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503883    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503850   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503872    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503934    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503910    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503895    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503890    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '36503885    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503926    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503899    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '36503873   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503909    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503844   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503933    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '36503901    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503877    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503924    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '36503886    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503845    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503889    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '36503930    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '36503891    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503903    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503852   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503888    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '36503902    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503892   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503900    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '36503927    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503931    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503932    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '36503908    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '36503911    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '36503907    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '36503876    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '39944941    4 -r-xr-xr-x   1 yarn     hadoop       1912 Aug 14  2018 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '6160839  432 -r-xr-xr-x   1 yarn     hadoop     434678 Aug 14  2018 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39002702  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944939   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160830 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Aug 14  2018 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39004046   16 -r-xr-xr-x   1 yarn     hadoop      16046 Aug 14  2018 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000256 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52365229   12 -r-xr-xr-x   1 yarn     hadoop      10092 Aug 14  2018 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160821   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 14  2018 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40042689    4 -rw-------   1 s112788  hadoop       1016 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '39944859  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944950   56 -r-xr-xr-x   1 yarn     hadoop      52413 Aug 14  2018 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52322311 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 22 18:00 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000304  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985405  740 -r-xr-xr-x   1 yarn     hadoop     751238 Dec 16 22:00 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40042687    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '6406688 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Dec 16 22:00 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160835  732 -r-xr-xr-x   1 yarn     hadoop     745325 Aug 14  2018 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39004001   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6406682  164 -r-xr-xr-x   1 yarn     hadoop     162717 Dec 16 22:00 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36448033  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512012 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Dec 16 22:00 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160833 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Aug 14  2018 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39084235 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Dec 16 22:00 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52338907   40 -r-xr-xr-x   1 yarn     hadoop      36888 Dec 16 22:00 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512029  480 -r-xr-xr-x   1 yarn     hadoop     486892 Dec 16 22:00 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512019 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Dec 16 22:00 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6480362   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 22 18:00 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '36448037 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52338904  152 -r-xr-xr-x   1 yarn     hadoop     148962 Dec 16 22:00 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985381  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000259  580 -r-xr-xr-x   1 yarn     hadoop     588001 Aug 14  2018 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39003998  216 -r-xr-xr-x   1 yarn     hadoop     213154 Aug 14  2018 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160828 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 14  2018 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36448040 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Aug 14  2018 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985402  228 -r-xr-xr-x   1 yarn     hadoop     226987 Dec 16 22:00 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944948  356 -r-xr-xr-x   1 yarn     hadoop     357604 Aug 14  2018 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '52365225  564 -r-xr-xr-x   1 yarn     hadoop     570101 Aug 14  2018 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39084232  348 -r-xr-xr-x   1 yarn     hadoop     349304 Dec 16 22:00 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22395', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDOUT=\"https://awdex01129.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000003/s112788/stdout?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01129.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDERR=\"https://awdex01129.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000003/s112788/stderr?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_01_000003\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7293/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7282/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7284/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/354/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/7290/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/345/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/346/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7289/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/344/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/352/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/338/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/336/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/4944/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/342/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/339/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/334/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/337/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/350/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/332/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/347/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7285/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7291/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7280/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/341/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/353/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7292/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/340/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/349/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/335/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/7286/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7288/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/331/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/348/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7283/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7287/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/343/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/351/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError=\\'kill %p\\' -Xms1024m -Xmx1024m \\'-Dlog4j.configuration=spark-log4j.properties\\' -Djava.io.tmpdir=$PWD/tmp \\'-Dspark.ui.port=0\\' \\'-Dspark.driver.port=40353\\' -Dspark.yarn.app.container.log.dir=/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.21:40353 --executor-id 2 --hostname awdex01129.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/stdout 2> /data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000003/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:4497', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:32 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:32 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:33 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:33 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01129.merckgroup.com:58307]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:34 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 58307.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-2e034938-ce57-4cd2-bd5a-479f795a75b6', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-d72614a0-e8d2-4f41-9fe3-23a107617dc7', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-351ba383-51b9-4053-8f8f-fca07175a6ce', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-0c4052ff-01bd-45bd-a37c-5b4226d70f95', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-b1017a3c-1ea1-4e68-9ef4-dedf94692d50', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-4c3f3be0-57d6-45d2-9256-bbb614440547', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO MemoryStore: MemoryStore started with capacity 511.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.151.21:40353', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO CoarseGrainedExecutorBackend: Successfully registered with driver', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO Executor: Starting executor ID 2 on host awdex01129.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60510.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO NettyBlockTransferService: Server created on 60510', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver from awdex01129.merckgroup.com:40353 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO CoarseGrainedExecutorBackend: Driver from 172.31.151.21:40353 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_01_000001 on awdex01129.merckgroup.com_45454_1581509897767', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14153', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data3/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 __spark__.jar -> /data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/331/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data3/hadoop/yarn/local/filecache/334/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/342/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/344/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 azure-storage-4.2.0.jar -> /data4/hadoop/yarn/local/filecache/350/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data3/hadoop/yarn/local/filecache/7290/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/7289/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 commons-lang3-3.4.jar -> /data4/hadoop/yarn/local/filecache/352/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop   978 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data6/hadoop/yarn/local/filecache/353/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data4/hadoop/yarn/local/filecache/338/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data6/hadoop/yarn/local/filecache/335/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 guava-11.0.2.jar -> /data2/hadoop/yarn/local/filecache/348/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data5/hadoop/yarn/local/filecache/7282/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/343/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    73 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/346/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 hive-site.xml -> /data3/hadoop/yarn/local/filecache/340/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data3/hadoop/yarn/local/filecache/336/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/337/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/345/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 joda-time-2.5.jar -> /data6/hadoop/yarn/local/filecache/349/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 json-simple-1.1.jar -> /data5/hadoop/yarn/local/filecache/354/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22344 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data2/hadoop/yarn/local/filecache/7293/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data4/hadoop/yarn/local/filecache/7291/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/351/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data3/hadoop/yarn/local/filecache/347/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/332/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/7288/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data4/hadoop/yarn/local/filecache/7287/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/7283/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 py4j-0.9-src.zip -> /data4/hadoop/yarn/local/filecache/4944/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 pyspark.zip -> /data3/hadoop/yarn/local/filecache/341/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data3/hadoop/yarn/local/filecache/7286/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data2/hadoop/yarn/local/filecache/339/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data4/hadoop/yarn/local/filecache/7280/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    99 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data2/hadoop/yarn/local/filecache/7292/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/7285/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data5/hadoop/yarn/local/filecache/7284/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '14106648    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '36448037 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39003998  216 -r-xr-xr-x   1 yarn     hadoop     213154 Aug 14  2018 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000259  580 -r-xr-xr-x   1 yarn     hadoop     588001 Aug 14  2018 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14106651   24 -rwx------   1 s112788  hadoop      22344 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '6160828 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 14  2018 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39084232  348 -r-xr-xr-x   1 yarn     hadoop     349304 Dec 16 22:00 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985402  228 -r-xr-xr-x   1 yarn     hadoop     226987 Dec 16 22:00 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14106650    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '39944948  356 -r-xr-xr-x   1 yarn     hadoop     357604 Aug 14  2018 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '36512022  168 -r-xr-xr-x   1 yarn     hadoop     165361 Dec 16 22:00 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512019 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Dec 16 22:00 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52365229   12 -r-xr-xr-x   1 yarn     hadoop      10092 Aug 14  2018 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000256 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160833 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Aug 14  2018 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52338907   40 -r-xr-xr-x   1 yarn     hadoop      36888 Dec 16 22:00 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160778  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39002702  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36448033  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944941    4 -r-xr-xr-x   1 yarn     hadoop       1912 Aug 14  2018 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '6480362   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 22 18:00 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '6406682  164 -r-xr-xr-x   1 yarn     hadoop     162717 Dec 16 22:00 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6406685 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Dec 16 22:00 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944859  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39004046   16 -r-xr-xr-x   1 yarn     hadoop      16046 Aug 14  2018 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14000304  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39084235 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Dec 16 22:00 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944939   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36503841    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '36503851    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503883    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503850   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503872    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503934    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503910    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503895    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503890    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '36503885    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503926    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503899    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '36503873   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503909    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503844   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503933    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '36503901    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503877    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '36503924    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '36503886    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503845    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503889    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '36503930    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '36503891    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503903    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '36503852   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503888    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '36503902    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36503892   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503900    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '36503927    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503931    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '36503932    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '36503908    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '36503911    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '36503907    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '36503876    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '36512012 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Dec 16 22:00 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '14106652    4 -rw-------   1 s112788  hadoop        978 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '36448040 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Aug 14  2018 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52338904  152 -r-xr-xr-x   1 yarn     hadoop     148962 Dec 16 22:00 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39944950   56 -r-xr-xr-x   1 yarn     hadoop      52413 Aug 14  2018 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6406688 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Dec 16 22:00 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52322311 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 22 18:00 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39004001   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985405  740 -r-xr-xr-x   1 yarn     hadoop     751238 Dec 16 22:00 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '39985381  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160835  732 -r-xr-xr-x   1 yarn     hadoop     745325 Aug 14  2018 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '36512029  480 -r-xr-xr-x   1 yarn     hadoop     486892 Dec 16 22:00 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160821   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 14  2018 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160830 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Aug 14  2018 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '52365225  564 -r-xr-xr-x   1 yarn     hadoop     570101 Aug 14  2018 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6160839  432 -r-xr-xr-x   1 yarn     hadoop     434678 Aug 14  2018 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22344', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export MAX_APP_ATTEMPTS=\"2\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export APP_SUBMIT_TIME_ENV=\"1581509844840\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01129.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export APPLICATION_WEB_PROXY_BASE=\"/proxy/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_01_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/usercache/s112788/filecache/165/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7293/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7282/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/7284/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/354/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/7290/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/usercache/s112788/filecache/164/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/345/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/346/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7289/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/344/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/352/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/338/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/336/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/4944/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/342/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/339/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/334/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/337/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/350/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/332/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/347/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/7285/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7291/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7280/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/341/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/353/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7292/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/340/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/349/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/335/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/7286/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7288/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/331/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/348/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/7283/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/7287/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/343/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/351/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -Xmx1024m -Djava.io.tmpdir=$PWD/tmp \\'-Dlog4j.configuration=spark-log4j.properties\\' -Dhdp.version=2.5.5.0-157 -Dspark.yarn.app.container.log.dir=/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class \\'com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport\\' --jar file:/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18135/container_e151_1580556634479_18135_01_000002/qd_rdq_2.10-1.0.1.jar --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/business_metadata_TrackwiseOLDL_AuditExport.conf\\' --executor-memory 1024m --executor-cores 1 --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/stdout 2> /data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_01_000001/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:63651', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data4/hadoop/yarn/local/filecache/333/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/4945/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:27 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:28 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1580556634479_18136_000001', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO ApplicationMaster: Starting the user application in a separate Thread', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO ApplicationMaster: Waiting for spark context initialization', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO ApplicationMaster: Waiting for spark context initialization ... ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SparkContext: Running Spark version 1.6.3', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:29 INFO Utils: Successfully started service 'sparkDriver' on port 40353.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.31.151.21:50193]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:29 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 50193.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SparkEnv: Registering MapOutputTracker', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO SparkEnv: Registering BlockManagerMaster', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-4e408e5a-c765-4794-8f3a-a915af3cc315', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-03dc3a8c-baba-430f-a3a2-1fb55628402e', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-855a1146-6383-46ad-85df-ebaf3f6fd01b', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-cb3622de-482e-4f71-b97e-5ed6b3a634a7', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-be1941bf-96f7-4a86-849c-dfd4b98bfb0c', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-47794372-8837-477e-80fe-b56bbac3cfbb', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:29 INFO MemoryStore: MemoryStore started with capacity 514.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO SparkEnv: Registering OutputCommitCoordinator', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:30 INFO Utils: Successfully started service 'SparkUI' on port 40722.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.151.21:40722', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnClusterScheduler: Created YarnClusterScheduler', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580556634479_18136 and attemptId Some(appattempt_1580556634479_18136_000001)', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45993.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO NettyBlockTransferService: Server created on 45993', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.151.21:45993 with 514.1 MB RAM, BlockManagerId(driver, 172.31.151.21, 45993)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.31.151.21:40353)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnRMClient: Registering the ApplicationMaster', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Will request 2 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO AMRMClientImpl: Received new token for : awdex01127.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Launching container container_e151_1580556634479_18136_01_000002 for on host awdex01127.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.151.21:40353,  executorHostname: awdex01127.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ExecutorRunnable: Starting Executor Container', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ExecutorRunnable: Setting up ContainerLaunchContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ExecutorRunnable: Preparing Local resources', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:30 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534249918566 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1556198495792 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190578782 timestamp: 1502794813190 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1556198495093 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1581509843405 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534249584569 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1556198495346 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534249935160 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1556198495732 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534249602504 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip\" } size: 357604 timestamp: 1534249923167 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534249912111 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534249920815 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534249617009 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1556198495170 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534249589424 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534249910268 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534249620408 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1556198495466 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534249605276 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534249608400 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534249591635 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534250486996 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1556198495553 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml\" } size: 1912 timestamp: 1534249916343 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534249614752 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip\" } size: 142504 timestamp: 1581509844696 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534249612403 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534249610121 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1556198981014 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534249593898 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1556198495246 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1556198495038 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1556198495906 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534249599002 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1556198496014 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1556198495843 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534249587039 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534249600776 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534249913882 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534249596609 type: FILE visibility: PUBLIC)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 WARN Client: Exception encountered while connecting to the server : ', 'logdata': ''}\n",
      "{'ClassMessage': 'org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:595)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:762)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:758)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.security.AccessController.doPrivileged(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat javax.security.auth.Subject.doAs(Subject.java:422)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1865)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:757)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1620)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:816)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Method.invoke(Method.java:498)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2158)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1423)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$sparkJar(Client.scala:1262)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.populateClasspath(Client.scala:1397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.prepareEnvironment(ExecutorRunnable.scala:290)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env$lzycompute(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:80)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Thread.run(Thread.java:745)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: ', 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'YARN executor launch context:', 'logdata': ''}\n",
      "{'ClassMessage': '  env:', 'logdata': ''}\n",
      "{'ClassMessage': '    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDERR -> https://awdex01127.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000002/s112788/stderr?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1580556634479_18136', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 142504', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_USER -> s112788', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581509844696', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_MODE -> true', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDOUT -> https://awdex01127.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000002/s112788/stdout?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES -> hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': '  command:', 'logdata': ''}\n",
      "{'ClassMessage': \"    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=40353' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.21:40353 --executor-id 1 --hostname awdex01127.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\", 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': '      ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01127.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO AMRMClientImpl: Received new token for : awdex01129.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO YarnAllocator: Launching container container_e151_1580556634479_18136_01_000003 for on host awdex01129.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.151.21:40353,  executorHostname: awdex01129.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: Starting Executor Container', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: Setting up ContainerLaunchContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: Preparing Local resources', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534249918566 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1556198495792 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190578782 timestamp: 1502794813190 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1556198495093 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1581509843405 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534249584569 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1556198495346 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534249935160 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1556198495732 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534249602504 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip\" } size: 357604 timestamp: 1534249923167 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534249912111 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534249920815 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534249617009 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1556198495170 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534249589424 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534249910268 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534249620408 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1556198495466 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534249605276 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534249608400 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534249591635 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534250486996 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1556198495553 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml\" } size: 1912 timestamp: 1534249916343 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534249614752 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip\" } size: 142504 timestamp: 1581509844696 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534249612403 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534249610121 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1556198981014 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534249593898 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1556198495246 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1556198495038 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1556198495906 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534249599002 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1556198496014 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1556198495843 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534249587039 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534249600776 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534249913882 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534249596609 type: FILE visibility: PUBLIC)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ExecutorRunnable: ', 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'YARN executor launch context:', 'logdata': ''}\n",
      "{'ClassMessage': '  env:', 'logdata': ''}\n",
      "{'ClassMessage': '    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDERR -> https://awdex01129.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000003/s112788/stderr?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1580556634479_18136', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 142504', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_USER -> s112788', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581509844696', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_MODE -> true', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDOUT -> https://awdex01129.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_01_000003/s112788/stdout?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES -> hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': '  command:', 'logdata': ''}\n",
      "{'ClassMessage': \"    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=40353' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.21:40353 --executor-id 2 --hostname awdex01129.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\", 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': '      ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:31 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01129.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO AMRMClientImpl: Received new token for : awdex01128.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01129.merckgroup.com:58579) with ID 2', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:34 INFO BlockManagerMasterEndpoint: Registering block manager awdex01129.merckgroup.com:60510 with 511.1 MB RAM, BlockManagerId(2, awdex01129.merckgroup.com, 60510)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01127.merckgroup.com:42269) with ID 1', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO BlockManagerMasterEndpoint: Registering block manager awdex01127.merckgroup.com:39444 with 511.1 MB RAM, BlockManagerId(1, awdex01127.merckgroup.com, 39444)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO PipelineConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO FilesystemAccess$: Max Pipeline Run ID found:21', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:36 INFO InputConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:37 INFO PrepareTrackwiseOLDL_AuditExport$: Checking input path hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:37 INFO EnvironmentConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:37 INFO AtlasRepository: Atlas URLs provided for this pipeline: https://awdex01123.merckgroup.com:21443/api/atlas/,https://awdex01124.merckgroup.com:21443/api/atlas/', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:47 INFO AtlasRepository: response: HttpResponse({\"requestId\":\"qtp1489092624-3691154 - c5bc4ba8-b732-4026-8229-84eee31c499b\",\"definition\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"f45e0423-5f62-4c4b-b6c4-5fcd13154b89\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\\\"* * * * *\\\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs:\\\\/\\\\/aagxp\\\\/mc_staging\\\\/rdq_uc7_files\\\\/trackwise_oldl_auditexport\\\\/00_00_01\\\\/run_000021\\\\/raw\\\\/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aagxp_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000021_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-12T12:16:43.402Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-12T12:16:43.992Z\",\"geoScope\":\"NA\",\"owner\":\"s112788\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: fep-conf-prod.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}},200,Map(Content-Type -> Vector(application/json; charset=UTF-8), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=p070mut9xwa21stq7unqe9nbo;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 200 OK), Transfer-Encoding -> Vector(chunked), X-Frame-Options -> Vector(DENY)))', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:47 INFO AtlasRepository: Atlas request to https://awdex01123.merckgroup.com:21443/api/atlas/entities succeeded.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:47 INFO MetadataAccess: getEntityByFileLocation: {\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"f45e0423-5f62-4c4b-b6c4-5fcd13154b89\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\\\"* * * * *\\\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aagxp_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000021_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-12T12:16:43.402Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-12T12:16:43.992Z\",\"geoScope\":\"NA\",\"owner\":\"s112788\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: fep-conf-prod.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:47 INFO PrepareTrackwiseOLDL_AuditExport$: Excel Input path:hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport/Audit_Export_Report_01_20200203_v3__m271552_2020-02-12T13-02-31+0100.xls', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO PrepareTrackwiseOLDL_AuditExport$: Expected sheet Name: Sheet1', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 ERROR ApplicationMaster: User class threw exception: java.lang.reflect.InvocationTargetException', 'logdata': ''}\n",
      "{'ClassMessage': 'java.lang.reflect.InvocationTargetException', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate$$anonfun$mapRowToObject$1.apply(RdqTransformationTemplate.scala:202)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate$$anonfun$mapRowToObject$1.apply(RdqTransformationTemplate.scala:199)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.List.foreach(List.scala:318)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.mapRowToObject(RdqTransformationTemplate.scala:198)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport$.createDataframeFromExcelWorkbook(PrepareTrackwiseOLDL_AuditExport.scala:25)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.doSpecificTransformation(RdqTransformationTemplate.scala:171)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.pipelinecore.TransformationStepTemplate$class.executeTransformationStep(TransformationStepTemplate.scala:42)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.executeTransformationStep(RdqTransformationTemplate.scala:25)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.main(RdqTransformationTemplate.scala:241)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport.main(PrepareTrackwiseOLDL_AuditExport.scala)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Method.invoke(Method.java:498)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:561)', 'logdata': ''}\n",
      "{'ClassMessage': 'Caused by: java.lang.NumberFormatException: For input string: \"\"', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Integer.parseInt(Integer.java:592)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Integer.parseInt(Integer.java:615)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:31)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.util.ParseRow$.getIntValue(ParseRow.scala:67)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.schemas.TrackwiseOLDL_AuditExport.<init>(TrackwiseOLDL_AuditExport.scala:120)', 'logdata': ''}\n",
      "{'ClassMessage': '\\t... 23 more', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.reflect.InvocationTargetException)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO SparkContext: Invoking stop() from shutdown hook', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO SparkUI: Stopped Spark web UI at http://172.31.151.21:40722', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO YarnAllocator: Driver requested a total number of 0 executor(s).', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO YarnClusterSchedulerBackend: Shutting down all executors', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO YarnClusterSchedulerBackend: Asking each executor to shut down', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices', 'logdata': ''}\n",
      "{'ClassMessage': '(serviceOption=None,', 'logdata': ''}\n",
      "{'ClassMessage': ' services=List(),', 'logdata': ''}\n",
      "{'ClassMessage': ' started=false)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO BlockManagerMaster: BlockManagerMaster stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO SparkContext: Successfully stopped SparkContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-cb831f78-0f45-4982-8a2c-9ee0efae949b', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-50d10575-f288-48c1-8d13-e526f3582eab', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-be5382c5-2e35-464e-bad9-05ca9c4ebf39', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-a6e27884-8e13-45df-a732-60c18bd848a7', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-69470286-12ab-405f-a3e7-7df2322d0ee6', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:48 INFO ShutdownHookManager: Deleting directory /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-ab655855-8f7e-4948-b5f8-542a99cbe98e', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:17 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_02_000001 on awdex01130.merckgroup.com_45454_1581509898487', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14151', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112788/filecache/161/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 __spark__.jar -> /data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data6/hadoop/yarn/local/usercache/s112788/filecache/160/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data6/hadoop/yarn/local/filecache/416/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/419/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/427/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data5/hadoop/yarn/local/filecache/429/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 azure-storage-4.2.0.jar -> /data6/hadoop/yarn/local/filecache/435/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data6/hadoop/yarn/local/filecache/2698/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data6/hadoop/yarn/local/filecache/2697/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 commons-lang3-3.4.jar -> /data4/hadoop/yarn/local/filecache/437/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop   978 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data5/hadoop/yarn/local/filecache/438/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data4/hadoop/yarn/local/filecache/423/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data3/hadoop/yarn/local/filecache/420/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/432/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/2690/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/428/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    73 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/431/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 hive-site.xml -> /data2/hadoop/yarn/local/filecache/425/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data5/hadoop/yarn/local/filecache/421/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/422/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/430/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 joda-time-2.5.jar -> /data4/hadoop/yarn/local/filecache/434/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 json-simple-1.1.jar -> /data5/hadoop/yarn/local/filecache/439/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22341 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data2/hadoop/yarn/local/filecache/2701/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data4/hadoop/yarn/local/filecache/2699/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/436/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/433/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/417/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/2696/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data2/hadoop/yarn/local/filecache/3760/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/2691/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 py4j-0.9-src.zip -> /data5/hadoop/yarn/local/filecache/415/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 pyspark.zip -> /data6/hadoop/yarn/local/filecache/426/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data5/hadoop/yarn/local/filecache/3759/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data4/hadoop/yarn/local/filecache/424/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data3/hadoop/yarn/local/filecache/2688/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    99 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data3/hadoop/yarn/local/filecache/2700/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data3/hadoop/yarn/local/filecache/2693/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data5/hadoop/yarn/local/filecache/2692/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '27099140    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '26961322  564 -r-xr-xr-x   1 yarn     hadoop     570101 Aug 14  2018 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796725 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Apr 25  2019 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48636393  164 -r-xr-xr-x   1 yarn     hadoop     162717 Apr 25  2019 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756327    4 -r-xr-xr-x   1 yarn     hadoop       1912 Aug 14  2018 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '26961329  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961314  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48636397  152 -r-xr-xr-x   1 yarn     hadoop     148962 Apr 25  2019 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796722 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Apr 25  2019 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '3358875  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58859666 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 21  2017 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644566   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 14  2018 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '40755293 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Feb  7  2019 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900951  740 -r-xr-xr-x   1 yarn     hadoop     751238 Apr 25  2019 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644699   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '3261374 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Aug 14  2018 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961332 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Aug 14  2018 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58902291  732 -r-xr-xr-x   1 yarn     hadoop     745325 Aug 14  2018 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796728  480 -r-xr-xr-x   1 yarn     hadoop     486892 Apr 25  2019 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6791659 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Apr 25  2019 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644714   16 -r-xr-xr-x   1 yarn     hadoop      16046 Aug 14  2018 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48636419  168 -r-xr-xr-x   1 yarn     hadoop     165361 Apr 25  2019 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961339  432 -r-xr-xr-x   1 yarn     hadoop     434678 Aug 14  2018 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '27001379 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Apr 25  2019 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900727    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '58900801    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '58900760   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900795    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '58900819    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900797    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900764    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900793    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900776    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900799    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900816    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '58900768    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900740   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900771    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900813    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900789    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900823    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '58900766    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900800    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '58900817    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '58900821    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900824    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900765   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900782    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '58900763   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900790   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900743    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900818    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900822    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '58900812    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900796    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900788    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '58900794    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '58900770    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900820    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '58900777    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '58900761    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '27099144   24 -rwx------   1 s112788  hadoop      22341 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58861338  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644706   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961320 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Aug 14  2018 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '41017597 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Jun 19  2019 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58902209  356 -r-xr-xr-x   1 yarn     hadoop     357604 Aug 14  2018 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '26961316  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644702   12 -r-xr-xr-x   1 yarn     hadoop      10092 Aug 14  2018 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961310   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 14  2018 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644710  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756330  216 -r-xr-xr-x   1 yarn     hadoop     213154 Aug 14  2018 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796718  348 -r-xr-xr-x   1 yarn     hadoop     349304 Apr 25  2019 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756334   56 -r-xr-xr-x   1 yarn     hadoop      52413 Aug 14  2018 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961324 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900948   40 -r-xr-xr-x   1 yarn     hadoop      36888 Apr 25  2019 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '27099145    4 -rw-------   1 s112788  hadoop        978 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '27099143    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '26961335  580 -r-xr-xr-x   1 yarn     hadoop     588001 Aug 14  2018 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7209420  228 -r-xr-xr-x   1 yarn     hadoop     226987 Jun 19  2019 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48587725 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22341', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export MAX_APP_ATTEMPTS=\"2\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export APP_SUBMIT_TIME_ENV=\"1581509844840\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01130.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export APPLICATION_WEB_PROXY_BASE=\"/proxy/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_02_000001\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/420/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/434/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/417/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/430/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/2692/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2700/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/2697/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/438/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/437/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/426/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/428/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2691/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/427/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/423/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2693/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/436/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2688/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/433/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/416/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/usercache/s112788/filecache/161/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/432/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/422/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/425/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/429/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2701/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/421/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/usercache/s112788/filecache/160/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3760/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/419/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2696/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/415/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/424/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/435/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/431/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/3759/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/2699/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/2698/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/439/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2690/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -Xmx1024m -Djava.io.tmpdir=$PWD/tmp \\'-Dlog4j.configuration=spark-log4j.properties\\' -Dhdp.version=2.5.5.0-157 -Dspark.yarn.app.container.log.dir=/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class \\'com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport\\' --jar file:/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18135/container_e151_1580556634479_18135_01_000002/qd_rdq_2.10-1.0.1.jar --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf\\' --arg \\'hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/business_metadata_TrackwiseOLDL_AuditExport.conf\\' --executor-memory 1024m --executor-cores 1 --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/stdout 2> /data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000001/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:67381', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:51 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:52 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1580556634479_18136_000002', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO ApplicationMaster: Starting the user application in a separate Thread', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO ApplicationMaster: Waiting for spark context initialization', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO ApplicationMaster: Waiting for spark context initialization ... ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SparkContext: Running Spark version 1.6.3', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:53 INFO Utils: Successfully started service 'sparkDriver' on port 44088.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.31.151.112:40257]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:53 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 40257.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SparkEnv: Registering MapOutputTracker', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO SparkEnv: Registering BlockManagerMaster', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-4ca035e9-1e29-409a-8b2a-10c3253810e3', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-2fa2e99f-1bc8-4660-9c03-0b1219af10a8', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-3335f334-ae02-4c71-b149-8ed7c45d33ee', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-75aa5a43-3428-4386-9093-3a2c8222114f', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-a7dffbf4-2e11-4ea4-8826-e1fa3968f34a', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-dc6a4832-6708-4b89-928a-c4c80440bcfc', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:53 INFO MemoryStore: MemoryStore started with capacity 514.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO SparkEnv: Registering OutputCommitCoordinator', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:54 INFO Utils: Successfully started service 'SparkUI' on port 44105.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.151.112:44105', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnClusterScheduler: Created YarnClusterScheduler', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580556634479_18136 and attemptId Some(appattempt_1580556634479_18136_000002)', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43730.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO NettyBlockTransferService: Server created on 43730', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.151.112:43730 with 514.1 MB RAM, BlockManagerId(driver, 172.31.151.112, 43730)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.31.151.112:44088)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnRMClient: Registering the ApplicationMaster', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnAllocator: Will request 2 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:54 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO AMRMClientImpl: Received new token for : awdex01130.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO AMRMClientImpl: Received new token for : awdex01126.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO YarnAllocator: Launching container container_e151_1580556634479_18136_02_000002 for on host awdex01130.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.151.112:44088,  executorHostname: awdex01130.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO YarnAllocator: Launching container container_e151_1580556634479_18136_02_000003 for on host awdex01126.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Starting Executor Container', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.151.112:44088,  executorHostname: awdex01126.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Starting Executor Container', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Setting up ContainerLaunchContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Setting up ContainerLaunchContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Preparing Local resources', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Preparing Local resources', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534249918566 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1556198495792 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190578782 timestamp: 1502794813190 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1556198495093 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1581509843405 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534249584569 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1556198495346 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534249935160 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1556198495732 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534249602504 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip\" } size: 357604 timestamp: 1534249923167 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534249912111 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534249920815 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534249617009 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1556198495170 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534249589424 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534249910268 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534249620408 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1556198495466 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534249605276 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534249608400 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534249591635 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534250486996 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1556198495553 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml\" } size: 1912 timestamp: 1534249916343 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534249614752 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip\" } size: 142504 timestamp: 1581509844696 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534249612403 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534249610121 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1556198981014 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534249593898 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1556198495246 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1556198495038 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1556198495906 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534249599002 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1556198496014 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1556198495843 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534249587039 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534249600776 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534249913882 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534249596609 type: FILE visibility: PUBLIC)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534249918566 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1556198495792 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190578782 timestamp: 1502794813190 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1556198495093 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1581509843405 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534249584569 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1556198495346 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534249935160 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1556198495732 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534249602504 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip\" } size: 357604 timestamp: 1534249923167 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534249912111 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534249920815 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534249617009 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1556198495170 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534249589424 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534249910268 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534249620408 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1556198495466 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534249605276 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534249608400 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534249591635 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534250486996 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1556198495553 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml\" } size: 1912 timestamp: 1534249916343 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534249614752 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip\" } size: 142504 timestamp: 1581509844696 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534249612403 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534249610121 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar\" } size: 226987 timestamp: 1556198981014 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534249593898 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1556198495246 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1556198495038 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1556198495906 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534249599002 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1556198496014 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1556198495843 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534249587039 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534249600776 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534249913882 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aagxp\" port: -1 file: \"/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534249596609 type: FILE visibility: PUBLIC)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 WARN Client: Exception encountered while connecting to the server : ', 'logdata': ''}\n",
      "{'ClassMessage': 'org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:595)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:762)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:758)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.security.AccessController.doPrivileged(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat javax.security.auth.Subject.doAs(Subject.java:422)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1865)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:757)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1620)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:816)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Method.invoke(Method.java:498)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2158)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1423)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$sparkJar(Client.scala:1262)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.populateClasspath(Client.scala:1397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.prepareEnvironment(ExecutorRunnable.scala:290)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env$lzycompute(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:80)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Thread.run(Thread.java:745)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 WARN Client: Exception encountered while connecting to the server : ', 'logdata': ''}\n",
      "{'ClassMessage': 'org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:595)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:762)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:758)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.security.AccessController.doPrivileged(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat javax.security.auth.Subject.doAs(Subject.java:422)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1865)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:757)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1620)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:816)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Method.invoke(Method.java:498)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2158)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1423)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1419)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$sparkJar(Client.scala:1262)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.Client$.populateClasspath(Client.scala:1397)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.prepareEnvironment(ExecutorRunnable.scala:290)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env$lzycompute(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.env(ExecutorRunnable.scala:61)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:80)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Thread.run(Thread.java:745)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 WARN RetryInvocationHandler: A failover has occurred since the start of ClientNamenodeProtocolTranslatorPB.getFileInfo over awdex01123.merckgroup.com/172.31.151.16:8020', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: ', 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'YARN executor launch context:', 'logdata': ''}\n",
      "{'ClassMessage': '  env:', 'logdata': ''}\n",
      "{'ClassMessage': '    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDERR -> https://awdex01130.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000002/s112788/stderr?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1580556634479_18136', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 142504', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_USER -> s112788', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581509844696', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_MODE -> true', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDOUT -> https://awdex01130.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000002/s112788/stdout?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES -> hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': '  command:', 'logdata': ''}\n",
      "{'ClassMessage': \"    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=44088' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.112:44088 --executor-id 1 --hostname awdex01130.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\", 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': '      ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ExecutorRunnable: ', 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'YARN executor launch context:', 'logdata': ''}\n",
      "{'ClassMessage': '  env:', 'logdata': ''}\n",
      "{'ClassMessage': '    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDERR -> https://awdex01126.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000003/s112788/stderr?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1580556634479_18136', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 142504', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_USER -> s112788', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581509844696', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_MODE -> true', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_LOG_URL_STDOUT -> https://awdex01126.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000003/s112788/stdout?start=-4096', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_FILES -> hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': '  command:', 'logdata': ''}\n",
      "{'ClassMessage': \"    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=44088' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.112:44088 --executor-id 2 --hostname awdex01126.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\", 'logdata': ''}\n",
      "{'ClassMessage': '===============================================================================', 'logdata': ''}\n",
      "{'ClassMessage': '      ', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01126.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:55 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01130.merckgroup.com:45454', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01130.merckgroup.com:55995) with ID 1', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO BlockManagerMasterEndpoint: Registering block manager awdex01130.merckgroup.com:44901 with 511.1 MB RAM, BlockManagerId(1, awdex01130.merckgroup.com, 44901)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01126.merckgroup.com:56018) with ID 2', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO BlockManagerMasterEndpoint: Registering block manager awdex01126.merckgroup.com:36748 with 511.1 MB RAM, BlockManagerId(2, awdex01126.merckgroup.com, 36748)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO PipelineConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO FilesystemAccess$: Max Pipeline Run ID found:21', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO InputConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO PrepareTrackwiseOLDL_AuditExport$: Checking input path hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO EnvironmentConf: Configuration file loaded hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:04 INFO AtlasRepository: Atlas URLs provided for this pipeline: https://awdex01123.merckgroup.com:21443/api/atlas/,https://awdex01124.merckgroup.com:21443/api/atlas/', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:15 INFO AtlasRepository: response: HttpResponse({\"requestId\":\"qtp1489092624-3691154 - 0e2efb68-eefd-4d87-b44a-35602a3e06fb\",\"definition\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"f45e0423-5f62-4c4b-b6c4-5fcd13154b89\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\\\"* * * * *\\\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs:\\\\/\\\\/aagxp\\\\/mc_staging\\\\/rdq_uc7_files\\\\/trackwise_oldl_auditexport\\\\/00_00_01\\\\/run_000021\\\\/raw\\\\/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aagxp_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000021_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-12T12:16:43.402Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-12T12:16:43.992Z\",\"geoScope\":\"NA\",\"owner\":\"s112788\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: fep-conf-prod.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}},200,Map(Content-Type -> Vector(application/json; charset=UTF-8), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=5r0zau7q2z2v1c7d6icy020ln;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 200 OK), Transfer-Encoding -> Vector(chunked), X-Frame-Options -> Vector(DENY)))', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:15 INFO AtlasRepository: Atlas request to https://awdex01123.merckgroup.com:21443/api/atlas/entities succeeded.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:15 INFO MetadataAccess: getEntityByFileLocation: {\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"f45e0423-5f62-4c4b-b6c4-5fcd13154b89\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\\\"* * * * *\\\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aagxp_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000021_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-12T12:16:43.402Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-12T12:16:43.992Z\",\"geoScope\":\"NA\",\"owner\":\"s112788\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: fep-conf-prod.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:15 INFO PrepareTrackwiseOLDL_AuditExport$: Excel Input path:hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000021/raw/TrackwiseOLDL_AuditExport/Audit_Export_Report_01_20200203_v3__m271552_2020-02-12T13-02-31+0100.xls', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO PrepareTrackwiseOLDL_AuditExport$: Expected sheet Name: Sheet1', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 ERROR ApplicationMaster: User class threw exception: java.lang.reflect.InvocationTargetException', 'logdata': ''}\n",
      "{'ClassMessage': 'java.lang.reflect.InvocationTargetException', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate$$anonfun$mapRowToObject$1.apply(RdqTransformationTemplate.scala:202)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate$$anonfun$mapRowToObject$1.apply(RdqTransformationTemplate.scala:199)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.List.foreach(List.scala:318)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.mapRowToObject(RdqTransformationTemplate.scala:198)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport$.createDataframeFromExcelWorkbook(PrepareTrackwiseOLDL_AuditExport.scala:25)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.doSpecificTransformation(RdqTransformationTemplate.scala:171)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.pipelinecore.TransformationStepTemplate$class.executeTransformationStep(TransformationStepTemplate.scala:42)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.executeTransformationStep(RdqTransformationTemplate.scala:25)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.main(RdqTransformationTemplate.scala:241)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport.main(PrepareTrackwiseOLDL_AuditExport.scala)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.reflect.Method.invoke(Method.java:498)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:561)', 'logdata': ''}\n",
      "{'ClassMessage': 'Caused by: java.lang.NumberFormatException: For input string: \"\"', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Integer.parseInt(Integer.java:592)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat java.lang.Integer.parseInt(Integer.java:615)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:31)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.util.ParseRow$.getIntValue(ParseRow.scala:67)', 'logdata': ''}\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.schemas.TrackwiseOLDL_AuditExport.<init>(TrackwiseOLDL_AuditExport.scala:120)', 'logdata': ''}\n",
      "{'ClassMessage': '\\t... 23 more', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.reflect.InvocationTargetException)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO SparkContext: Invoking stop() from shutdown hook', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO SparkUI: Stopped Spark web UI at http://172.31.151.112:44105', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO YarnAllocator: Driver requested a total number of 0 executor(s).', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO YarnClusterSchedulerBackend: Shutting down all executors', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO YarnClusterSchedulerBackend: Asking each executor to shut down', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices', 'logdata': ''}\n",
      "{'ClassMessage': '(serviceOption=None,', 'logdata': ''}\n",
      "{'ClassMessage': ' services=List(),', 'logdata': ''}\n",
      "{'ClassMessage': ' started=false)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO BlockManagerMaster: BlockManagerMaster stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO SparkContext: Successfully stopped SparkContext', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.reflect.InvocationTargetException)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ApplicationMaster: Deleting staging directory .sparkStaging/application_1580556634479_18136', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-8feba5d9-dc6a-467d-afd8-ec9a1d1112c0', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-d87ae389-dddd-40fa-8153-b3f3bbcd679f', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-a9a0b00f-912f-43dc-9787-9f27c2f85d57', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-a20ba93d-0a6b-4eee-aaa5-eb3fb61cbe86', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-b00ffb68-3539-4ecc-8c26-7a2876d09351', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:17 INFO ShutdownHookManager: Deleting directory /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/spark-5487a0f7-cc0d-416f-9793-f2b27d3743cb', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'Container: container_e151_1580556634479_18136_02_000002 on awdex01130.merckgroup.com_45454_1581509898487', 'logdata': ''}\n",
      "{'ClassMessage': '========================================================================================================', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:14151', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l:', 'logdata': ''}\n",
      "{'ClassMessage': 'total 152', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    78 Feb 12 13:17 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112788/filecache/161/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 __spark__.jar -> /data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    94 Feb 12 13:17 __spark_conf__ -> /data6/hadoop/yarn/local/usercache/s112788/filecache/160/__spark_conf__3254237861647914825.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 aws-java-sdk-core-1.10.6.jar -> /data6/hadoop/yarn/local/filecache/416/aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 aws-java-sdk-kms-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/419/aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 aws-java-sdk-s3-1.10.6.jar -> /data4/hadoop/yarn/local/filecache/427/aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 azure-keyvault-core-0.8.0.jar -> /data5/hadoop/yarn/local/filecache/429/azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    62 Feb 12 13:17 azure-storage-4.2.0.jar -> /data6/hadoop/yarn/local/filecache/435/azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 commons-collections4-4.1.jar -> /data6/hadoop/yarn/local/filecache/2698/commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    59 Feb 12 13:17 commons-csv-1.1.jar -> /data6/hadoop/yarn/local/filecache/2697/commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    60 Feb 12 13:17 commons-lang3-3.4.jar -> /data4/hadoop/yarn/local/filecache/437/commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rw------- 1 s112788 hadoop  1016 Feb 12 13:17 container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 datanucleus-api-jdo-3.2.6.jar -> /data5/hadoop/yarn/local/filecache/438/datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-core-3.2.10.jar -> /data4/hadoop/yarn/local/filecache/423/datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 datanucleus-rdbms-3.2.9.jar -> /data3/hadoop/yarn/local/filecache/420/datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/432/guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/2690/gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    71 Feb 12 13:17 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/428/hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    73 Feb 12 13:17 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/431/hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 hive-site.xml -> /data2/hadoop/yarn/local/filecache/425/hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    68 Feb 12 13:17 jackson-annotations-2.4.0.jar -> /data5/hadoop/yarn/local/filecache/421/jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 jackson-core-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/422/jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    65 Feb 12 13:17 jackson-databind-2.4.4.jar -> /data4/hadoop/yarn/local/filecache/430/jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    56 Feb 12 13:17 joda-time-2.5.jar -> /data4/hadoop/yarn/local/filecache/434/joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 json-simple-1.1.jar -> /data5/hadoop/yarn/local/filecache/439/json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '-rwx------ 1 s112788 hadoop 22393 Feb 12 13:17 launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 lift-json_2.10-2.6.3.jar -> /data2/hadoop/yarn/local/filecache/2701/lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 ojdbc6.jar -> /data4/hadoop/yarn/local/filecache/2699/ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    88 Feb 12 13:17 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data5/hadoop/yarn/local/filecache/436/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/433/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    81 Feb 12 13:17 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data4/hadoop/yarn/local/filecache/417/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    52 Feb 12 13:17 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/2696/poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 poi-ooxml-3.17.jar -> /data2/hadoop/yarn/local/filecache/3760/poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/2691/poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    55 Feb 12 13:17 py4j-0.9-src.zip -> /data5/hadoop/yarn/local/filecache/415/py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    50 Feb 12 13:17 pyspark.zip -> /data6/hadoop/yarn/local/filecache/426/pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    61 Feb 12 13:17 qd_rdq_2.10-1.0.1.jar -> /data5/hadoop/yarn/local/filecache/3759/qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    63 Feb 12 13:17 scala-library-2.10.5.jar -> /data4/hadoop/yarn/local/filecache/424/scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    66 Feb 12 13:17 scalaj-http_2.10-2.3.0.jar -> /data3/hadoop/yarn/local/filecache/2688/scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    99 Feb 12 13:17 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    64 Feb 12 13:17 spark-csv_2.10-1.5.0.jar -> /data3/hadoop/yarn/local/filecache/2700/spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'drwxr-s--- 2 s112788 hadoop  4096 Feb 12 13:17 tmp', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    67 Feb 12 13:17 univocity-parsers-1.5.1.jar -> /data3/hadoop/yarn/local/filecache/2693/univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'lrwxrwxrwx 1 s112788 hadoop    58 Feb 12 13:17 xmlbeans-2.6.0.jar -> /data5/hadoop/yarn/local/filecache/2692/xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls:', 'logdata': ''}\n",
      "{'ClassMessage': '40919553    4 drwxr-s---   3 s112788  hadoop       4096 Feb 12 13:17 .', 'logdata': ''}\n",
      "{'ClassMessage': '26961314  260 -r-xr-xr-x   1 yarn     hadoop     258578 Aug 14  2018 ./aws-java-sdk-kms-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796718  348 -r-xr-xr-x   1 yarn     hadoop     349304 Apr 25  2019 ./gxppipelinecore_2.10-2.0.13.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40919555    4 drwxr-s---   2 s112788  hadoop       4096 Feb 12 13:17 ./tmp', 'logdata': ''}\n",
      "{'ClassMessage': '3261374 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Aug 14  2018 ./guava-11.0.2.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796722 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Apr 25  2019 ./poi-ooxml-schemas-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48587725 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Aug 14  2018 ./datanucleus-rdbms-3.2.9.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961320 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Aug 14  2018 ./datanucleus-core-3.2.10.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961339  432 -r-xr-xr-x   1 yarn     hadoop     434678 Aug 14  2018 ./commons-lang3-3.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900951  740 -r-xr-xr-x   1 yarn     hadoop     751238 Apr 25  2019 ./commons-collections4-4.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756330  216 -r-xr-xr-x   1 yarn     hadoop     213154 Aug 14  2018 ./hadoop-azure-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644566   44 -r-xr-xr-x   1 yarn     hadoop      44846 Aug 14  2018 ./py4j-0.9-src.zip', 'logdata': ''}\n",
      "{'ClassMessage': '48636393  164 -r-xr-xr-x   1 yarn     hadoop     162717 Apr 25  2019 ./scalaj-http_2.10-2.3.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '41017597 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Jun 19  2019 ./poi-ooxml-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900727    4 drwx------   2 s112788  ldapuser     4096 Feb 12 13:17 ./__spark_conf__', 'logdata': ''}\n",
      "{'ClassMessage': '58900801    4 -r-x------   1 s112788  ldapuser     2316 Feb 12 13:17 ./__spark_conf__/ssl-client.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '58900760   12 -r-x------   1 s112788  ldapuser     8606 Feb 12 13:17 ./__spark_conf__/mapred-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900795    4 -r-x------   1 s112788  ldapuser      234 Feb 12 13:17 ./__spark_conf__/yarn_jaas.conf', 'logdata': ''}\n",
      "{'ClassMessage': '58900819    4 -r-x------   1 s112788  ldapuser     1027 Feb 12 13:17 ./__spark_conf__/ssl-server.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900797    4 -r-x------   1 s112788  ldapuser     2490 Feb 12 13:17 ./__spark_conf__/hadoop-metrics.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900764    4 -r-x------   1 s112788  ldapuser     3979 Feb 12 13:17 ./__spark_conf__/hadoop-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900793    4 -r-x------   1 s112788  ldapuser     1020 Feb 12 13:17 ./__spark_conf__/commons-logging.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900776    4 -r-x------   1 s112788  ldapuser     3518 Feb 12 13:17 ./__spark_conf__/kms-acls.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900799    4 -r-x------   1 s112788  ldapuser      661 Feb 12 13:17 ./__spark_conf__/mapred-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900816    4 -r-x------   1 s112788  ldapuser      131 Feb 12 13:17 ./__spark_conf__/slaves', 'logdata': ''}\n",
      "{'ClassMessage': '58900768    4 -r-x------   1 s112788  ldapuser     2250 Feb 12 13:17 ./__spark_conf__/yarn-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900740   12 -r-x------   1 s112788  ldapuser    10457 Feb 12 13:17 ./__spark_conf__/log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900771    8 -r-x------   1 s112788  ldapuser     5238 Feb 12 13:17 ./__spark_conf__/capacity-scheduler.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900813    4 -r-x------   1 s112788  ldapuser     1308 Feb 12 13:17 ./__spark_conf__/hadoop-policy.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900789    8 -r-x------   1 s112788  ldapuser     5434 Feb 12 13:17 ./__spark_conf__/yarn-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900823    4 -r-x------   1 s112788  ldapuser      945 Feb 12 13:17 ./__spark_conf__/taskcontroller.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '58900766    4 -r-x------   1 s112788  ldapuser     1631 Feb 12 13:17 ./__spark_conf__/kms-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900800    4 -r-x------   1 s112788  ldapuser     1602 Feb 12 13:17 ./__spark_conf__/health_check', 'logdata': ''}\n",
      "{'ClassMessage': '58900817    4 -r-x------   1 s112788  ldapuser      362 Feb 12 13:17 ./__spark_conf__/topology_mappings.data', 'logdata': ''}\n",
      "{'ClassMessage': '58900821    8 -r-x------   1 s112788  ldapuser     5511 Feb 12 13:17 ./__spark_conf__/kms-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900824    4 -r-x------   1 s112788  ldapuser      721 Feb 12 13:17 ./__spark_conf__/__spark_conf__.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900765   20 -r-x------   1 s112788  ldapuser    16737 Feb 12 13:17 ./__spark_conf__/core-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900782    4 -r-x------   1 s112788  ldapuser     2358 Feb 12 13:17 ./__spark_conf__/topology_script.py', 'logdata': ''}\n",
      "{'ClassMessage': '58900763   24 -r-x------   1 s112788  ldapuser    23546 Feb 12 13:17 ./__spark_conf__/yarn-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900790   12 -r-x------   1 s112788  ldapuser    10455 Feb 12 13:17 ./__spark_conf__/hdfs-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900743    8 -r-x------   1 s112788  ldapuser     5367 Feb 12 13:17 ./__spark_conf__/hadoop-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900818    4 -r-x------   1 s112788  ldapuser      951 Feb 12 13:17 ./__spark_conf__/mapred-env.cmd', 'logdata': ''}\n",
      "{'ClassMessage': '58900822    8 -r-x------   1 s112788  ldapuser     4113 Feb 12 13:17 ./__spark_conf__/mapred-queues.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '58900812    4 -r-x------   1 s112788  ldapuser     1527 Feb 12 13:17 ./__spark_conf__/kms-env.sh', 'logdata': ''}\n",
      "{'ClassMessage': '58900796    8 -r-x------   1 s112788  ldapuser     4221 Feb 12 13:17 ./__spark_conf__/task-log4j.properties', 'logdata': ''}\n",
      "{'ClassMessage': '58900788    4 -r-x------   1 s112788  ldapuser     1335 Feb 12 13:17 ./__spark_conf__/configuration.xsl', 'logdata': ''}\n",
      "{'ClassMessage': '58900794    4 -r-x------   1 s112788  ldapuser     1270 Feb 12 13:17 ./__spark_conf__/container-executor.cfg', 'logdata': ''}\n",
      "{'ClassMessage': '58900770    4 -r-x------   1 s112788  ldapuser      902 Feb 12 13:17 ./__spark_conf__/ssl-client.xml', 'logdata': ''}\n",
      "{'ClassMessage': '58900820    4 -r-x------   1 s112788  ldapuser     2697 Feb 12 13:17 ./__spark_conf__/ssl-server.xml.example', 'logdata': ''}\n",
      "{'ClassMessage': '58900777    4 -r-x------   1 s112788  ldapuser      758 Feb 12 13:17 ./__spark_conf__/mapred-site.xml.template', 'logdata': ''}\n",
      "{'ClassMessage': '58900761    4 -r-x------   1 s112788  ldapuser     2084 Feb 12 13:17 ./__spark_conf__/hadoop-metrics2.properties', 'logdata': ''}\n",
      "{'ClassMessage': '26961329  168 -r-xr-xr-x   1 yarn     hadoop     165879 Aug 14  2018 ./hadoop-aws-2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '7209420  228 -r-xr-xr-x   1 yarn     hadoop     226987 Jun 19  2019 ./qd_rdq_2.10-1.0.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756334   56 -r-xr-xr-x   1 yarn     hadoop      52413 Aug 14  2018 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961310   24 -r-xr-xr-x   1 yarn     hadoop      22715 Aug 14  2018 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '3358875  228 -r-x------   1 s112788  ldapuser   226987 Feb 12 13:17 ./__app__.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796728  480 -r-xr-xr-x   1 yarn     hadoop     486892 Apr 25  2019 ./lift-json_2.10-2.6.3.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40919557    4 -rw-------   1 s112788  hadoop       1016 Feb 12 13:17 ./container_tokens', 'logdata': ''}\n",
      "{'ClassMessage': '27001379 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Apr 25  2019 ./ojdbc6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961322  564 -r-xr-xr-x   1 yarn     hadoop     570101 Aug 14  2018 ./aws-java-sdk-s3-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40756327    4 -r-xr-xr-x   1 yarn     hadoop       1912 Aug 14  2018 ./hive-site.xml', 'logdata': ''}\n",
      "{'ClassMessage': '26961324 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Aug 14  2018 ./scala-library-2.10.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40796725 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Apr 25  2019 ./poi-3.17.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58900948   40 -r-xr-xr-x   1 yarn     hadoop      36888 Apr 25  2019 ./commons-csv-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961335  580 -r-xr-xr-x   1 yarn     hadoop     588001 Aug 14  2018 ./joda-time-2.5.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961316  228 -r-xr-xr-x   1 yarn     hadoop     225302 Aug 14  2018 ./jackson-core-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58902209  356 -r-xr-xr-x   1 yarn     hadoop     357604 Aug 14  2018 ./pyspark.zip', 'logdata': ''}\n",
      "{'ClassMessage': '6644706   16 -r-xr-xr-x   1 yarn     hadoop      12749 Aug 14  2018 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644714   16 -r-xr-xr-x   1 yarn     hadoop      16046 Aug 14  2018 ./json-simple-1.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48636419  168 -r-xr-xr-x   1 yarn     hadoop     165361 Apr 25  2019 ./spark-csv_2.10-1.5.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40755293 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Feb  7  2019 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar', 'logdata': ''}\n",
      "{'ClassMessage': '40919556   24 -rwx------   1 s112788  hadoop      22393 Feb 12 13:17 ./launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '6791659 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Apr 25  2019 ./xmlbeans-2.6.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58902291  732 -r-xr-xr-x   1 yarn     hadoop     745325 Aug 14  2018 ./azure-storage-4.2.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '26961332 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Aug 14  2018 ./jackson-databind-2.4.4.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644702   12 -r-xr-xr-x   1 yarn     hadoop      10092 Aug 14  2018 ./azure-keyvault-core-0.8.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644699   40 -r-xr-xr-x   1 yarn     hadoop      38605 Aug 14  2018 ./jackson-annotations-2.4.0.jar', 'logdata': ''}\n",
      "{'ClassMessage': '48636397  152 -r-xr-xr-x   1 yarn     hadoop     148962 Apr 25  2019 ./univocity-parsers-1.5.1.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58861338  508 -r-xr-xr-x   1 yarn     hadoop     516062 Aug 14  2018 ./aws-java-sdk-core-1.10.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '6644710  336 -r-xr-xr-x   1 yarn     hadoop     339666 Aug 14  2018 ./datanucleus-api-jdo-3.2.6.jar', 'logdata': ''}\n",
      "{'ClassMessage': '58859666 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Aug 21  2017 ./__spark__.jar', 'logdata': ''}\n",
      "{'ClassMessage': 'broken symlinks(find -L . -maxdepth 5 -type l -ls):', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:directory.info', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:22393', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '#!/bin/bash', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES=\"hdfs://aagxp/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/qd_rdq_2.10-1.0.1.jar#__app__.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1.jar#qd_rdq_2.10-1.0.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aagxp/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/pyspark.zip#pyspark.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/spark/hive-site.xml#hive-site.xml,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aagxp/user/oozie/share/lib/lib_20180814140000/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDOUT=\"https://awdex01130.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000002/s112788/stdout?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HOST=\"awdex01130.merckgroup.com\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190578782,226987,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,226987,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1912,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581509844696\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOGNAME=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export JVM_PID=\"$$\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export PWD=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136,/data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_HTTP_PORT=\"8044\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002,/data2/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002,/data3/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002,/data4/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002,/data5/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002,/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=', 'logdata': ''}\n",
      "{'ClassMessage': '\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_PORT=\"45454\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1502794813190,1581509843405,1581509843501,1581509843626,1581509843719,1581509843835,1581509844007,1581509844067,1581509844132,1581509844192,1556198495038,1556198495093,1556198495170,1556198495246,1556198495346,1556198495466,1556198495553,1556198495732,1556198981014,1556198495792,1556198495843,1556198495906,1556198496014,1534249935160,1534249923167,1534249913882,1534249920815,1534249910268,1534249912111,1534249918566,1534249916343,1534249584569,1534249587039,1534249589424,1534249591635,1534249593898,1534249596609,1534249599002,1534249600776,1534249602504,1534249605276,1534249608400,1534249610121,1534249612403,1534249614752,1534249617009,1534249620408,1534250486996\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}', 'logdata': ''}\n",
      "{'ClassMessage': 'export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aagxp/user/s112788/.sparkStaging/application_1580556634479_18136/__spark_conf__3254237861647914825.zip#__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"142504\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_MODE=\"true\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/container_tokens\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_USER=\"s112788\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export LOCAL_USER_DIRS=\"/data1/hadoop/yarn/local/usercache/s112788/,/data2/hadoop/yarn/local/usercache/s112788/,/data3/hadoop/yarn/local/usercache/s112788/,/data4/hadoop/yarn/local/usercache/s112788/,/data5/hadoop/yarn/local/usercache/s112788/,/data6/hadoop/yarn/local/usercache/s112788/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_LOG_URL_STDERR=\"https://awdex01130.merckgroup.com:8044/node/containerlogs/container_e151_1580556634479_18136_02_000002/s112788/stderr?start=-4096\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export HOME=\"/home/\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export NM_AUX_SERVICE_spark2_shuffle=\"\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export CONTAINER_ID=\"container_e151_1580556634479_18136_02_000002\"', 'logdata': ''}\n",
      "{'ClassMessage': 'export MALLOC_ARENA_MAX=\"4\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/420/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/434/joda-time-2.5.jar\" \"joda-time-2.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/417/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/430/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/2692/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2700/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/2697/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/438/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/437/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/426/pyspark.zip\" \"pyspark.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/428/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2691/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/427/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"', 'logdata': ''}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/423/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2693/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/436/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data3/hadoop/yarn/local/filecache/2688/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/433/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/416/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/usercache/s112788/filecache/161/qd_rdq_2.10-1.0.1.jar\" \"__app__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar\" \"__spark__.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data1/hadoop/yarn/local/filecache/432/guava-11.0.2.jar\" \"guava-11.0.2.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/422/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/425/hive-site.xml\" \"hive-site.xml\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/429/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2701/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/421/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/usercache/s112788/filecache/160/__spark_conf__3254237861647914825.zip\" \"__spark_conf__\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/3760/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/419/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2696/poi-3.17.jar\" \"poi-3.17.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/415/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/424/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/435/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/431/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/3759/qd_rdq_2.10-1.0.1.jar\" \"qd_rdq_2.10-1.0.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data4/hadoop/yarn/local/filecache/2699/ojdbc6.jar\" \"ojdbc6.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data6/hadoop/yarn/local/filecache/2698/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data5/hadoop/yarn/local/filecache/439/json-simple-1.1.jar\" \"json-simple-1.1.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': 'ln -sf \"/data2/hadoop/yarn/local/filecache/2690/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '# Creating copy of launch script', 'logdata': ''}\n",
      "{'ClassMessage': 'cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': 'chmod 640 \"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/launch_container.sh\"', 'logdata': ''}\n",
      "{'ClassMessage': '# Determining directory contents', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'ls -l 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/directory.info\"', 'logdata': ''}\n",
      "{'ClassMessage': 'exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError=\\'kill %p\\' -Xms1024m -Xmx1024m \\'-Dlog4j.configuration=spark-log4j.properties\\' -Djava.io.tmpdir=$PWD/tmp \\'-Dspark.ui.port=0\\' \\'-Dspark.driver.port=44088\\' -Dspark.yarn.app.container.log.dir=/data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.151.112:44088 --executor-id 1 --hostname awdex01130.merckgroup.com --cores 1 --app-id application_1580556634479_18136 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/stdout 2> /data6/hadoop/yarn/log/application_1580556634479_18136/container_e151_1580556634479_18136_02_000002/stderr\"', 'logdata': ''}\n",
      "{'ClassMessage': 'hadoop_shell_errorcode=$?', 'logdata': ''}\n",
      "{'ClassMessage': 'if [ $hadoop_shell_errorcode -ne 0 ]', 'logdata': ''}\n",
      "{'ClassMessage': 'then', 'logdata': ''}\n",
      "{'ClassMessage': '  exit $hadoop_shell_errorcode', 'logdata': ''}\n",
      "{'ClassMessage': 'fi', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:launch_container.sh', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:4497', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\", 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Class path contains multiple SLF4J bindings.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/807/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/data6/hadoop/yarn/local/filecache/11/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.', 'logdata': ''}\n",
      "{'ClassMessage': 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:56 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:56 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:56 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:57 INFO SecurityManager: Changing view acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:57 INFO SecurityManager: Changing modify acls to: s112788', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112788); users with modify permissions: Set(s112788)', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:57 INFO Slf4jLogger: Slf4jLogger started', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:57 INFO Remoting: Starting remoting', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01130.merckgroup.com:44657]', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:58 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 44657.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-1f30a127-cf50-4254-8d54-957dd1892d8f', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-c0d26899-28b4-48f5-b9f8-01eefa209b01', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data3/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-31b0b6e6-d53a-4c5a-8441-d0e030da5e01', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data4/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-34b9c9c5-ee24-4df8-bc3d-9990410bf552', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data5/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-f3c0791c-9fc7-4f11-b6c9-ddaa53d3c59f', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO DiskBlockManager: Created local directory at /data6/hadoop/yarn/local/usercache/s112788/appcache/application_1580556634479_18136/blockmgr-27f0ba65-e567-46e4-932a-fd30f19c8608', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO MemoryStore: MemoryStore started with capacity 511.1 MB', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.151.112:44088', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO CoarseGrainedExecutorBackend: Successfully registered with driver', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO Executor: Starting executor ID 1 on host awdex01130.merckgroup.com', 'logdata': ''}\n",
      "{'ClassMessage': \"20/02/12 13:17:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44901.\", 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO NettyBlockTransferService: Server created on 44901', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO BlockManagerMaster: Trying to register BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:17:58 INFO BlockManagerMaster: Registered BlockManager', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO MemoryStore: MemoryStore cleared', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO BlockManager: BlockManager stopped', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver from awdex01130.merckgroup.com:44088 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO CoarseGrainedExecutorBackend: Driver from 172.31.151.112:44088 disconnected during shutdown', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.', 'logdata': ''}\n",
      "{'ClassMessage': '20/02/12 13:18:16 INFO ShutdownHookManager: Shutdown hook called', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stderr', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Upload Time:Wed Feb 12 13:18:18 +0100 2020', 'logdata': ''}\n",
      "{'ClassMessage': 'LogLength:0', 'logdata': ''}\n",
      "{'ClassMessage': 'Log Contents:', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n",
      "{'ClassMessage': 'End of LogType:stdout', 'logdata': ''}\n",
      "{'ClassMessage': '', 'logdata': ''}\n"
     ]
    }
   ],
   "source": [
    "filename = \"application2.log\"\n",
    "#lg = LineGrokker('%{LOGLEVEL:level}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{JAVALOGMESSAGE:ClassMessage}%{GREEDYDATA:logdata}', pr)\n",
    "import csv\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        cline=lg4.grok(line)\n",
    "        print(cline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "{'ClassMessage': '\\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate$$anonfun$mapRowToObject$1.apply(RdqTransformationTemplate.scala:202)', 'logdata': ''}\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "#filename = \"application2.log\"\n",
    "#filename = \"2020_02_29-application_1580556634479_40162.log\"\n",
    "import csv\n",
    "outcsv = open(\"testoutput6.csv\", \"w\")\n",
    "filename = \"testlog.txt\"\n",
    "\n",
    "#outF = open(\"testoutput\", \"w\")\n",
    "lg = LineGrokker('%{LOGLEVEL:level}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{JAVALOGMESSAGE:ClassMessage}%{GREEDYDATA:logdata}', pr)\n",
    "\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)\n",
    "#java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "pline=[]\n",
    "cline={}\n",
    "\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        #print(lg.grok(line))\n",
    "        cline=lg.grok(line)\n",
    "        if (cline is not None) and len(pline)==0:\n",
    "#            print(cline)\n",
    "#            outF.write(str(cline))\n",
    "#            outF.write(\"\\n\")\n",
    "            #pline=[]\n",
    "            print(1)\n",
    "            if cline['level']=='ERROR':\n",
    "                pline.append(cline)\n",
    "            else:\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "#                outF.write(str(cline))\n",
    "#                outF.write(\"\\n\")\n",
    "            continue\n",
    "#        elif (cline is None) and len(pline)!=0:\n",
    "#            cline=lg2.grok(line)\n",
    "#            if cline is not None:\n",
    "                #print(cline)\n",
    "                #print(2)\n",
    "#                pline.append(cline)\n",
    "#                continue\n",
    "#            cline=lg3.grok(line)\n",
    "#            if cline is not None:\n",
    "                #print(cline)\n",
    "                #print(3)\n",
    "#                pline.append(cline)                \n",
    "#            cline=lg4.grok(line)\n",
    "#            if cline is not None:\n",
    "#                print(cline)\n",
    "#                print(4)\n",
    "#                pline.append(cline)\n",
    "        elif (cline is not None) and len(pline)!=0:\n",
    "            cline=lg4.grok(line)\n",
    "            if cline is not None:\n",
    "                print(cline)\n",
    "#                print(4)\n",
    "                pline.append(cline)        \n",
    "            for indx in pline:\n",
    "                for i in indx.keys():\n",
    "                    outcsv.write(\"%s,\" % (indx[i]))\n",
    "            outcsv.write(\"\\n\")\n",
    "            for i in cline.keys():\n",
    "                outcsv.write(\"%s,\" % (cline[i]))\n",
    "            outcsv.write(\"\\n\")\n",
    "#            outF.write(str(pline))\n",
    "#            outF.write(\"\\n\")\n",
    "#            outF.write(str(cline))\n",
    "#            outF.write(\"\\n\")\n",
    "            pline=[]\n",
    "#        print(pline)\n",
    "        \n",
    "#outF.close()\n",
    "outcsv.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working code for : 20/02/12 13:18:48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "import re\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "#filename = \"application2.log\"\n",
    "filename = r\"C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\data\\raw_logs\\application_1568810042014_190518.log\"\n",
    "\n",
    "import csv\n",
    "outcsv = open(\"application_1584006084041_0213.csv\", \"w\")\n",
    "#filename = \"testlog.txt\"\n",
    "\n",
    "#outF = open(\"testoutput\", \"w\")\n",
    "lg = LineGrokker('%{LOGLEVEL:level}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{GREEDYDATA:logdata}', pr)\n",
    "pline=[]\n",
    "cline={}\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "#------------------------------------------------------------        \n",
    "        lineFormat = re.search(r'^(\\d+/\\d+/\\d+)',line)\n",
    "        if lineFormat != None:\n",
    "            cline=lg.grok(line)\n",
    "        else:\n",
    "            lg = LineGrokker('%{LOGLEVEL:level} %{SPACE}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "            cline=lg.grok(line)\n",
    "#------------------------------------------------------------        \n",
    "        #cline=lg.grok(line)\n",
    "        if (cline is not None) and len(pline)==0:         \n",
    "            if cline['level']=='ERROR':\n",
    "                pline.append(cline)\n",
    "            elif cline['level']=='er':\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "            else:\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "        elif (cline is not None) and len(pline)!=0:         \n",
    "            if (cline['level']=='er'):\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "            elif (cline['level']=='INFO'):\n",
    "                for indx in pline:\n",
    "                    for i in indx.keys():\n",
    "                        outcsv.write(\"%s \" % (indx[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                pline=[]                \n",
    "#            else:\n",
    "#                print(cline)\n",
    "#                for i in cline.keys():\n",
    "#                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "#                outcsv.write(\"2\\n\")\n",
    "        elif (cline is None) and len(pline)!=0:         \n",
    "            cline=lg4.grok(line)\n",
    "            pline.append(cline)\n",
    "outcsv.close()            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working code for :2020-01-07 13:06:03,622 [Driver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "#filename = \"application2.log\"\n",
    "#filename = r\"C:\\project\\uc7\\Issues\\application_1576136188037_14643.log\"\n",
    "filename = r\"C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\data\\raw_logs\\1application_1568810042014_190726.log\"\n",
    "\n",
    "import csv\n",
    "outcsv = open(\"1application_1568810042014_190726.csv\", \"w\")\n",
    "#filename = \"testlog.txt\"\n",
    "\n",
    "#outF = open(\"testoutput\", \"w\")\n",
    "lg = LineGrokker('%{LOGLEVEL:level} %{SPACE}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{GREEDYDATA:logdata}', pr)\n",
    "pline=[]\n",
    "cline={}\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        cline=lg.grok(line)\n",
    "        if (cline is not None) and len(pline)==0:         \n",
    "            if cline['level']=='ERROR':\n",
    "                pline.append(cline)\n",
    "            elif cline['level']=='er':\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "            else:\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "        elif (cline is not None) and len(pline)!=0:         \n",
    "            if (cline['level']=='er'):\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "            elif (cline['level']=='INFO'):\n",
    "                for indx in pline:\n",
    "                    for i in indx.keys():\n",
    "                        outcsv.write(\"%s \" % (indx[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                pline=[]                \n",
    "#            else:\n",
    "#                print(cline)\n",
    "#                for i in cline.keys():\n",
    "#                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "#                outcsv.write(\"2\\n\")\n",
    "        elif (cline is None) and len(pline)!=0:         \n",
    "            cline=lg4.grok(line)\n",
    "            pline.append(cline)\n",
    "outcsv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#'%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}'\n",
    "lg = LineGrokker('%{LOGLEVEL:level} %{SPACE}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#print(lg.grok('''2020-03-12 20:09:17,951 [Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0) java.lang.RuntimeException: java.lang.NullPointerException: hive.llap.daemon.service.hosts must be defined'''))\n",
    "#20/02/27 04:09:10 ERROR ApplicationMaster: User class threw exception: com.merck.mcloud.gxp.pipelinecore.CustomExceptions.SqoopException: Sqoop import unsuccessful. Abort!\n",
    "print(lg.grok('''Caused by: java.lang.NumberFormatException: For input string: \"\"'''))\n",
    "#2020-01-07 13:06:03,146 [Driver] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef  - Registered StateStoreCoordinator endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "#filename = \"application2.log\"\n",
    "#filename = r\"C:\\project\\uc7\\Issues\\application_1576136188037_14643.log\"\n",
    "filename = r\"C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\log_files\\1application_1568810042014_190734.log\"\n",
    "\n",
    "\n",
    "import csv\n",
    "outcsv = open(\"1application_1568810042014_190734.csv\", \"w\")\n",
    "#filename = \"testlog.txt\"\n",
    "\n",
    "#outF = open(\"testoutput\", \"w\")\n",
    "lg = LineGrokker('%{LOGLEVEL:level} %{SPACE}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{GREEDYDATA:logdata}', pr)\n",
    "pline=[]\n",
    "cline={}\n",
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        cline=lg.grok(line)\n",
    "        if (cline is not None) and len(pline)==0:         \n",
    "            if cline['level']=='ERROR':\n",
    "                pline.append(cline)\n",
    "            elif cline['level']=='er':\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "                pass\n",
    "            else:\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s|\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "        elif (cline is not None) and len(pline)!=0:         \n",
    "            if (cline['level']=='er'):\n",
    "                cline=lg4.grok(line)\n",
    "                pline.append(cline)\n",
    "                pass\n",
    "            elif (cline['level']=='INFO'):\n",
    "                for indx in pline:\n",
    "                    for i in indx.keys():\n",
    "                        outcsv.write(\"%s \" % (indx[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                for i in cline.keys():\n",
    "                    outcsv.write(\"%s|\" % (cline[i]))\n",
    "                outcsv.write(\"\\n\")\n",
    "                pline=[]                \n",
    "#            else:\n",
    "#                print(cline)\n",
    "#                for i in cline.keys():\n",
    "#                    outcsv.write(\"%s,\" % (cline[i]))\n",
    "#                outcsv.write(\"2\\n\")\n",
    "        elif (cline is None) and len(pline)!=0:         \n",
    "            cline=lg4.grok(line)\n",
    "            pline.append(cline)\n",
    "outcsv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileReadFormat="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korg import LineGrokker, PatternRepo\n",
    "pr = PatternRepo()  # use the std. logstash grok patterns\n",
    "#filename = \"application1.log\"\n",
    "#filename = \"application2.log\"\n",
    "filename = r\"C:\\Users\\sreddy\\OneDrive - MerckGroup\\New folder\\data\\raw_logs\\application_1568810042014_190518.log\"\n",
    "\n",
    "import csv\n",
    "outcsv = open(\"application_1584006084041_0213.csv\", \"w\")\n",
    "#filename = \"testlog.txt\"\n",
    "\n",
    "#outF = open(\"testoutput\", \"w\")\n",
    "lg = LineGrokker('%{LOGLEVEL:level}%{JAVALOGMESSAGE:JavaMessage}%{GREEDYDATA:logdata}', pr)\n",
    "#lg = LineGrokker('%{TIMESTAMP_ISO8601:timestmp} %{SYSLOG5424SD:prg} %{LOGLEVEL:level}%{SPACE}%{JAVACLASS:ClassName} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg2 = LineGrokker('%{DATA:ClassName} %{DATA:ClassName2} %{JAVALOGMESSAGE:JavaMessage} %{GREEDYDATA:logdata}', pr)\n",
    "lg3 = LineGrokker('%{JAVASTACKTRACEPART:JavaStackTrace}%{GREEDYDATA:logdata}', pr)\n",
    "lg4 = LineGrokker('%{GREEDYDATA:logdata}', pr)\n",
    "pline=[]\n",
    "cline={}\n",
    "#with open(filename, 'r') as filehandle:\n",
    "#    for line in filehandle:\n",
    "#        cline=lg.grok(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container: container_e175_1568810042014_190518_01_000003 on awdex01009.aws.merckcloud.com_45454_1581588577175\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14350\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 156\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:07 __app__.jar -> /data/hadoop/yarn/local/usercache/s112380/filecache/100/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 aws-java-sdk-core-1.10.6.jar -> /data/hadoop/yarn/local/filecache/39803/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 aws-java-sdk-kms-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/39809/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/39798/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:07 azure-keyvault-core-0.8.0.jar -> /data/hadoop/yarn/local/filecache/39800/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:07 azure-storage-4.2.0.jar -> /data/hadoop/yarn/local/filecache/39807/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:07 commons-collections4-4.1.jar -> /data1/hadoop/yarn/local/filecache/47354/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:07 commons-csv-1.1.jar -> /data/hadoop/yarn/local/filecache/47353/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:07 commons-lang3-3.4.jar -> /data1/hadoop/yarn/local/filecache/39811/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop  1024 Feb 13 10:07 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:07 datanucleus-api-jdo-3.2.6.jar -> /data1/hadoop/yarn/local/filecache/39826/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 datanucleus-core-3.2.10.jar -> /data1/hadoop/yarn/local/filecache/39822/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 datanucleus-rdbms-3.2.9.jar -> /data1/hadoop/yarn/local/filecache/39821/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:07 guava-11.0.2.jar -> /data2/hadoop/yarn/local/filecache/39804/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:07 gxppipelinecore_2.10-2.0.13.jar -> /data1/hadoop/yarn/local/filecache/47346/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:07 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39799/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:07 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39802/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:07 hive-site.xml -> /data/hadoop/yarn/local/filecache/39823/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:07 jackson-annotations-2.4.0.jar -> /data1/hadoop/yarn/local/filecache/39813/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:07 jackson-core-2.4.4.jar -> /data/hadoop/yarn/local/filecache/39814/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 jackson-databind-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/39801/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:07 joda-time-2.5.jar -> /data1/hadoop/yarn/local/filecache/39806/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:07 json-simple-1.1.jar -> /data1/hadoop/yarn/local/filecache/39816/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21895 Feb 13 10:07 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:07 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/47357/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:07 ojdbc6.jar -> /data2/hadoop/yarn/local/filecache/47355/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    90 Feb 13 10:07 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39810/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:07 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39805/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:07 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39819/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:07 poi-3.17.jar -> /data1/hadoop/yarn/local/filecache/47352/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:07 poi-ooxml-3.17.jar -> /data2/hadoop/yarn/local/filecache/47350/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/47347/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    56 Feb 13 10:07 py4j-0.9-src.zip -> /data/hadoop/yarn/local/filecache/40338/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:07 pyspark.zip -> /data/hadoop/yarn/local/filecache/40339/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 qd_rdq_2.10-1.0.1-RC14.jar -> /data1/hadoop/yarn/local/filecache/47351/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 scalaj-http_2.10-2.3.0.jar -> /data2/hadoop/yarn/local/filecache/47345/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:07 scala-library-2.10.5.jar -> /data2/hadoop/yarn/local/filecache/39815/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   101 Feb 13 10:07 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39820/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:07 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112380/filecache/99/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:07 spark-csv_2.10-1.5.0.jar -> /data1/hadoop/yarn/local/filecache/47356/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:07 __spark__.jar -> /data1/hadoop/yarn/local/filecache/39797/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:07 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 univocity-parsers-1.5.1.jar -> /data2/hadoop/yarn/local/filecache/47349/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:07 xmlbeans-2.6.0.jar -> /data1/hadoop/yarn/local/filecache/47348/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "28352633    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:07 .\n",
      "\n",
      "20668423  356 -r-xr-xr-x   1 yarn     hadoop     357604 Nov  1 05:01 ./pyspark.zip\n",
      "\n",
      "28352562 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:06 ./poi-ooxml-3.17.jar\n",
      "\n",
      "28352676   24 -rwx------   1 s112380  hadoop      21895 Feb 13 10:07 ./launch_container.sh\n",
      "\n",
      "83558448  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 12:45 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "83558451  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 12:45 ./azure-storage-4.2.0.jar\n",
      "\n",
      "122896390  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 12:45 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "76750859  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:06 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "76767249  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:06 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "28352636    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:07 ./tmp\n",
      "\n",
      "122896424 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:58 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "122896405  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 12:45 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "84819977   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:58 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "86237189    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:58 ./hive-site.xml\n",
      "\n",
      "76750856 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:06 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "76767242  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:06 ./commons-collections4-4.1.jar\n",
      "\n",
      "84705281   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 12:45 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "84705292 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 12:45 ./scala-library-2.10.5.jar\n",
      "\n",
      "122896428 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:58 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "123035651 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:58 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "84705289   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 12:45 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "28352677    4 -rw-------   1 s112380  hadoop       1024 Feb 13 10:07 ./container_tokens\n",
      "\n",
      "28352637    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:07 ./__spark_conf__\n",
      "\n",
      "28352672    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:07 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "28352675    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:07 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "28352658    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:07 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "28352651    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:07 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "28352639   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:07 ./__spark_conf__/log4j.properties\n",
      "\n",
      "28352669    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:07 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "28352657    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:07 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "28352660    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:07 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "28352661    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:07 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "28352667    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:07 ./__spark_conf__/slaves\n",
      "\n",
      "28352652    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:07 ./__spark_conf__/topology_script.py\n",
      "\n",
      "28352641    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:07 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "28352673    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:07 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "28352662    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:07 ./__spark_conf__/health_check\n",
      "\n",
      "28352643   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:07 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "28352650    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:07 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "28352642    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:07 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "28352638   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:07 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "28352666    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:07 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "28352646    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:07 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "28352665    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:07 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "28352647    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:07 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "28352640    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:07 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "28352659    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:07 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "28352670    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:07 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "28352654    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:07 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "28352656    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:07 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "28352649    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:07 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "28352668    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:07 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "28352648    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:07 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "28352653    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:07 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "28352671    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:07 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "28352645    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:07 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "28352655   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:07 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "28352644   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:07 ./__spark_conf__/core-site.xml\n",
      "\n",
      "28352664    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:07 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "28352674    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:07 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "28352559  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:06 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "28352556 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:06 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "122896412  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 12:45 ./commons-lang3-3.4.jar\n",
      "\n",
      "76750862 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:06 ./poi-3.17.jar\n",
      "\n",
      "122896387  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 12:45 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "84705283 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 12:45 ./guava-11.0.2.jar\n",
      "\n",
      "122896396  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 12:45 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "122896418   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 12:45 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "122896399  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 12:45 ./joda-time-2.5.jar\n",
      "\n",
      "28352568 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:06 ./ojdbc6.jar\n",
      "\n",
      "76750853  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:06 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "25059379   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:06 ./commons-csv-1.1.jar\n",
      "\n",
      "20668419   44 -r-xr-xr-x   1 yarn     hadoop      44846 Nov  1 05:01 ./py4j-0.9-src.zip\n",
      "\n",
      "83558403   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 12:45 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "122896393 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 12:45 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "28352547  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:06 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "122896409 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Oct 22 11:48 ./__spark__.jar\n",
      "\n",
      "76767256  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:06 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "83558454  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 12:45 ./jackson-core-2.4.4.jar\n",
      "\n",
      "122896421   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 12:45 ./json-simple-1.1.jar\n",
      "\n",
      "25059399  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:07 ./__app__.jar\n",
      "\n",
      "123035657  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:58 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21895\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export SPARK_LOG_URL_STDOUT=\"https://awdex01009.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000003/s112380/stdout?start=-4096\"\n",
      "\n",
      "export NM_HOST=\"awdex01009.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003,/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003,/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/,/data2/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_LOG_URL_STDERR=\"https://awdex01009.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000003/s112380/stderr?start=-4096\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_01_000003\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39802/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39819/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47349/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39807/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39797/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47351/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39820/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39809/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39798/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39803/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/usercache/s112380/filecache/100/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39823/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39814/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39826/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47353/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47350/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47352/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47346/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39810/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47357/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47348/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39805/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39815/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47354/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39816/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/usercache/s112380/filecache/99/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47355/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47347/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47356/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39801/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39822/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39811/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39821/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39806/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40339/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39800/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39813/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47345/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40338/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39804/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39799/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39477' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.104.16:39477 --executor-id 2 --hostname awdex01009.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/stdout 2> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000003/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:20182\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/39820/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/39797/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:07:01 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:07:02 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:07:03 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:07:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01009.aws.merckcloud.com:34417]\n",
      "\n",
      "20/02/13 10:07:03 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 34417.\n",
      "\n",
      "20/02/13 10:07:03 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-9792efda-bab5-4c84-89f1-abd32ae970fc\n",
      "\n",
      "20/02/13 10:07:03 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-d0f0d689-220a-4be4-81a6-15523db5df0e\n",
      "\n",
      "20/02/13 10:07:03 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-f4479430-cb63-4ed6-985d-0dfda92c953f\n",
      "\n",
      "20/02/13 10:07:03 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "\n",
      "20/02/13 10:07:03 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.104.16:39477\n",
      "\n",
      "20/02/13 10:07:03 INFO CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "\n",
      "20/02/13 10:07:03 INFO Executor: Starting executor ID 2 on host awdex01009.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:07:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38575.\n",
      "\n",
      "20/02/13 10:07:03 INFO NettyBlockTransferService: Server created on 38575\n",
      "\n",
      "20/02/13 10:07:03 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:07:03 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:07:31 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@476854bf,BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:07:44 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@476854bf,BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575))] in 2 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:07:57 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@476854bf,BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575))] in 3 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat scala.util.Failure.recover(Try.scala:185)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n",
      "\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.tryFailure(Promise.scala:112)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:245)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:246)\n",
      "\n",
      "\t... 7 more\n",
      "\n",
      "20/02/13 10:07:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@476854bf,BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat scala.util.Failure.recover(Try.scala:185)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n",
      "\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.tryFailure(Promise.scala:112)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:245)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\n",
      "\t... 3 more\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:246)\n",
      "\n",
      "\t... 7 more\n",
      "\n",
      "20/02/13 10:08:07 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@51972c21,BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 7366319460778958129 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 7704242638157879220 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 4808212128591197026 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 7183363164714244835 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown\n",
      "\n",
      "20/02/13 10:08:09 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n",
      "\n",
      "20/02/13 10:08:09 INFO DiskBlockManager: Shutdown hook called\n",
      "\n",
      "20/02/13 10:08:09 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "Container: container_e175_1568810042014_190518_02_000001 on awdex01010.aws.merckcloud.com_45454_1581588577474\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:69906\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:08:09 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:08:10 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1568810042014_190518_000002\n",
      "\n",
      "20/02/13 10:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "20/02/13 10:08:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:11 INFO ApplicationMaster: Starting the user application in a separate Thread\n",
      "\n",
      "20/02/13 10:08:11 INFO ApplicationMaster: Waiting for spark context initialization\n",
      "\n",
      "20/02/13 10:08:11 INFO ApplicationMaster: Waiting for spark context initialization ... \n",
      "\n",
      "20/02/13 10:08:11 INFO SparkContext: Running Spark version 1.6.3\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:11 INFO Utils: Successfully started service 'sparkDriver' on port 33042.\n",
      "\n",
      "20/02/13 10:08:12 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:08:12 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:08:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.31.97.178:50577]\n",
      "\n",
      "20/02/13 10:08:12 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 50577.\n",
      "\n",
      "20/02/13 10:08:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "\n",
      "20/02/13 10:08:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "\n",
      "20/02/13 10:08:12 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-0d0db519-6a59-4620-b96d-af34ba720316\n",
      "\n",
      "20/02/13 10:08:12 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-76e48b76-42ef-4ea7-8fc1-f66234fd6b8c\n",
      "\n",
      "20/02/13 10:08:12 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-d16c11bb-85b3-467c-8d75-1d55d14ea251\n",
      "\n",
      "20/02/13 10:08:12 INFO MemoryStore: MemoryStore started with capacity 514.1 MB\n",
      "\n",
      "20/02/13 10:08:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "\n",
      "20/02/13 10:08:12 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "\n",
      "20/02/13 10:08:12 INFO Utils: Successfully started service 'SparkUI' on port 34321.\n",
      "\n",
      "20/02/13 10:08:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.97.178:34321\n",
      "\n",
      "20/02/13 10:08:12 INFO YarnClusterScheduler: Created YarnClusterScheduler\n",
      "\n",
      "20/02/13 10:08:12 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1568810042014_190518 and attemptId Some(appattempt_1568810042014_190518_000002)\n",
      "\n",
      "20/02/13 10:08:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44822.\n",
      "\n",
      "20/02/13 10:08:12 INFO NettyBlockTransferService: Server created on 44822\n",
      "\n",
      "20/02/13 10:08:12 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:08:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.97.178:44822 with 514.1 MB RAM, BlockManagerId(driver, 172.31.97.178, 44822)\n",
      "\n",
      "20/02/13 10:08:12 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:08:12 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.31.97.178:33042)\n",
      "\n",
      "20/02/13 10:08:12 INFO YarnRMClient: Registering the ApplicationMaster\n",
      "\n",
      "20/02/13 10:08:13 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Will request 2 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)\n",
      "\n",
      "20/02/13 10:08:13 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals\n",
      "\n",
      "20/02/13 10:08:13 INFO AMRMClientImpl: Received new token for : awdex01016.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Launching container container_e175_1568810042014_190518_02_000002 for on host awdex01016.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.97.178:33042,  executorHostname: awdex01016.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Starting Executor Container\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.\n",
      "\n",
      "20/02/13 10:08:13 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Setting up ContainerLaunchContext\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Preparing Local resources\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534148696604 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1551865169204 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190503288 timestamp: 1536921442593 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1551865168521 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1581588408931 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534148683990 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1551865168745 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534148701957 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1551865169151 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534148684575 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip\" } size: 357604 timestamp: 1534148696849 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534148696362 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534148696706 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534148685010 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1551865168577 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534148684121 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534148696265 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534148685067 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1551865168863 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534148684624 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534148684682 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534148684206 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1-RC14.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1552982988554 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534148774104 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1551865168953 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml\" } size: 1920 timestamp: 1534148696504 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534148684923 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip\" } size: 155669 timestamp: 1581588410189 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534148684835 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534148684764 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534148684282 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1551865168637 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1551865168471 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1551865169314 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534148684447 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1551865169434 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1551865169256 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534148684047 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534148684511 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534148696458 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534148684346 type: FILE visibility: PUBLIC)\n",
      "\n",
      "20/02/13 10:08:13 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: \n",
      "\n",
      "===============================================================================\n",
      "\n",
      "YARN executor launch context:\n",
      "\n",
      "  env:\n",
      "\n",
      "    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\n",
      "\n",
      "    SPARK_LOG_URL_STDERR -> https://awdex01016.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000002/s112380/stderr?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\n",
      "\n",
      "    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1568810042014_190518\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 155669\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\n",
      "\n",
      "    SPARK_USER -> s112380\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581588410189\n",
      "\n",
      "    SPARK_YARN_MODE -> true\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\n",
      "\n",
      "    SPARK_LOG_URL_STDOUT -> https://awdex01016.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000002/s112380/stdout?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES -> hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE\n",
      "\n",
      "\n",
      "\n",
      "  command:\n",
      "\n",
      "    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=33042' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.97.178:33042 --executor-id 1 --hostname awdex01016.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "      \n",
      "\n",
      "20/02/13 10:08:13 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01016.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:08:13 INFO AMRMClientImpl: Received new token for : awdex01015.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Launching container container_e175_1568810042014_190518_02_000003 for on host awdex01015.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.97.178:33042,  executorHostname: awdex01015.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:13 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Starting Executor Container\n",
      "\n",
      "20/02/13 10:08:13 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Setting up ContainerLaunchContext\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Preparing Local resources\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534148696604 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1551865169204 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190503288 timestamp: 1536921442593 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1551865168521 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1581588408931 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534148683990 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1551865168745 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534148701957 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1551865169151 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534148684575 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip\" } size: 357604 timestamp: 1534148696849 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534148696362 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534148696706 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534148685010 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1551865168577 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534148684121 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534148696265 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534148685067 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1551865168863 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534148684624 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534148684682 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534148684206 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1-RC14.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1552982988554 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534148774104 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1551865168953 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml\" } size: 1920 timestamp: 1534148696504 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534148684923 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip\" } size: 155669 timestamp: 1581588410189 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534148684835 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534148684764 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534148684282 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1551865168637 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1551865168471 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1551865169314 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534148684447 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1551865169434 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1551865169256 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534148684047 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534148684511 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534148696458 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534148684346 type: FILE visibility: PUBLIC)\n",
      "\n",
      "20/02/13 10:08:13 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\n",
      "\n",
      "20/02/13 10:08:13 INFO ExecutorRunnable: \n",
      "\n",
      "===============================================================================\n",
      "\n",
      "YARN executor launch context:\n",
      "\n",
      "  env:\n",
      "\n",
      "    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\n",
      "\n",
      "    SPARK_LOG_URL_STDERR -> https://awdex01015.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000003/s112380/stderr?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\n",
      "\n",
      "    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1568810042014_190518\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 155669\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\n",
      "\n",
      "    SPARK_USER -> s112380\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581588410189\n",
      "\n",
      "    SPARK_YARN_MODE -> true\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\n",
      "\n",
      "    SPARK_LOG_URL_STDOUT -> https://awdex01015.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000003/s112380/stdout?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES -> hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE\n",
      "\n",
      "\n",
      "\n",
      "  command:\n",
      "\n",
      "    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=33042' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.97.178:33042 --executor-id 2 --hostname awdex01015.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "      \n",
      "\n",
      "20/02/13 10:08:13 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01015.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:08:16 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01015.aws.merckcloud.com:43955) with ID 2\n",
      "\n",
      "20/02/13 10:08:16 INFO BlockManagerMasterEndpoint: Registering block manager awdex01015.aws.merckcloud.com:59047 with 511.1 MB RAM, BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047)\n",
      "\n",
      "20/02/13 10:08:16 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.\n",
      "\n",
      "20/02/13 10:08:18 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01016.aws.merckcloud.com:56172) with ID 1\n",
      "\n",
      "20/02/13 10:08:18 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "\n",
      "20/02/13 10:08:18 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done\n",
      "\n",
      "20/02/13 10:08:18 INFO BlockManagerMasterEndpoint: Registering block manager awdex01016.aws.merckcloud.com:49116 with 511.1 MB RAM, BlockManagerId(1, awdex01016.aws.merckcloud.com, 49116)\n",
      "\n",
      "20/02/13 10:08:19 INFO PipelineConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf\n",
      "\n",
      "20/02/13 10:08:19 INFO FilesystemAccess$: Max Pipeline Run ID found:52\n",
      "\n",
      "20/02/13 10:08:19 INFO InputConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf\n",
      "\n",
      "20/02/13 10:08:19 INFO PrepareTrackwiseOLDL_AuditExport$: Checking input path hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport\n",
      "\n",
      "20/02/13 10:08:19 INFO EnvironmentConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf\n",
      "\n",
      "20/02/13 10:08:19 INFO AtlasRepository: Atlas URLs provided for this pipeline: https://awdex01007.aws.merckcloud.com:21443/api/atlas/,https://awdex01008.aws.merckcloud.com:21443/api/atlas/\n",
      "\n",
      "20/02/13 10:08:19 INFO AtlasRepository: response: HttpResponse(,302,Map(Content-Length -> Vector(0), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Location -> Vector(https://awdex01008.aws.merckcloud.com:21443/api/atlas/entities?type=DataSet_mc_v2_0&amp;property=fileLocation&amp;value=hdfs%3A%2F%2Faaprod%2Fmc_staging%2Frdq_uc7_files%2Ftrackwise_oldl_auditexport%2F00_00_01%2Frun_000052%2Fraw%2FTrackwiseOLDL_AuditExport), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=1ghybimg9u2nd8n4e3cclty2c;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 302 Found), X-Frame-Options -> Vector(DENY)))\n",
      "\n",
      "20/02/13 10:08:19 WARN AtlasRepository: Atlas request to https://awdex01007.aws.merckcloud.com:21443/api/atlas/entities failed with status 3xx or 5xx.\n",
      "\n",
      "20/02/13 10:08:19 WARN AtlasRepository: response.body: \n",
      "\n",
      "20/02/13 10:08:25 INFO AtlasRepository: response: HttpResponse({\"requestId\":\"qtp1489092624-4723708 - 698bc5a9-73d8-4623-aa32-72377d08f58b\",\"definition\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"31676f18-8ba9-4a66-81aa-02ee048ca9f3\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\"* * * * *\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs:\\/\\/aaprod\\/mc_staging\\/rdq_uc7_files\\/trackwise_oldl_auditexport\\/00_00_01\\/run_000052\\/raw\\/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aaprod_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000052_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-13T10:06:13.284Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-13T10:06:13.628Z\",\"geoScope\":\"NA\",\"owner\":\"s112380\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: deda1x3044.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}},200,Map(Content-Type -> Vector(application/json; charset=UTF-8), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=19t0isdcvpdvc13a4f6vvgrr96;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 200 OK), Transfer-Encoding -> Vector(chunked), X-Frame-Options -> Vector(DENY)))\n",
      "\n",
      "20/02/13 10:08:25 INFO AtlasRepository: Atlas request to https://awdex01008.aws.merckcloud.com:21443/api/atlas/entities succeeded.\n",
      "\n",
      "20/02/13 10:08:25 INFO MetadataAccess: getEntityByFileLocation: {\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"31676f18-8ba9-4a66-81aa-02ee048ca9f3\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\"* * * * *\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aaprod_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000052_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-13T10:06:13.284Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-13T10:06:13.628Z\",\"geoScope\":\"NA\",\"owner\":\"s112380\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: deda1x3044.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}\n",
      "\n",
      "20/02/13 10:08:25 INFO PrepareTrackwiseOLDL_AuditExport$: Excel Input path:hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport/Audit_Export_Report_01_20200203_v2__m271552_2020-02-12T13-02-07+0100.xlsx\n",
      "\n",
      "Exception in thread \"dispatcher-event-loop-5\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat scala.collection.MapLike$FilteredKeys.foreach(MapLike.scala:231)\n",
      "\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n",
      "\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:197)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:123)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:116)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:217)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:09:25 WARN TransportChannelHandler: Exception in connection from awdex01015.aws.merckcloud.com/172.31.104.16:43955\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat java.lang.String.replace(String.java:2078)\n",
      "\n",
      "\tat java.io.ObjectStreamClass.computeDefaultSUID(ObjectStreamClass.java:1840)\n",
      "\n",
      "\tat java.io.ObjectStreamClass.access$100(ObjectStreamClass.java:72)\n",
      "\n",
      "\tat java.io.ObjectStreamClass$1.run(ObjectStreamClass.java:253)\n",
      "\n",
      "\tat java.io.ObjectStreamClass$1.run(ObjectStreamClass.java:251)\n",
      "\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\n",
      "\tat java.io.ObjectStreamClass.getSerialVersionUID(ObjectStreamClass.java:250)\n",
      "\n",
      "\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:611)\n",
      "\n",
      "\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1829)\n",
      "\n",
      "\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)\n",
      "\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986)\n",
      "\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)\n",
      "\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)\n",
      "\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1$$anonfun$apply$1.apply(NettyRpcEnv.scala:262)\n",
      "\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:314)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:261)\n",
      "\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:260)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:592)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:574)\n",
      "\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)\n",
      "\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)\n",
      "\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)\n",
      "\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)\n",
      "\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n",
      "\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n",
      "\n",
      "20/02/13 10:09:25 ERROR ApplicationMaster: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat org.apache.xerces.dom.ElementImpl.getAttributes(Unknown Source)\n",
      "\n",
      "\tat org.apache.xerces.dom.DeferredElementNSImpl.synchronizeData(Unknown Source)\n",
      "\n",
      "\tat org.apache.xerces.dom.ElementNSImpl.getNamespaceURI(Unknown Source)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1420)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.parseToXmlObject(Locale.java:1385)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.parseToXmlObject(Locale.java:1370)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.schema.SchemaTypeLoaderBase.parse(SchemaTypeLoaderBase.java:370)\n",
      "\n",
      "\tat org.apache.poi.POIXMLTypeLoader.parse(POIXMLTypeLoader.java:164)\n",
      "\n",
      "\tat org.openxmlformats.schemas.spreadsheetml.x2006.main.WorksheetDocument$Factory.parse(Unknown Source)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFSheet.read(XSSFSheet.java:226)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFSheet.onDocumentRead(XSSFSheet.java:218)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.parseSheet(XSSFWorkbook.java:443)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.onDocumentRead(XSSFWorkbook.java:408)\n",
      "\n",
      "\tat org.apache.poi.POIXMLDocument.load(POIXMLDocument.java:169)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:270)\n",
      "\n",
      "\tat org.apache.poi.ss.usermodel.WorkbookFactory.create(WorkbookFactory.java:184)\n",
      "\n",
      "\tat org.apache.poi.ss.usermodel.WorkbookFactory.create(WorkbookFactory.java:149)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.getExcelWorkbook(RdqTransformationTemplate.scala:150)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.doSpecificTransformation(RdqTransformationTemplate.scala:171)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.pipelinecore.TransformationStepTemplate$class.executeTransformationStep(TransformationStepTemplate.scala:42)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.executeTransformationStep(RdqTransformationTemplate.scala:25)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.main(RdqTransformationTemplate.scala:241)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport.main(PrepareTrackwiseOLDL_AuditExport.scala)\n",
      "\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\n",
      "20/02/13 10:09:25 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded)\n",
      "\n",
      "20/02/13 10:09:25 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat akka.dispatch.AbstractNodeQueue.add(AbstractNodeQueue.java:46)\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler.akka$actor$LightArrayRevolverScheduler$$schedule(Scheduler.scala:313)\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler$$anon$2$$anon$1.run(Scheduler.scala:245)\n",
      "\n",
      "\tat akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)\n",
      "\n",
      "\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)\n",
      "\n",
      "\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "\n",
      "\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "\n",
      "\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "\n",
      "\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "\n",
      "20/02/13 10:09:25 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5318772731848173764, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=81]}} to awdex01015.aws.merckcloud.com/172.31.104.16:43955; closing connection\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:25 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "\n",
      "20/02/13 10:09:25 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "\n",
      "20/02/13 10:09:25 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "\n",
      "20/02/13 10:09:25 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n",
      "\n",
      "20/02/13 10:09:25 INFO SparkUI: Stopped Spark web UI at http://172.31.97.178:34321\n",
      "\n",
      "20/02/13 10:09:25 INFO YarnAllocator: Driver requested a total number of 0 executor(s).\n",
      "\n",
      "20/02/13 10:09:25 INFO YarnClusterSchedulerBackend: Shutting down all executors\n",
      "\n",
      "20/02/13 10:09:27 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())\n",
      "\n",
      "20/02/13 10:09:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException\n",
      "\n",
      "java.util.concurrent.TimeoutException\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\n",
      "\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67)\n",
      "\n",
      "20/02/13 10:09:35 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "\n",
      "(serviceOption=None,\n",
      "\n",
      " services=List(),\n",
      "\n",
      " started=false)\n",
      "\n",
      "20/02/13 10:09:35 ERROR Utils: Uncaught exception in thread pool-2-thread-1\n",
      "\n",
      "org.apache.spark.SparkException: Error asking standalone scheduler to shut down executors\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:339)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:344)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:89)\n",
      "\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:446)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClusterScheduler.stop(YarnClusterScheduler.scala:38)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1584)\n",
      "\n",
      "\tat org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1739)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)\n",
      "\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1738)\n",
      "\n",
      "\tat org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)\n",
      "\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.lang.InterruptedException\n",
      "\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)\n",
      "\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)\n",
      "\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:335)\n",
      "\n",
      "\t... 25 more\n",
      "\n",
      "20/02/13 10:09:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "\n",
      "20/02/13 10:09:35 INFO MemoryStore: MemoryStore cleared\n",
      "\n",
      "20/02/13 10:09:35 INFO BlockManager: BlockManager stopped\n",
      "\n",
      "20/02/13 10:09:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "\n",
      "20/02/13 10:09:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "\n",
      "20/02/13 10:09:35 INFO SparkContext: Successfully stopped SparkContext\n",
      "\n",
      "20/02/13 10:09:35 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded)\n",
      "\n",
      "20/02/13 10:09:35 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.\n",
      "\n",
      "20/02/13 10:09:35 INFO ApplicationMaster: Deleting staging directory .sparkStaging/application_1568810042014_190518\n",
      "\n",
      "20/02/13 10:09:35 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "20/02/13 10:09:35 INFO ShutdownHookManager: Deleting directory /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-204390c2-bb01-4ab7-91df-551b164498f4\n",
      "\n",
      "20/02/13 10:09:35 INFO ShutdownHookManager: Deleting directory /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-6352df4f-4958-4db4-a13c-6cbf14d97d0c\n",
      "\n",
      "20/02/13 10:09:35 INFO ShutdownHookManager: Deleting directory /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-39236dba-1380-4966-a0ff-8fd0c95b7988\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14328\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 156\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:08 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112380/filecache/91/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 aws-java-sdk-core-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/39987/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 aws-java-sdk-kms-1.10.6.jar -> /data/hadoop/yarn/local/filecache/39993/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 aws-java-sdk-s3-1.10.6.jar -> /data2/hadoop/yarn/local/filecache/39982/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 azure-keyvault-core-0.8.0.jar -> /data/hadoop/yarn/local/filecache/39984/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 azure-storage-4.2.0.jar -> /data/hadoop/yarn/local/filecache/39991/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 commons-collections4-4.1.jar -> /data1/hadoop/yarn/local/filecache/47307/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:08 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/47305/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:08 commons-lang3-3.4.jar -> /data2/hadoop/yarn/local/filecache/39995/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop   987 Feb 13 10:08 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 datanucleus-api-jdo-3.2.6.jar -> /data2/hadoop/yarn/local/filecache/40009/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 datanucleus-core-3.2.10.jar -> /data1/hadoop/yarn/local/filecache/40006/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 datanucleus-rdbms-3.2.9.jar -> /data1/hadoop/yarn/local/filecache/40005/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:08 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/39988/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:08 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/47298/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:08 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39983/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:08 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39986/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    54 Feb 13 10:08 hive-site.xml -> /data2/hadoop/yarn/local/filecache/40007/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 jackson-annotations-2.4.0.jar -> /data1/hadoop/yarn/local/filecache/39997/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 jackson-core-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/39998/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 jackson-databind-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/39985/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:08 joda-time-2.5.jar -> /data2/hadoop/yarn/local/filecache/39990/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:08 json-simple-1.1.jar -> /data/hadoop/yarn/local/filecache/40000/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21847 Feb 13 10:08 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:08 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/47309/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:08 ojdbc6.jar -> /data1/hadoop/yarn/local/filecache/47306/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    90 Feb 13 10:08 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39994/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:08 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39989/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:08 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/40003/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:08 poi-3.17.jar -> /data1/hadoop/yarn/local/filecache/47304/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:08 poi-ooxml-3.17.jar -> /data/hadoop/yarn/local/filecache/47303/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/47299/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:08 py4j-0.9-src.zip -> /data2/hadoop/yarn/local/filecache/40002/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:08 pyspark.zip -> /data/hadoop/yarn/local/filecache/40008/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 qd_rdq_2.10-1.0.1-RC14.jar -> /data2/hadoop/yarn/local/filecache/47302/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:08 scalaj-http_2.10-2.3.0.jar -> /data/hadoop/yarn/local/filecache/47297/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 scala-library-2.10.5.jar -> /data/hadoop/yarn/local/filecache/39999/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   101 Feb 13 10:08 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:08 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112380/filecache/90/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 spark-csv_2.10-1.5.0.jar -> /data/hadoop/yarn/local/filecache/47308/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 __spark__.jar -> /data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:08 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 univocity-parsers-1.5.1.jar -> /data2/hadoop/yarn/local/filecache/47301/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:08 xmlbeans-2.6.0.jar -> /data1/hadoop/yarn/local/filecache/47300/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "54026244    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:08 .\n",
      "\n",
      "154108296 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 11:22 ./scala-library-2.10.5.jar\n",
      "\n",
      "165027841  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:06 ./commons-collections4-4.1.jar\n",
      "\n",
      "97673525   44 -r-xr-xr-x   1 yarn     hadoop      44846 Oct 22 12:50 ./py4j-0.9-src.zip\n",
      "\n",
      "174588084  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 11:22 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "83279873 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:06 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "149536999  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 11:22 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "154108300   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 11:22 ./json-simple-1.1.jar\n",
      "\n",
      "97673555 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:50 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "108412929  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:06 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "54026248    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:08 ./tmp\n",
      "\n",
      "149348385    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:50 ./hive-site.xml\n",
      "\n",
      "160735238 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:50 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "174588087 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 11:22 ./guava-11.0.2.jar\n",
      "\n",
      "149545317  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 11:22 ./commons-lang3-3.4.jar\n",
      "\n",
      "154108290  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 11:22 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "174588096   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 11:22 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "174588090   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 11:22 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "164814849 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:06 ./ojdbc6.jar\n",
      "\n",
      "108355585  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:06 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "164544513 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:06 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "54026246    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:07 ./__spark_conf__\n",
      "\n",
      "4800521    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:07 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "4800519   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:07 ./__spark_conf__/core-site.xml\n",
      "\n",
      "4800541    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:07 ./__spark_conf__/slaves\n",
      "\n",
      "4800522    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:07 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "4800513   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:07 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "4800533    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:07 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "4800536    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:07 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "4800525    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:07 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "4800538    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:07 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "4800544    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:07 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "4800529    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:07 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "4800518   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:07 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "4800515    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:07 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "4800520    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:07 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "4800543    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:07 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "4800516    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:07 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "4800539    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:07 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "4800514   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:07 ./__spark_conf__/log4j.properties\n",
      "\n",
      "4800526    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:07 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "4800524    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:07 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "4800527    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:07 ./__spark_conf__/topology_script.py\n",
      "\n",
      "4800548    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:07 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "4800549    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:07 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "4800523    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:07 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "4800530   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:07 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "4800534    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:07 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "4800540    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:07 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "4800545    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:07 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "4800531    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:07 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "4800528    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:07 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "4800542    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:07 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "4800546    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:07 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "4800537    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:07 ./__spark_conf__/health_check\n",
      "\n",
      "4800517    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:07 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "4800535    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:07 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "4800532    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:07 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "4800547    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:07 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "149553155  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:50 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "174588099  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 11:22 ./jackson-core-2.4.4.jar\n",
      "\n",
      "149168178  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 11:22 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "50028545  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:06 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "49881089  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:06 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "174588093   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 11:22 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "149545283  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 11:22 ./joda-time-2.5.jar\n",
      "\n",
      "164921345   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:06 ./commons-csv-1.1.jar\n",
      "\n",
      "50241538  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:06 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "160735235 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:50 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "149537002 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 11:22 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "164839425 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:06 ./poi-3.17.jar\n",
      "\n",
      "154108281   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 11:22 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "4784130    4 -rw-------   1 s112380  hadoop        987 Feb 13 10:08 ./container_tokens\n",
      "\n",
      "162971657 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Oct 22 12:58 ./__spark__.jar\n",
      "\n",
      "108404737 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:06 ./poi-ooxml-3.17.jar\n",
      "\n",
      "154108284  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 11:22 ./azure-storage-4.2.0.jar\n",
      "\n",
      "24027137  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:07 ./__app__.jar\n",
      "\n",
      "164978689  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:06 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "141312296   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:50 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "4784129   24 -rwx------   1 s112380  hadoop      21847 Feb 13 10:08 ./launch_container.sh\n",
      "\n",
      "149545085  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 11:22 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "141312299  356 -r-xr-xr-x   1 yarn     hadoop     357604 Oct 22 12:50 ./pyspark.zip\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21847\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export MAX_APP_ATTEMPTS=\"2\"\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export APP_SUBMIT_TIME_ENV=\"1581588410329\"\n",
      "\n",
      "export NM_HOST=\"awdex01010.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export APPLICATION_WEB_PROXY_BASE=\"/proxy/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001,/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001,/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/,/data2/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_02_000001\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39995/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47307/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39991/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/usercache/s112380/filecache/91/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40008/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40003/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47303/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39983/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47297/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47299/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47298/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39994/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40006/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40002/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40007/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47305/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39985/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39998/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40000/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47309/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47304/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40009/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/usercache/s112380/filecache/90/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39999/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39990/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39988/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39989/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39997/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39993/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47300/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39984/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39986/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47306/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39987/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39982/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40005/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47308/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47301/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47302/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -Xmx1024m -Djava.io.tmpdir=$PWD/tmp '-Dlog4j.configuration=spark-log4j.properties' -Dhdp.version=2.5.5.0-157 -Dspark.yarn.app.container.log.dir=/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class 'com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport' --jar file:/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190517/container_e175_1568810042014_190517_01_000002/qd_rdq_2.10-1.0.1-RC14.jar --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/business_metadata_TrackwiseOLDL_AuditExport.conf' --executor-memory 1024m --executor-cores 1 --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/stdout 2> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000001/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n",
      "Container: container_e175_1568810042014_190518_01_000002 on awdex01010.aws.merckcloud.com_45454_1581588577474\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14328\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 156\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:07 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112380/filecache/91/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:07 aws-java-sdk-core-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/39987/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 aws-java-sdk-kms-1.10.6.jar -> /data/hadoop/yarn/local/filecache/39993/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 aws-java-sdk-s3-1.10.6.jar -> /data2/hadoop/yarn/local/filecache/39982/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:07 azure-keyvault-core-0.8.0.jar -> /data/hadoop/yarn/local/filecache/39984/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:07 azure-storage-4.2.0.jar -> /data/hadoop/yarn/local/filecache/39991/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:07 commons-collections4-4.1.jar -> /data1/hadoop/yarn/local/filecache/47307/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:07 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/47305/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:07 commons-lang3-3.4.jar -> /data2/hadoop/yarn/local/filecache/39995/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop  1024 Feb 13 10:07 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:07 datanucleus-api-jdo-3.2.6.jar -> /data2/hadoop/yarn/local/filecache/40009/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 datanucleus-core-3.2.10.jar -> /data1/hadoop/yarn/local/filecache/40006/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 datanucleus-rdbms-3.2.9.jar -> /data1/hadoop/yarn/local/filecache/40005/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:07 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/39988/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:07 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/47298/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:07 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39983/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:07 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/39986/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    54 Feb 13 10:07 hive-site.xml -> /data2/hadoop/yarn/local/filecache/40007/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:07 jackson-annotations-2.4.0.jar -> /data1/hadoop/yarn/local/filecache/39997/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:07 jackson-core-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/39998/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 jackson-databind-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/39985/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:07 joda-time-2.5.jar -> /data2/hadoop/yarn/local/filecache/39990/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:07 json-simple-1.1.jar -> /data/hadoop/yarn/local/filecache/40000/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21904 Feb 13 10:07 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:07 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/47309/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:07 ojdbc6.jar -> /data1/hadoop/yarn/local/filecache/47306/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    90 Feb 13 10:07 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39994/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:07 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/39989/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:07 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/40003/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:07 poi-3.17.jar -> /data1/hadoop/yarn/local/filecache/47304/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:07 poi-ooxml-3.17.jar -> /data/hadoop/yarn/local/filecache/47303/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/47299/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:07 py4j-0.9-src.zip -> /data2/hadoop/yarn/local/filecache/40002/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:07 pyspark.zip -> /data/hadoop/yarn/local/filecache/40008/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:07 qd_rdq_2.10-1.0.1-RC14.jar -> /data2/hadoop/yarn/local/filecache/47302/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:07 scalaj-http_2.10-2.3.0.jar -> /data/hadoop/yarn/local/filecache/47297/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:07 scala-library-2.10.5.jar -> /data/hadoop/yarn/local/filecache/39999/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   101 Feb 13 10:07 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:07 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112380/filecache/90/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:07 spark-csv_2.10-1.5.0.jar -> /data/hadoop/yarn/local/filecache/47308/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:07 __spark__.jar -> /data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:07 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:07 univocity-parsers-1.5.1.jar -> /data2/hadoop/yarn/local/filecache/47301/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:07 xmlbeans-2.6.0.jar -> /data1/hadoop/yarn/local/filecache/47300/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "54026244    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:07 .\n",
      "\n",
      "154108296 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 11:22 ./scala-library-2.10.5.jar\n",
      "\n",
      "165027841  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:06 ./commons-collections4-4.1.jar\n",
      "\n",
      "97673525   44 -r-xr-xr-x   1 yarn     hadoop      44846 Oct 22 12:50 ./py4j-0.9-src.zip\n",
      "\n",
      "174588084  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 11:22 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "83279873 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:06 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "149536999  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 11:22 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "154108300   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 11:22 ./json-simple-1.1.jar\n",
      "\n",
      "97673555 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:50 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "108412929  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:06 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "54026248    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:07 ./tmp\n",
      "\n",
      "149348385    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:50 ./hive-site.xml\n",
      "\n",
      "160735238 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:50 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "174588087 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 11:22 ./guava-11.0.2.jar\n",
      "\n",
      "149545317  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 11:22 ./commons-lang3-3.4.jar\n",
      "\n",
      "154108290  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 11:22 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "174588096   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 11:22 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "174588090   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 11:22 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "164814849 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:06 ./ojdbc6.jar\n",
      "\n",
      "108355585  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:06 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "164544513 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:06 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "54026246    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:07 ./__spark_conf__\n",
      "\n",
      "4800521    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:07 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "4800519   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:07 ./__spark_conf__/core-site.xml\n",
      "\n",
      "4800541    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:07 ./__spark_conf__/slaves\n",
      "\n",
      "4800522    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:07 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "4800513   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:07 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "4800533    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:07 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "4800536    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:07 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "4800525    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:07 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "4800538    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:07 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "4800544    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:07 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "4800529    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:07 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "4800518   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:07 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "4800515    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:07 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "4800520    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:07 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "4800543    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:07 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "4800516    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:07 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "4800539    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:07 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "4800514   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:07 ./__spark_conf__/log4j.properties\n",
      "\n",
      "4800526    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:07 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "4800524    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:07 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "4800527    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:07 ./__spark_conf__/topology_script.py\n",
      "\n",
      "4800548    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:07 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "4800549    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:07 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "4800523    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:07 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "4800530   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:07 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "4800534    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:07 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "4800540    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:07 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "4800545    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:07 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "4800531    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:07 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "4800528    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:07 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "4800542    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:07 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "4800546    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:07 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "4800537    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:07 ./__spark_conf__/health_check\n",
      "\n",
      "4800517    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:07 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "4800535    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:07 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "4800532    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:07 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "4800547    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:07 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "149553155  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:50 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "174588099  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 11:22 ./jackson-core-2.4.4.jar\n",
      "\n",
      "149168178  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 11:22 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "50028545  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:06 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "49881089  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:06 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "174588093   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 11:22 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "149545283  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 11:22 ./joda-time-2.5.jar\n",
      "\n",
      "164921345   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:06 ./commons-csv-1.1.jar\n",
      "\n",
      "50241538  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:06 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "160735235 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:50 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "149537002 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 11:22 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "164839425 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:06 ./poi-3.17.jar\n",
      "\n",
      "154108281   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 11:22 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "4784130    4 -rw-------   1 s112380  hadoop       1024 Feb 13 10:07 ./container_tokens\n",
      "\n",
      "162971657 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Oct 22 12:58 ./__spark__.jar\n",
      "\n",
      "108404737 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:06 ./poi-ooxml-3.17.jar\n",
      "\n",
      "154108284  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 11:22 ./azure-storage-4.2.0.jar\n",
      "\n",
      "24027137  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:07 ./__app__.jar\n",
      "\n",
      "164978689  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:06 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "141312296   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:50 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "4784129   24 -rwx------   1 s112380  hadoop      21904 Feb 13 10:07 ./launch_container.sh\n",
      "\n",
      "149545085  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 11:22 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "141312299  356 -r-xr-xr-x   1 yarn     hadoop     357604 Oct 22 12:50 ./pyspark.zip\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21904\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export SPARK_LOG_URL_STDOUT=\"https://awdex01010.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000002/s112380/stdout?start=-4096\"\n",
      "\n",
      "export NM_HOST=\"awdex01010.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002,/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002,/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/,/data2/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_LOG_URL_STDERR=\"https://awdex01010.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000002/s112380/stderr?start=-4096\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_01_000002\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39995/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47307/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39991/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/usercache/s112380/filecache/91/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40008/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40003/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39983/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47303/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47297/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47299/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47298/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39994/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40006/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40002/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40007/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47305/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39985/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39998/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40000/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47309/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47304/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40009/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/usercache/s112380/filecache/90/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39999/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39990/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39988/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39989/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39997/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39993/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47300/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/39984/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39986/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/47306/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/39987/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/39982/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40005/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/47308/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47301/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/47302/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39477' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.104.16:39477 --executor-id 1 --hostname awdex01010.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/stdout 2> /data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000002/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:17715\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/40004/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/40011/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:07:02 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:07:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:07:03 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:07:03 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:07:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:07:03 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:07:03 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:07:04 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01010.aws.merckcloud.com:50294]\n",
      "\n",
      "20/02/13 10:07:04 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 50294.\n",
      "\n",
      "20/02/13 10:07:04 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-8e134cb8-f663-4334-ab91-ed98738efdb0\n",
      "\n",
      "20/02/13 10:07:04 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-3c2014e1-81a1-4a8c-9521-0152c9de38a4\n",
      "\n",
      "20/02/13 10:07:04 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-1fcc11bc-1be9-4882-8d0b-7e1149f906a3\n",
      "\n",
      "20/02/13 10:07:04 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "\n",
      "20/02/13 10:07:04 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.104.16:39477\n",
      "\n",
      "20/02/13 10:07:04 INFO CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "\n",
      "20/02/13 10:07:04 INFO Executor: Starting executor ID 1 on host awdex01010.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:07:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35056.\n",
      "\n",
      "20/02/13 10:07:04 INFO NettyBlockTransferService: Server created on 35056\n",
      "\n",
      "20/02/13 10:07:04 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:07:04 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:07:31 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@11e1d447,BlockManagerId(1, awdex01010.aws.merckcloud.com, 35056))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:07:44 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@11e1d447,BlockManagerId(1, awdex01010.aws.merckcloud.com, 35056))] in 2 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:07:57 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@11e1d447,BlockManagerId(1, awdex01010.aws.merckcloud.com, 35056))] in 3 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat scala.util.Failure.recover(Try.scala:185)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n",
      "\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.tryFailure(Promise.scala:112)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:245)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:246)\n",
      "\n",
      "\t... 7 more\n",
      "\n",
      "20/02/13 10:07:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@11e1d447,BlockManagerId(1, awdex01010.aws.merckcloud.com, 35056))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat scala.util.Failure.recover(Try.scala:185)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n",
      "\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.tryFailure(Promise.scala:112)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:245)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\n",
      "\t... 3 more\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:246)\n",
      "\n",
      "\t... 7 more\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 5343796477656322884 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 6073887560252618449 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 WARN TransportResponseHandler: Ignoring response for RPC 8023949327846785164 from /172.31.104.16:39477 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:07 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown\n",
      "\n",
      "20/02/13 10:08:07 INFO MemoryStore: MemoryStore cleared\n",
      "\n",
      "20/02/13 10:08:07 INFO BlockManager: BlockManager stopped\n",
      "\n",
      "20/02/13 10:08:07 INFO CoarseGrainedExecutorBackend: Driver from awdex01015.aws.merckcloud.com:39477 disconnected during shutdown\n",
      "\n",
      "20/02/13 10:08:07 INFO CoarseGrainedExecutorBackend: Driver from 172.31.104.16:39477 disconnected during shutdown\n",
      "\n",
      "20/02/13 10:08:07 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "Container: container_e175_1568810042014_190518_02_000003 on awdex01015.aws.merckcloud.com_45454_1581588577429\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14340\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 160\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:08 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112380/filecache/106/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 aws-java-sdk-core-1.10.6.jar -> /data2/hadoop/yarn/local/filecache/31220/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 aws-java-sdk-kms-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/31226/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/31215/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/31217/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 azure-storage-4.2.0.jar -> /data2/hadoop/yarn/local/filecache/31224/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 commons-collections4-4.1.jar -> /data/hadoop/yarn/local/filecache/37937/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:08 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/37936/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:08 commons-lang3-3.4.jar -> /data2/hadoop/yarn/local/filecache/31228/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop  1024 Feb 13 10:08 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 datanucleus-api-jdo-3.2.6.jar -> /data/hadoop/yarn/local/filecache/31243/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 datanucleus-core-3.2.10.jar -> /data1/hadoop/yarn/local/filecache/31239/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 datanucleus-rdbms-3.2.9.jar -> /data2/hadoop/yarn/local/filecache/31238/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:08 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/31221/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:08 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/37929/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:08 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31216/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:08 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31219/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:08 hive-site.xml -> /data/hadoop/yarn/local/filecache/31240/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 jackson-annotations-2.4.0.jar -> /data2/hadoop/yarn/local/filecache/31230/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 jackson-core-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/31231/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 jackson-databind-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/31218/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:08 joda-time-2.5.jar -> /data1/hadoop/yarn/local/filecache/31223/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:08 json-simple-1.1.jar -> /data2/hadoop/yarn/local/filecache/31233/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21899 Feb 13 10:08 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:08 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/37940/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:08 ojdbc6.jar -> /data2/hadoop/yarn/local/filecache/37938/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    90 Feb 13 10:08 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/31227/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:08 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/31222/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:08 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31236/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:08 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/37935/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:08 poi-ooxml-3.17.jar -> /data/hadoop/yarn/local/filecache/37933/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/37930/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:08 py4j-0.9-src.zip -> /data1/hadoop/yarn/local/filecache/31702/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:08 pyspark.zip -> /data/hadoop/yarn/local/filecache/31703/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:08 qd_rdq_2.10-1.0.1-RC14.jar -> /data/hadoop/yarn/local/filecache/37934/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 scalaj-http_2.10-2.3.0.jar -> /data2/hadoop/yarn/local/filecache/37927/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:08 scala-library-2.10.5.jar -> /data2/hadoop/yarn/local/filecache/31232/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   100 Feb 13 10:08 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:08 __spark_conf__ -> /data/hadoop/yarn/local/usercache/s112380/filecache/105/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 spark-csv_2.10-1.5.0.jar -> /data/hadoop/yarn/local/filecache/37939/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 __spark__.jar -> /data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:08 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/37932/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:08 xmlbeans-2.6.0.jar -> /data2/hadoop/yarn/local/filecache/37931/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "86040587    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:08 .\n",
      "\n",
      "86040589    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:08 ./tmp\n",
      "\n",
      "86040594    4 -rw-------   1 s112380  hadoop       1024 Feb 13 10:08 ./container_tokens\n",
      "\n",
      "99000401 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:57 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "31113526 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:04 ./poi-ooxml-3.17.jar\n",
      "\n",
      "98205711   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 12:45 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "98459796  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 12:45 ./commons-lang3-3.4.jar\n",
      "\n",
      "98459654  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 12:45 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "86040579 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:04 ./ojdbc6.jar\n",
      "\n",
      "97566723 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:57 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "98205702  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 12:45 ./joda-time-2.5.jar\n",
      "\n",
      "98197507  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 12:45 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "98197516 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 12:45 ./guava-11.0.2.jar\n",
      "\n",
      "99000324   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 12:45 ./json-simple-1.1.jar\n",
      "\n",
      "80461859  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:04 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "52273159   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:04 ./commons-csv-1.1.jar\n",
      "\n",
      "98492419 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Oct 22 12:17 ./__spark__.jar\n",
      "\n",
      "31113536  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:04 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "86040590   24 -rwx------   1 s112380  hadoop      21899 Feb 13 10:08 ./launch_container.sh\n",
      "\n",
      "98459802   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 12:45 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "31113560  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:04 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "98459754  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 12:45 ./azure-storage-4.2.0.jar\n",
      "\n",
      "98459808 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 12:45 ./scala-library-2.10.5.jar\n",
      "\n",
      "52273156  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:04 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "109412355    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:57 ./hive-site.xml\n",
      "\n",
      "80461876 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:04 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "69165091    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:06 ./__spark_conf__\n",
      "\n",
      "69165126    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:06 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "69165106    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:06 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "69165122   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:06 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "69165113    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:06 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "69165116    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:06 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "69165103   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:06 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "69165107    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:06 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "69165117    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:06 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "69165114    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:06 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "69165104   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:06 ./__spark_conf__/log4j.properties\n",
      "\n",
      "69165131    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:06 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "69165118    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:06 ./__spark_conf__/topology_script.py\n",
      "\n",
      "69165119    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:06 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "69165138    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:06 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "69165108   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:06 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "69165140    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:06 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "69165129    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:06 ./__spark_conf__/health_check\n",
      "\n",
      "69165137    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:06 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "69165125    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:06 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "69165139    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:06 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "69165128    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:06 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "69165133    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:06 ./__spark_conf__/slaves\n",
      "\n",
      "69165109   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:06 ./__spark_conf__/core-site.xml\n",
      "\n",
      "69165141    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:06 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "69165134    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:06 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "69165136    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:06 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "69165111    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:06 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "69165127    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:06 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "69165105    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:06 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "69165120    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:06 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "69165123    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:06 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "69165115    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:06 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "69165112    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:06 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "69165124    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:06 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "69165130    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:06 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "69165132    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:06 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "69165135    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:06 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "98197510   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 12:45 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "98459805  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 12:45 ./jackson-core-2.4.4.jar\n",
      "\n",
      "98459744  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 12:45 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "109379586 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:57 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "52305929  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:06 ./__app__.jar\n",
      "\n",
      "98459748  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 12:45 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "94322694   44 -r-xr-xr-x   1 yarn     hadoop      44846 Nov  1 04:14 ./py4j-0.9-src.zip\n",
      "\n",
      "31113539  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:04 ./commons-collections4-4.1.jar\n",
      "\n",
      "98205708  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 12:45 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "80461880 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:04 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "109412358  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:57 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "98205699   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 12:45 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "85565443 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:04 ./poi-3.17.jar\n",
      "\n",
      "52305923  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:04 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "99000398   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:57 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "80461845  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:04 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "31105364  356 -r-xr-xr-x   1 yarn     hadoop     357604 Nov  1 04:14 ./pyspark.zip\n",
      "\n",
      "98197513 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 12:45 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21899\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export SPARK_LOG_URL_STDOUT=\"https://awdex01015.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000003/s112380/stdout?start=-4096\"\n",
      "\n",
      "export NM_HOST=\"awdex01015.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003,/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003,/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/,/data2/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_LOG_URL_STDERR=\"https://awdex01015.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000003/s112380/stderr?start=-4096\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_02_000003\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31228/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37934/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31217/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31226/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31218/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31233/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31230/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37937/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37927/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31215/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31236/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31238/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37936/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/usercache/s112380/filecache/106/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31243/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37940/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37929/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31224/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31702/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37933/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31703/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31216/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31227/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37931/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31240/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31223/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37935/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/usercache/s112380/filecache/105/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31231/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31239/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31219/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37930/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31221/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37938/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31220/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37932/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37939/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31232/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31222/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir=$PWD/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=33042' -Dspark.yarn.app.container.log.dir=/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.97.178:33042 --executor-id 2 --hostname awdex01015.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/stdout 2> /data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000003/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:37684\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:08:14 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:08:14 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:14 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:15 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:15 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:15 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:08:15 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:08:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01015.aws.merckcloud.com:40202]\n",
      "\n",
      "20/02/13 10:08:15 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 40202.\n",
      "\n",
      "20/02/13 10:08:16 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-4a979b67-a235-415c-88fe-81dd76ba9d1c\n",
      "\n",
      "20/02/13 10:08:16 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-8812a7e7-459a-4aef-ba6f-62fa028acaa3\n",
      "\n",
      "20/02/13 10:08:16 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-4acac262-f92f-4a1b-960c-bc8f63d278de\n",
      "\n",
      "20/02/13 10:08:16 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "\n",
      "20/02/13 10:08:16 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.97.178:33042\n",
      "\n",
      "20/02/13 10:08:16 INFO CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "\n",
      "20/02/13 10:08:16 INFO Executor: Starting executor ID 2 on host awdex01015.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59047.\n",
      "\n",
      "20/02/13 10:08:16 INFO NettyBlockTransferService: Server created on 59047\n",
      "\n",
      "20/02/13 10:08:16 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:08:16 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:08:41 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@11d5b84f,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:08:44 WARN TransportResponseHandler: Ignoring response for RPC 6791207939523400570 from /172.31.97.178:33042 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:08:54 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@11d5b84f,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 2 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:07 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@11d5b84f,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 3 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@11d5b84f,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\t... 13 more\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:17 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@7a716971,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 10 seconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)\n",
      "\n",
      "\tat scala.util.Try$.apply(Try.scala:161)\n",
      "\n",
      "\tat scala.util.Failure.recover(Try.scala:185)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n",
      "\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n",
      "\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)\n",
      "\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)\n",
      "\n",
      "\tat scala.concurrent.Promise$class.tryFailure(Promise.scala:112)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:245)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds\n",
      "\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:246)\n",
      "\n",
      "\t... 7 more\n",
      "\n",
      "20/02/13 10:09:25 WARN TransportChannelHandler: Exception in connection from /172.31.97.178:33042\n",
      "\n",
      "java.io.IOException: Connection reset by peer\n",
      "\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n",
      "\n",
      "\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)\n",
      "\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)\n",
      "\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)\n",
      "\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:09:25 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from awdex01010.aws.merckcloud.com/172.31.97.178:33042 is closed\n",
      "\n",
      "20/02/13 10:09:25 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@7a716971,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 2 attempts\n",
      "\n",
      "java.io.IOException: Connection reset by peer\n",
      "\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n",
      "\n",
      "\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)\n",
      "\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)\n",
      "\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)\n",
      "\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:09:25 WARN CoarseGrainedExecutorBackend: An unknown (awdex01010.aws.merckcloud.com:33042) driver disconnected.\n",
      "\n",
      "20/02/13 10:09:28 ERROR TransportClient: Failed to send RPC 7277779866278153791 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:28 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@7a716971,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 3 attempts\n",
      "\n",
      "java.io.IOException: Failed to send RPC 7277779866278153791 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@7a716971,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.io.IOException: Failed to send RPC 7277779866278153791 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\t... 1 more\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:28 ERROR TransportClient: Failed to send RPC 6989706538468269164 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:28 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@5edae9ef,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 1 attempts\n",
      "\n",
      "java.io.IOException: Failed to send RPC 6989706538468269164 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:31 ERROR TransportClient: Failed to send RPC 8076549729833742012 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:31 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@5edae9ef,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 2 attempts\n",
      "\n",
      "java.io.IOException: Failed to send RPC 8076549729833742012 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:34 ERROR TransportClient: Failed to send RPC 6746039567232729282 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:34 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@5edae9ef,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 3 attempts\n",
      "\n",
      "java.io.IOException: Failed to send RPC 6746039567232729282 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@5edae9ef,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.io.IOException: Failed to send RPC 6746039567232729282 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\t... 1 more\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:34 ERROR TransportClient: Failed to send RPC 6898960953420721396 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:34 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@3d36581a,BlockManagerId(2, awdex01015.aws.merckcloud.com, 59047))] in 1 attempts\n",
      "\n",
      "java.io.IOException: Failed to send RPC 6898960953420721396 to awdex01010.aws.merckcloud.com/172.31.97.178:33042: java.nio.channels.ClosedChannelException\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n",
      "\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n",
      "\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n",
      "\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\n",
      "20/02/13 10:09:37 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n",
      "\n",
      "20/02/13 10:09:37 INFO DiskBlockManager: Shutdown hook called\n",
      "\n",
      "20/02/13 10:09:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "Container: container_e175_1568810042014_190518_01_000001 on awdex01015.aws.merckcloud.com_45454_1581588577429\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14340\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 160\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:06 __app__.jar -> /data1/hadoop/yarn/local/usercache/s112380/filecache/106/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:06 aws-java-sdk-core-1.10.6.jar -> /data2/hadoop/yarn/local/filecache/31220/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:06 aws-java-sdk-kms-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/31226/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:06 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/31215/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:06 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/31217/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:06 azure-storage-4.2.0.jar -> /data2/hadoop/yarn/local/filecache/31224/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:06 commons-collections4-4.1.jar -> /data/hadoop/yarn/local/filecache/37937/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:06 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/37936/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    62 Feb 13 10:06 commons-lang3-3.4.jar -> /data2/hadoop/yarn/local/filecache/31228/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop   987 Feb 13 10:06 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:06 datanucleus-api-jdo-3.2.6.jar -> /data/hadoop/yarn/local/filecache/31243/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:06 datanucleus-core-3.2.10.jar -> /data1/hadoop/yarn/local/filecache/31239/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:06 datanucleus-rdbms-3.2.9.jar -> /data2/hadoop/yarn/local/filecache/31238/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:06 guava-11.0.2.jar -> /data1/hadoop/yarn/local/filecache/31221/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:06 gxppipelinecore_2.10-2.0.13.jar -> /data2/hadoop/yarn/local/filecache/37929/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:06 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31216/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:06 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31219/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:06 hive-site.xml -> /data/hadoop/yarn/local/filecache/31240/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:06 jackson-annotations-2.4.0.jar -> /data2/hadoop/yarn/local/filecache/31230/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:06 jackson-core-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/31231/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:06 jackson-databind-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/31218/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:06 joda-time-2.5.jar -> /data1/hadoop/yarn/local/filecache/31223/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:06 json-simple-1.1.jar -> /data2/hadoop/yarn/local/filecache/31233/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21845 Feb 13 10:06 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:06 lift-json_2.10-2.6.3.jar -> /data1/hadoop/yarn/local/filecache/37940/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:06 ojdbc6.jar -> /data2/hadoop/yarn/local/filecache/37938/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    90 Feb 13 10:06 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/31227/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:06 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/31222/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:06 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/31236/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:06 poi-3.17.jar -> /data2/hadoop/yarn/local/filecache/37935/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:06 poi-ooxml-3.17.jar -> /data/hadoop/yarn/local/filecache/37933/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:06 poi-ooxml-schemas-3.17.jar -> /data2/hadoop/yarn/local/filecache/37930/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    57 Feb 13 10:06 py4j-0.9-src.zip -> /data1/hadoop/yarn/local/filecache/31702/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:06 pyspark.zip -> /data/hadoop/yarn/local/filecache/31703/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:06 qd_rdq_2.10-1.0.1-RC14.jar -> /data/hadoop/yarn/local/filecache/37934/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:06 scalaj-http_2.10-2.3.0.jar -> /data2/hadoop/yarn/local/filecache/37927/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    65 Feb 13 10:06 scala-library-2.10.5.jar -> /data2/hadoop/yarn/local/filecache/31232/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   100 Feb 13 10:06 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:06 __spark_conf__ -> /data/hadoop/yarn/local/usercache/s112380/filecache/105/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:06 spark-csv_2.10-1.5.0.jar -> /data/hadoop/yarn/local/filecache/37939/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:06 __spark__.jar -> /data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:06 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:06 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/37932/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:06 xmlbeans-2.6.0.jar -> /data2/hadoop/yarn/local/filecache/37931/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "52305927    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:06 .\n",
      "\n",
      "99000324   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 12:45 ./json-simple-1.1.jar\n",
      "\n",
      "98459744  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 12:45 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "98492419 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Oct 22 12:17 ./__spark__.jar\n",
      "\n",
      "98197507  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 12:45 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "98459796  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 12:45 ./commons-lang3-3.4.jar\n",
      "\n",
      "80461880 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:04 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "52305923  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:04 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "98197510   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 12:45 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "98459808 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 12:45 ./scala-library-2.10.5.jar\n",
      "\n",
      "98459754  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 12:45 ./azure-storage-4.2.0.jar\n",
      "\n",
      "80461876 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:04 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "98205702  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 12:45 ./joda-time-2.5.jar\n",
      "\n",
      "98459805  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 12:45 ./jackson-core-2.4.4.jar\n",
      "\n",
      "98205699   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 12:45 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "98459802   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 12:45 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "109412355    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:57 ./hive-site.xml\n",
      "\n",
      "99000398   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:57 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "80461845  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:04 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "52305934    4 -rw-------   1 s112380  hadoop        987 Feb 13 10:06 ./container_tokens\n",
      "\n",
      "98459748  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 12:45 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "94322694   44 -r-xr-xr-x   1 yarn     hadoop      44846 Nov  1 04:14 ./py4j-0.9-src.zip\n",
      "\n",
      "109412358  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:57 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "52273156  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:04 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "31113526 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:04 ./poi-ooxml-3.17.jar\n",
      "\n",
      "31113560  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:04 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "98197516 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 12:45 ./guava-11.0.2.jar\n",
      "\n",
      "52305933   24 -rwx------   1 s112380  hadoop      21845 Feb 13 10:06 ./launch_container.sh\n",
      "\n",
      "109379586 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:57 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "85565443 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:04 ./poi-3.17.jar\n",
      "\n",
      "98197513 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 12:45 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "31113539  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:04 ./commons-collections4-4.1.jar\n",
      "\n",
      "98205711   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 12:45 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "98205708  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 12:45 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "31113536  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:04 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "52305929  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:06 ./__app__.jar\n",
      "\n",
      "69165091    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:06 ./__spark_conf__\n",
      "\n",
      "69165126    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:06 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "69165106    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:06 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "69165122   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:06 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "69165113    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:06 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "69165116    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:06 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "69165103   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:06 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "69165107    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:06 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "69165117    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:06 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "69165114    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:06 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "69165104   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:06 ./__spark_conf__/log4j.properties\n",
      "\n",
      "69165131    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:06 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "69165118    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:06 ./__spark_conf__/topology_script.py\n",
      "\n",
      "69165119    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:06 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "69165138    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:06 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "69165108   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:06 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "69165140    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:06 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "69165129    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:06 ./__spark_conf__/health_check\n",
      "\n",
      "69165137    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:06 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "69165125    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:06 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "69165139    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:06 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "69165128    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:06 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "69165133    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:06 ./__spark_conf__/slaves\n",
      "\n",
      "69165109   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:06 ./__spark_conf__/core-site.xml\n",
      "\n",
      "69165141    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:06 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "69165134    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:06 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "69165136    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:06 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "69165111    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:06 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "69165127    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:06 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "69165105    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:06 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "69165120    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:06 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "69165123    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:06 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "69165115    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:06 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "69165112    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:06 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "69165124    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:06 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "69165130    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:06 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "69165132    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:06 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "69165135    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:06 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "31105364  356 -r-xr-xr-x   1 yarn     hadoop     357604 Nov  1 04:14 ./pyspark.zip\n",
      "\n",
      "99000401 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:57 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "80461859  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:04 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "52305932    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:06 ./tmp\n",
      "\n",
      "86040579 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:04 ./ojdbc6.jar\n",
      "\n",
      "98459654  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 12:45 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "97566723 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:57 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "52273159   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:04 ./commons-csv-1.1.jar\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21845\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export MAX_APP_ATTEMPTS=\"2\"\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export APP_SUBMIT_TIME_ENV=\"1581588410329\"\n",
      "\n",
      "export NM_HOST=\"awdex01015.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export APPLICATION_WEB_PROXY_BASE=\"/proxy/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001,/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001,/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/,/data2/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_01_000001\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31228/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37934/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31217/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31226/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31218/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31233/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31230/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37937/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37927/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31215/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31236/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31238/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37936/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/usercache/s112380/filecache/106/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31243/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37940/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37929/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31224/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31702/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37933/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31703/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31216/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31227/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37931/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31240/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31223/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37935/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/usercache/s112380/filecache/105/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31231/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31239/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31219/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37930/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31221/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/37938/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31220/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/37932/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/37939/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31232/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/31222/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -Xmx1024m -Djava.io.tmpdir=$PWD/tmp '-Dlog4j.configuration=spark-log4j.properties' -Dhdp.version=2.5.5.0-157 -Dspark.yarn.app.container.log.dir=/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class 'com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport' --jar file:/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190517/container_e175_1568810042014_190517_01_000002/qd_rdq_2.10-1.0.1-RC14.jar --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf' --arg 'hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/business_metadata_TrackwiseOLDL_AuditExport.conf' --executor-memory 1024m --executor-cores 1 --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/stdout 2> /data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_01_000001/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:61533\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/31214/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data/hadoop/yarn/local/filecache/31237/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:06:54 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:06:55 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1568810042014_190518_000001\n",
      "\n",
      "20/02/13 10:06:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "20/02/13 10:06:56 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "\n",
      "20/02/13 10:06:56 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:06:56 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:06:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:06:56 INFO ApplicationMaster: Starting the user application in a separate Thread\n",
      "\n",
      "20/02/13 10:06:56 INFO ApplicationMaster: Waiting for spark context initialization\n",
      "\n",
      "20/02/13 10:06:56 INFO ApplicationMaster: Waiting for spark context initialization ... \n",
      "\n",
      "20/02/13 10:06:56 INFO SparkContext: Running Spark version 1.6.3\n",
      "\n",
      "20/02/13 10:06:56 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:06:56 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/02/13 10:06:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:06:56 INFO Utils: Successfully started service 'sparkDriver' on port 39477.\n",
      "\n",
      "20/02/13 10:06:56 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:06:57 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:06:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.31.104.16:49726]\n",
      "\n",
      "20/02/13 10:06:57 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 49726.\n",
      "\n",
      "20/02/13 10:06:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "\n",
      "20/02/13 10:06:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "\n",
      "20/02/13 10:06:57 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-46128332-169c-4973-a0ea-deb0a7ebae17\n",
      "\n",
      "20/02/13 10:06:57 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-e4e5e07e-7ab3-4451-9101-25fc03cd1a4e\n",
      "\n",
      "20/02/13 10:06:57 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-05460a46-eb2b-499e-9e75-6dbe301c00fb\n",
      "\n",
      "20/02/13 10:06:57 INFO MemoryStore: MemoryStore started with capacity 487.9 MB\n",
      "\n",
      "20/02/13 10:06:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "\n",
      "20/02/13 10:06:57 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "\n",
      "20/02/13 10:06:57 INFO Utils: Successfully started service 'SparkUI' on port 17259.\n",
      "\n",
      "20/02/13 10:06:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.31.104.16:17259\n",
      "\n",
      "20/02/13 10:06:57 INFO YarnClusterScheduler: Created YarnClusterScheduler\n",
      "\n",
      "20/02/13 10:06:57 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1568810042014_190518 and attemptId Some(appattempt_1568810042014_190518_000001)\n",
      "\n",
      "20/02/13 10:06:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10607.\n",
      "\n",
      "20/02/13 10:06:57 INFO NettyBlockTransferService: Server created on 10607\n",
      "\n",
      "20/02/13 10:06:57 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:06:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.104.16:10607 with 487.9 MB RAM, BlockManagerId(driver, 172.31.104.16, 10607)\n",
      "\n",
      "20/02/13 10:06:57 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:06:57 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.31.104.16:39477)\n",
      "\n",
      "20/02/13 10:06:57 INFO YarnRMClient: Registering the ApplicationMaster\n",
      "\n",
      "20/02/13 10:06:57 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Will request 2 executor containers, each with 1 cores and 1408 MB memory including 384 MB overhead\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Container request (host: Any, capability: <memory:1408, vCores:1>)\n",
      "\n",
      "20/02/13 10:06:58 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals\n",
      "\n",
      "20/02/13 10:06:58 INFO AMRMClientImpl: Received new token for : awdex01010.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:06:58 INFO AMRMClientImpl: Received new token for : awdex01009.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Launching container container_e175_1568810042014_190518_01_000002 for on host awdex01010.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.104.16:39477,  executorHostname: awdex01010.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Launching container container_e175_1568810042014_190518_01_000003 for on host awdex01009.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Starting Executor Container\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.31.104.16:39477,  executorHostname: awdex01009.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Starting Executor Container\n",
      "\n",
      "20/02/13 10:06:58 INFO YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.\n",
      "\n",
      "20/02/13 10:06:58 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "\n",
      "20/02/13 10:06:58 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Setting up ContainerLaunchContext\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Setting up ContainerLaunchContext\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Preparing Local resources\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Preparing Local resources\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534148696604 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1551865169204 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190503288 timestamp: 1536921442593 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1551865168521 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1581588408931 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534148683990 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1551865168745 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534148701957 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1551865169151 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534148684575 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip\" } size: 357604 timestamp: 1534148696849 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534148696362 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534148696706 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534148685010 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1551865168577 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534148684121 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534148696265 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534148685067 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1551865168863 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534148684624 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534148684682 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534148684206 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1-RC14.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1552982988554 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534148774104 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1551865168953 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml\" } size: 1920 timestamp: 1534148696504 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534148684923 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip\" } size: 155669 timestamp: 1581588410189 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534148684835 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534148684764 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534148684282 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1551865168637 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1551865168471 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1551865169314 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534148684447 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1551865169434 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1551865169256 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534148684047 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534148684511 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534148696458 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534148684346 type: FILE visibility: PUBLIC)\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: Prepared Local resources Map(oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" } size: 22715 timestamp: 1534148696604 type: FILE visibility: PUBLIC, scalaj-http_2.10-2.3.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar\" } size: 162717 timestamp: 1551865169204 type: FILE visibility: PUBLIC, __spark__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\" } size: 190503288 timestamp: 1536921442593 type: FILE visibility: PUBLIC, commons-csv-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar\" } size: 36888 timestamp: 1551865168521 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1581588408931 type: FILE visibility: PRIVATE, aws-java-sdk-core-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar\" } size: 516062 timestamp: 1534148683990 type: FILE visibility: PUBLIC, ojdbc6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar\" } size: 1988051 timestamp: 1551865168745 type: FILE visibility: PUBLIC, spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" } size: 190578782 timestamp: 1534148701957 type: FILE visibility: PUBLIC, poi-ooxml-schemas-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar\" } size: 5924600 timestamp: 1551865169151 type: FILE visibility: PUBLIC, hadoop-azure-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar\" } size: 213154 timestamp: 1534148684575 type: FILE visibility: PUBLIC, pyspark.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip\" } size: 357604 timestamp: 1534148696849 type: FILE visibility: PUBLIC, datanucleus-core-3.2.10.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar\" } size: 1890075 timestamp: 1534148696362 type: FILE visibility: PUBLIC, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip\" } size: 44846 timestamp: 1534148696706 type: FILE visibility: PUBLIC, oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" } size: 12749 timestamp: 1534148685010 type: FILE visibility: PUBLIC, gxppipelinecore_2.10-2.0.13.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar\" } size: 349304 timestamp: 1551865168577 type: FILE visibility: PUBLIC, aws-java-sdk-s3-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar\" } size: 570101 timestamp: 1534148684121 type: FILE visibility: PUBLIC, datanucleus-api-jdo-3.2.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar\" } size: 339666 timestamp: 1534148696265 type: FILE visibility: PUBLIC, oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" } size: 52413 timestamp: 1534148685067 type: FILE visibility: PUBLIC, poi-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar\" } size: 2701171 timestamp: 1551865168863 type: FILE visibility: PUBLIC, jackson-annotations-2.4.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar\" } size: 38605 timestamp: 1534148684624 type: FILE visibility: PUBLIC, jackson-core-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar\" } size: 225302 timestamp: 1534148684682 type: FILE visibility: PUBLIC, azure-keyvault-core-0.8.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar\" } size: 10092 timestamp: 1534148684206 type: FILE visibility: PUBLIC, qd_rdq_2.10-1.0.1-RC14.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar\" } size: 223737 timestamp: 1552982988554 type: FILE visibility: PUBLIC, scala-library-2.10.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar\" } size: 7130772 timestamp: 1534148774104 type: FILE visibility: PUBLIC, poi-ooxml-3.17.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar\" } size: 1479023 timestamp: 1551865168953 type: FILE visibility: PUBLIC, hive-site.xml -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml\" } size: 1920 timestamp: 1534148696504 type: FILE visibility: PUBLIC, json-simple-1.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar\" } size: 16046 timestamp: 1534148684923 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip\" } size: 155669 timestamp: 1581588410189 type: ARCHIVE visibility: PRIVATE, joda-time-2.5.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar\" } size: 588001 timestamp: 1534148684835 type: FILE visibility: PUBLIC, jackson-databind-2.4.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar\" } size: 1076926 timestamp: 1534148684764 type: FILE visibility: PUBLIC, azure-storage-4.2.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar\" } size: 745325 timestamp: 1534148684282 type: FILE visibility: PUBLIC, lift-json_2.10-2.6.3.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar\" } size: 486892 timestamp: 1551865168637 type: FILE visibility: PUBLIC, commons-collections4-4.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar\" } size: 751238 timestamp: 1551865168471 type: FILE visibility: PUBLIC, univocity-parsers-1.5.1.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar\" } size: 148962 timestamp: 1551865169314 type: FILE visibility: PUBLIC, guava-11.0.2.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar\" } size: 1648200 timestamp: 1534148684447 type: FILE visibility: PUBLIC, xmlbeans-2.6.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar\" } size: 2730866 timestamp: 1551865169434 type: FILE visibility: PUBLIC, spark-csv_2.10-1.5.0.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar\" } size: 165361 timestamp: 1551865169256 type: FILE visibility: PUBLIC, aws-java-sdk-kms-1.10.6.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar\" } size: 258578 timestamp: 1534148684047 type: FILE visibility: PUBLIC, hadoop-aws-2.7.3.2.5.5.0-157.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar\" } size: 165879 timestamp: 1534148684511 type: FILE visibility: PUBLIC, datanucleus-rdbms-3.2.9.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar\" } size: 1809447 timestamp: 1534148696458 type: FILE visibility: PUBLIC, commons-lang3-3.4.jar -> resource { scheme: \"hdfs\" host: \"aaprod\" port: -1 file: \"/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar\" } size: 434678 timestamp: 1534148684346 type: FILE visibility: PUBLIC)\n",
      "\n",
      "20/02/13 10:06:58 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\n",
      "\n",
      "20/02/13 10:06:58 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar\n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: \n",
      "\n",
      "===============================================================================\n",
      "\n",
      "YARN executor launch context:\n",
      "\n",
      "  env:\n",
      "\n",
      "    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\n",
      "\n",
      "    SPARK_LOG_URL_STDERR -> https://awdex01010.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000002/s112380/stderr?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\n",
      "\n",
      "    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1568810042014_190518\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 155669\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\n",
      "\n",
      "    SPARK_USER -> s112380\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581588410189\n",
      "\n",
      "    SPARK_YARN_MODE -> true\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\n",
      "\n",
      "    SPARK_LOG_URL_STDOUT -> https://awdex01010.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000002/s112380/stdout?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES -> hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE\n",
      "\n",
      "\n",
      "\n",
      "  command:\n",
      "\n",
      "    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39477' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.104.16:39477 --executor-id 1 --hostname awdex01010.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "      \n",
      "\n",
      "20/02/13 10:06:58 INFO ExecutorRunnable: \n",
      "\n",
      "===============================================================================\n",
      "\n",
      "YARN executor launch context:\n",
      "\n",
      "  env:\n",
      "\n",
      "    CLASSPATH -> $PWD/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES -> hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\n",
      "\n",
      "    SPARK_LOG_URL_STDERR -> https://awdex01009.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000003/s112380/stderr?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\n",
      "\n",
      "    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1568810042014_190518\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 155669\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\n",
      "\n",
      "    SPARK_USER -> s112380\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1581588410189\n",
      "\n",
      "    SPARK_YARN_MODE -> true\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\n",
      "\n",
      "    SPARK_LOG_URL_STDOUT -> https://awdex01009.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_01_000003/s112380/stdout?start=-4096\n",
      "\n",
      "    SPARK_YARN_CACHE_FILES -> hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\n",
      "\n",
      "    SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE\n",
      "\n",
      "\n",
      "\n",
      "  command:\n",
      "\n",
      "    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39477' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.104.16:39477 --executor-id 2 --hostname awdex01009.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "      \n",
      "\n",
      "20/02/13 10:06:58 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01010.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:06:58 INFO ContainerManagementProtocolProxy: Opening proxy : awdex01009.aws.merckcloud.com:45454\n",
      "\n",
      "20/02/13 10:07:03 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01009.aws.merckcloud.com:43479) with ID 2\n",
      "\n",
      "20/02/13 10:07:03 INFO BlockManagerMasterEndpoint: Registering block manager awdex01009.aws.merckcloud.com:38575 with 511.1 MB RAM, BlockManagerId(2, awdex01009.aws.merckcloud.com, 38575)\n",
      "\n",
      "20/02/13 10:07:04 INFO YarnClusterSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (awdex01010.aws.merckcloud.com:42500) with ID 1\n",
      "\n",
      "20/02/13 10:07:04 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "\n",
      "20/02/13 10:07:04 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done\n",
      "\n",
      "20/02/13 10:07:04 INFO BlockManagerMasterEndpoint: Registering block manager awdex01010.aws.merckcloud.com:35056 with 511.1 MB RAM, BlockManagerId(1, awdex01010.aws.merckcloud.com, 35056)\n",
      "\n",
      "20/02/13 10:07:04 INFO PipelineConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/pipeline.conf\n",
      "\n",
      "20/02/13 10:07:04 INFO FilesystemAccess$: Max Pipeline Run ID found:52\n",
      "\n",
      "20/02/13 10:07:04 INFO InputConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/input.conf\n",
      "\n",
      "20/02/13 10:07:05 INFO PrepareTrackwiseOLDL_AuditExport$: Checking input path hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport\n",
      "\n",
      "20/02/13 10:07:05 INFO EnvironmentConf: Configuration file loaded hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/conf/environment.conf\n",
      "\n",
      "20/02/13 10:07:05 INFO AtlasRepository: Atlas URLs provided for this pipeline: https://awdex01007.aws.merckcloud.com:21443/api/atlas/,https://awdex01008.aws.merckcloud.com:21443/api/atlas/\n",
      "\n",
      "20/02/13 10:07:05 INFO AtlasRepository: response: HttpResponse(,302,Map(Content-Length -> Vector(0), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Location -> Vector(https://awdex01008.aws.merckcloud.com:21443/api/atlas/entities?type=DataSet_mc_v2_0&amp;property=fileLocation&amp;value=hdfs%3A%2F%2Faaprod%2Fmc_staging%2Frdq_uc7_files%2Ftrackwise_oldl_auditexport%2F00_00_01%2Frun_000052%2Fraw%2FTrackwiseOLDL_AuditExport), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=18ucw0ry0mgaf1shiau2p9smo5;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 302 Found), X-Frame-Options -> Vector(DENY)))\n",
      "\n",
      "20/02/13 10:07:05 WARN AtlasRepository: Atlas request to https://awdex01007.aws.merckcloud.com:21443/api/atlas/entities failed with status 3xx or 5xx.\n",
      "\n",
      "20/02/13 10:07:05 WARN AtlasRepository: response.body: \n",
      "\n",
      "20/02/13 10:07:11 INFO AtlasRepository: response: HttpResponse({\"requestId\":\"qtp1489092624-4723694 - 288c3ea6-3f45-4052-b660-c642818130ed\",\"definition\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"31676f18-8ba9-4a66-81aa-02ee048ca9f3\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\"* * * * *\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs:\\/\\/aaprod\\/mc_staging\\/rdq_uc7_files\\/trackwise_oldl_auditexport\\/00_00_01\\/run_000052\\/raw\\/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aaprod_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000052_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-13T10:06:13.284Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-13T10:06:13.628Z\",\"geoScope\":\"NA\",\"owner\":\"s112380\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: deda1x3044.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}},200,Map(Content-Type -> Vector(application/json; charset=UTF-8), Expires -> Vector(Thu, 01 Jan 1970 00:00:00 GMT), Server -> Vector(Jetty(9.2.12.v20150709)), Set-Cookie -> Vector(ATLASSESSIONID=1jhvtbhx7ttqy1lh9iutrpbivs;Path=/;Secure;HttpOnly), Status -> Vector(HTTP/1.1 200 OK), Transfer-Encoding -> Vector(chunked), X-Frame-Options -> Vector(DENY)))\n",
      "\n",
      "20/02/13 10:07:11 INFO AtlasRepository: Atlas request to https://awdex01008.aws.merckcloud.com:21443/api/atlas/entities succeeded.\n",
      "\n",
      "20/02/13 10:07:11 INFO MetadataAccess: getEntityByFileLocation: {\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Reference\",\"id\":{\"jsonClass\":\"org.apache.atlas.typesystem.json.InstanceSerialization$_Id\",\"id\":\"31676f18-8ba9-4a66-81aa-02ee048ca9f3\",\"version\":0,\"typeName\":\"DataSet_mc_v2_0\",\"state\":\"ACTIVE\"},\"typeName\":\"DataSet_mc_v2_0\",\"values\":{\"name\":\"TrackwiseOLDL_AuditExport\",\"shortTitle\":\"RDQ Trackwise Old Landscape Audit Export Report\",\"ingestType\":\"batchSnapshot\",\"archivingRetentionDate\":\"NA\",\"updateFrequency\":\"\\\"* * * * *\\\"\",\"description\":\"manually collected data from RDQ: Trackwise Old Landscape Audit Export Report\",\"dataSteward\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"dataCustodian\":\"[ {name: Galante Valerio, firstName: Nuno, muid: M221407} ]\",\"approvedPurpose\":\"TrackwiseOLDL_AuditExport\",\"retentionSchedule\":\"NA\",\"subDomain\":\"NA\",\"sector\":\"HC\",\"dataOwner\":\"[{sector: HC, subDomain: RDQ}]\",\"fileLocation\":\"hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport\",\"replicationRetentionDate\":\"NA\",\"qualifiedName\":\"DataSet_mc_v2_0_hdfs___aaprod_mc_staging_rdq_uc7_files_trackwise_oldl_auditexport_00_00_01_run_000052_raw_TrackwiseOLDL_AuditExport\",\"informationClassification\":\"confidential\",\"collectionStatus\":\"NA\",\"creationDate\":\"2020-02-13T10:06:13.284Z\",\"metaDataVersion\":\"v2_0\",\"firstCreation\":\"2020-02-13T10:06:13.628Z\",\"geoScope\":\"NA\",\"owner\":\"s112380\",\"dataPublisher\":\"[{sector: HC, subDomain: RDQ}]\",\"sourceSystem\":\"{sourceSystemName: GxP sFTP server, sourceSystemConnection: deda1x3044.merckgroup.com:22}\",\"geoJurisdiction\":\"NA\"},\"traitNames\":[],\"traits\":{}}\n",
      "\n",
      "20/02/13 10:07:11 INFO PrepareTrackwiseOLDL_AuditExport$: Excel Input path:hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/run_000052/raw/TrackwiseOLDL_AuditExport/Audit_Export_Report_01_20200203_v2__m271552_2020-02-12T13-02-07+0100.xlsx\n",
      "\n",
      "20/02/13 10:08:07 WARN NioEventLoop: Unexpected exception in the selector loop.\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat io.netty.util.internal.MpscLinkedQueue.offer(MpscLinkedQueue.java:126)\n",
      "\n",
      "\tat io.netty.util.internal.MpscLinkedQueue.add(MpscLinkedQueue.java:221)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.fetchFromScheduledTaskQueue(SingleThreadEventExecutor.java:259)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:346)\n",
      "\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n",
      "\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:08:07 ERROR ActorSystemImpl: exception on LARS timer thread\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:409)\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:08:07 ERROR ApplicationMaster: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Cur$CurLoadContext.attr(Cur.java:3044)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1440)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNodeChildren(Locale.java:1403)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.loadNode(Locale.java:1445)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.parseToXmlObject(Locale.java:1385)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.store.Locale.parseToXmlObject(Locale.java:1370)\n",
      "\n",
      "\tat org.apache.xmlbeans.impl.schema.SchemaTypeLoaderBase.parse(SchemaTypeLoaderBase.java:370)\n",
      "\n",
      "\tat org.apache.poi.POIXMLTypeLoader.parse(POIXMLTypeLoader.java:164)\n",
      "\n",
      "\tat org.openxmlformats.schemas.spreadsheetml.x2006.main.WorksheetDocument$Factory.parse(Unknown Source)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFSheet.read(XSSFSheet.java:226)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFSheet.onDocumentRead(XSSFSheet.java:218)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.parseSheet(XSSFWorkbook.java:443)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.onDocumentRead(XSSFWorkbook.java:408)\n",
      "\n",
      "\tat org.apache.poi.POIXMLDocument.load(POIXMLDocument.java:169)\n",
      "\n",
      "\tat org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:270)\n",
      "\n",
      "\tat org.apache.poi.ss.usermodel.WorkbookFactory.create(WorkbookFactory.java:184)\n",
      "\n",
      "\tat org.apache.poi.ss.usermodel.WorkbookFactory.create(WorkbookFactory.java:149)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.getExcelWorkbook(RdqTransformationTemplate.scala:150)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.doSpecificTransformation(RdqTransformationTemplate.scala:171)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.pipelinecore.TransformationStepTemplate$class.executeTransformationStep(TransformationStepTemplate.scala:42)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.executeTransformationStep(RdqTransformationTemplate.scala:25)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.RdqTransformationTemplate.main(RdqTransformationTemplate.scala:241)\n",
      "\n",
      "\tat com.merck.mcloud.gxp.rdq.qd.transformations.PrepareTrackwiseOLDL_AuditExport.main(PrepareTrackwiseOLDL_AuditExport.scala)\n",
      "\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:561)\n",
      "\n",
      "20/02/13 10:08:07 INFO ActorSystemImpl: starting new LARS thread\n",
      "\n",
      "20/02/13 10:08:07 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriverActorSystem-scheduler-1] shutting down ActorSystem [sparkDriverActorSystem]\n",
      "\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:409)\n",
      "\n",
      "\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "20/02/13 10:08:07 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.OutOfMemoryError: GC overhead limit exceeded)\n",
      "\n",
      "20/02/13 10:08:07 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n",
      "\n",
      "20/02/13 10:08:07 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n",
      "\n",
      "20/02/13 10:08:07 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "\n",
      "20/02/13 10:08:07 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n",
      "\n",
      "20/02/13 10:08:07 INFO SparkUI: Stopped Spark web UI at http://172.31.104.16:17259\n",
      "\n",
      "20/02/13 10:08:07 INFO YarnAllocator: Driver requested a total number of 0 executor(s).\n",
      "\n",
      "20/02/13 10:08:07 INFO YarnClusterSchedulerBackend: Shutting down all executors\n",
      "\n",
      "20/02/13 10:08:07 INFO YarnClusterSchedulerBackend: Asking each executor to shut down\n",
      "\n",
      "20/02/13 10:08:07 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "\n",
      "(serviceOption=None,\n",
      "\n",
      " services=List(),\n",
      "\n",
      " started=false)\n",
      "\n",
      "20/02/13 10:08:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "\n",
      "20/02/13 10:08:08 INFO MemoryStore: MemoryStore cleared\n",
      "\n",
      "20/02/13 10:08:08 INFO BlockManager: BlockManager stopped\n",
      "\n",
      "20/02/13 10:08:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "\n",
      "20/02/13 10:08:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "\n",
      "20/02/13 10:08:08 INFO SparkContext: Successfully stopped SparkContext\n",
      "\n",
      "20/02/13 10:08:08 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "20/02/13 10:08:08 INFO ShutdownHookManager: Deleting directory /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-654052ac-07be-41ef-82a9-3c2ec15d267c\n",
      "\n",
      "20/02/13 10:08:08 INFO ShutdownHookManager: Deleting directory /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-72152ee1-bcf8-42ca-bf20-ad975b5c4c4d\n",
      "\n",
      "20/02/13 10:08:08 INFO ShutdownHookManager: Deleting directory /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/spark-10cdc451-36bf-454f-b6f4-50c3d33378bb\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "Container: container_e175_1568810042014_190518_02_000002 on awdex01016.aws.merckcloud.com_45454_1581588577458\n",
      "\n",
      "=============================================================================================================\n",
      "\n",
      "LogType:stderr\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14240\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data1/hadoop/yarn/local/filecache/33284/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/data2/hadoop/yarn/local/filecache/31211/spark-hdp-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: Found binding in [jar:file:/usr/hdp/2.5.5.0-157/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\n",
      "20/02/13 10:08:16 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: Changing view acls to: s112380\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: Changing modify acls to: s112380\n",
      "\n",
      "20/02/13 10:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(s112380); users with modify permissions: Set(s112380)\n",
      "\n",
      "20/02/13 10:08:18 INFO Slf4jLogger: Slf4jLogger started\n",
      "\n",
      "20/02/13 10:08:18 INFO Remoting: Starting remoting\n",
      "\n",
      "20/02/13 10:08:18 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@awdex01016.aws.merckcloud.com:36900]\n",
      "\n",
      "20/02/13 10:08:18 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 36900.\n",
      "\n",
      "20/02/13 10:08:18 INFO DiskBlockManager: Created local directory at /data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-ca9b8fd6-7b1f-495f-8fe3-926e7b7792a6\n",
      "\n",
      "20/02/13 10:08:18 INFO DiskBlockManager: Created local directory at /data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-9499e76c-c1fb-4fd6-a9b0-2c254556b539\n",
      "\n",
      "20/02/13 10:08:18 INFO DiskBlockManager: Created local directory at /data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/blockmgr-0197636d-25d5-4432-a9ff-433f5123cddf\n",
      "\n",
      "20/02/13 10:08:18 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "\n",
      "20/02/13 10:08:18 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@172.31.97.178:33042\n",
      "\n",
      "20/02/13 10:08:18 INFO CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "\n",
      "20/02/13 10:08:18 INFO Executor: Starting executor ID 1 on host awdex01016.aws.merckcloud.com\n",
      "\n",
      "20/02/13 10:08:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49116.\n",
      "\n",
      "20/02/13 10:08:18 INFO NettyBlockTransferService: Server created on 49116\n",
      "\n",
      "20/02/13 10:08:18 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "\n",
      "20/02/13 10:08:18 INFO BlockManagerMaster: Registered BlockManager\n",
      "\n",
      "20/02/13 10:08:57 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@18ae00ef,BlockManagerId(1, awdex01016.aws.merckcloud.com, 49116))] in 1 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:10 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@18ae00ef,BlockManagerId(1, awdex01016.aws.merckcloud.com, 49116))] in 2 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:23 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@18ae00ef,BlockManagerId(1, awdex01016.aws.merckcloud.com, 49116))] in 3 attempts\n",
      "\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:23 WARN Executor: Issue communicating with driver in heartbeater\n",
      "\n",
      "org.apache.spark.SparkException: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@18ae00ef,BlockManagerId(1, awdex01016.aws.merckcloud.com, 49116))]\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:476)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:505)\n",
      "\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n",
      "\n",
      "\tat org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:505)\n",
      "\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)\n",
      "\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)\n",
      "\n",
      "\t... 13 more\n",
      "\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\n",
      "\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n",
      "\n",
      "\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n",
      "\n",
      "\tat scala.concurrent.Await$.result(package.scala:107)\n",
      "\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\n",
      "\t... 14 more\n",
      "\n",
      "20/02/13 10:09:25 WARN TransportResponseHandler: Ignoring response for RPC 5303661316006053532 from /172.31.97.178:33042 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:09:25 WARN TransportResponseHandler: Ignoring response for RPC 8995168592856830024 from /172.31.97.178:33042 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:09:25 WARN TransportResponseHandler: Ignoring response for RPC 8266383342405040434 from /172.31.97.178:33042 (81 bytes) since it is not outstanding\n",
      "\n",
      "20/02/13 10:09:35 WARN CoarseGrainedExecutorBackend: An unknown (awdex01010.aws.merckcloud.com:33042) driver disconnected.\n",
      "\n",
      "20/02/13 10:09:37 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n",
      "\n",
      "20/02/13 10:09:37 INFO DiskBlockManager: Shutdown hook called\n",
      "\n",
      "20/02/13 10:09:37 INFO ShutdownHookManager: Shutdown hook called\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "\n",
      "\n",
      "\n",
      "LogType:stdout\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:0\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:stdout\n",
      "\n",
      "\n",
      "\n",
      "LogType:directory.info\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:14333\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "ls -l:\n",
      "\n",
      "total 160\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    81 Feb 13 10:08 __app__.jar -> /data/hadoop/yarn/local/usercache/s112380/filecache/95/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 aws-java-sdk-core-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/33267/aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 aws-java-sdk-kms-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/33273/aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 aws-java-sdk-s3-1.10.6.jar -> /data1/hadoop/yarn/local/filecache/33262/aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 azure-keyvault-core-0.8.0.jar -> /data1/hadoop/yarn/local/filecache/33264/azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 azure-storage-4.2.0.jar -> /data1/hadoop/yarn/local/filecache/33271/azure-storage-4.2.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 commons-collections4-4.1.jar -> /data2/hadoop/yarn/local/filecache/40069/commons-collections4-4.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:08 commons-csv-1.1.jar -> /data1/hadoop/yarn/local/filecache/40068/commons-csv-1.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    61 Feb 13 10:08 commons-lang3-3.4.jar -> /data/hadoop/yarn/local/filecache/33275/commons-lang3-3.4.jar\n",
      "\n",
      "-rw------- 1 s112380 hadoop  1024 Feb 13 10:08 container_tokens\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    69 Feb 13 10:08 datanucleus-api-jdo-3.2.6.jar -> /data/hadoop/yarn/local/filecache/33289/datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 datanucleus-core-3.2.10.jar -> /data/hadoop/yarn/local/filecache/33286/datanucleus-core-3.2.10.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 datanucleus-rdbms-3.2.9.jar -> /data2/hadoop/yarn/local/filecache/33285/datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    56 Feb 13 10:08 guava-11.0.2.jar -> /data/hadoop/yarn/local/filecache/33268/guava-11.0.2.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    72 Feb 13 10:08 gxppipelinecore_2.10-2.0.13.jar -> /data1/hadoop/yarn/local/filecache/40061/gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    73 Feb 13 10:08 hadoop-aws-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/33263/hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    75 Feb 13 10:08 hadoop-azure-2.7.3.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/33266/hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:08 hive-site.xml -> /data/hadoop/yarn/local/filecache/33287/hive-site.xml\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    70 Feb 13 10:08 jackson-annotations-2.4.0.jar -> /data1/hadoop/yarn/local/filecache/33277/jackson-annotations-2.4.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 jackson-core-2.4.4.jar -> /data1/hadoop/yarn/local/filecache/33278/jackson-core-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 jackson-databind-2.4.4.jar -> /data2/hadoop/yarn/local/filecache/33265/jackson-databind-2.4.4.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    58 Feb 13 10:08 joda-time-2.5.jar -> /data2/hadoop/yarn/local/filecache/33270/joda-time-2.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    60 Feb 13 10:08 json-simple-1.1.jar -> /data2/hadoop/yarn/local/filecache/33280/json-simple-1.1.jar\n",
      "\n",
      "-rwx------ 1 s112380 hadoop 21897 Feb 13 10:08 launch_container.sh\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 lift-json_2.10-2.6.3.jar -> /data/hadoop/yarn/local/filecache/40072/lift-json_2.10-2.6.3.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    51 Feb 13 10:08 ojdbc6.jar -> /data1/hadoop/yarn/local/filecache/40070/ojdbc6.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    89 Feb 13 10:08 oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/33274/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    82 Feb 13 10:08 oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar -> /data/hadoop/yarn/local/filecache/33269/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    83 Feb 13 10:08 oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar -> /data2/hadoop/yarn/local/filecache/33283/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    53 Feb 13 10:08 poi-3.17.jar -> /data1/hadoop/yarn/local/filecache/40067/poi-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:08 poi-ooxml-3.17.jar -> /data1/hadoop/yarn/local/filecache/40065/poi-ooxml-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:08 poi-ooxml-schemas-3.17.jar -> /data/hadoop/yarn/local/filecache/40062/poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    56 Feb 13 10:08 py4j-0.9-src.zip -> /data/hadoop/yarn/local/filecache/33282/py4j-0.9-src.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    52 Feb 13 10:08 pyspark.zip -> /data2/hadoop/yarn/local/filecache/33288/pyspark.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    67 Feb 13 10:08 qd_rdq_2.10-1.0.1-RC14.jar -> /data2/hadoop/yarn/local/filecache/40066/qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    66 Feb 13 10:08 scalaj-http_2.10-2.3.0.jar -> /data/hadoop/yarn/local/filecache/40059/scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 scala-library-2.10.5.jar -> /data/hadoop/yarn/local/filecache/33279/scala-library-2.10.5.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop   101 Feb 13 10:08 spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar -> /data1/hadoop/yarn/local/filecache/33284/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    93 Feb 13 10:08 __spark_conf__ -> /data2/hadoop/yarn/local/usercache/s112380/filecache/94/__spark_conf__2027599449732736376.zip\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    64 Feb 13 10:08 spark-csv_2.10-1.5.0.jar -> /data/hadoop/yarn/local/filecache/40071/spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    63 Feb 13 10:08 __spark__.jar -> /data2/hadoop/yarn/local/filecache/31211/spark-hdp-assembly.jar\n",
      "\n",
      "drwxr-s--- 2 s112380 hadoop  4096 Feb 13 10:08 tmp\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    68 Feb 13 10:08 univocity-parsers-1.5.1.jar -> /data1/hadoop/yarn/local/filecache/40064/univocity-parsers-1.5.1.jar\n",
      "\n",
      "lrwxrwxrwx 1 s112380 hadoop    59 Feb 13 10:08 xmlbeans-2.6.0.jar -> /data2/hadoop/yarn/local/filecache/40063/xmlbeans-2.6.0.jar\n",
      "\n",
      "find -L . -maxdepth 5 -ls:\n",
      "\n",
      "704517    4 drwxr-s---   3 s112380  hadoop       4096 Feb 13 10:08 .\n",
      "\n",
      "153969077   16 -r-xr-xr-x   1 yarn     hadoop      12749 Oct 22 11:12 ./oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "532847  228 -r-xr-xr-x   1 yarn     hadoop     225302 Oct 22 11:12 ./jackson-core-2.4.4.jar\n",
      "\n",
      "131973223  580 -r-xr-xr-x   1 yarn     hadoop     588001 Oct 22 11:12 ./joda-time-2.5.jar\n",
      "\n",
      "311789 186304 -r-xr-xr-x   1 yarn     hadoop   190578782 Oct 22 12:50 ./spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "46145993  164 -r-xr-xr-x   1 yarn     hadoop     162717 Feb 13 10:05 ./scalaj-http_2.10-2.3.0.jar\n",
      "\n",
      "524593  564 -r-xr-xr-x   1 yarn     hadoop     570101 Oct 22 11:12 ./aws-java-sdk-s3-1.10.6.jar\n",
      "\n",
      "524608  260 -r-xr-xr-x   1 yarn     hadoop     258578 Oct 22 11:12 ./aws-java-sdk-kms-1.10.6.jar\n",
      "\n",
      "704518    4 drwxr-s---   2 s112380  hadoop       4096 Feb 13 10:08 ./tmp\n",
      "\n",
      "132923635  356 -r-xr-xr-x   1 yarn     hadoop     357604 Oct 22 12:50 ./pyspark.zip\n",
      "\n",
      "524599  508 -r-xr-xr-x   1 yarn     hadoop     516062 Oct 22 11:12 ./aws-java-sdk-core-1.10.6.jar\n",
      "\n",
      "153944068   44 -r-xr-xr-x   1 yarn     hadoop      44846 Oct 22 12:50 ./py4j-0.9-src.zip\n",
      "\n",
      "131973217 1056 -r-xr-xr-x   1 yarn     hadoop    1076926 Oct 22 11:12 ./jackson-databind-2.4.4.jar\n",
      "\n",
      "153969071 1616 -r-xr-xr-x   1 yarn     hadoop    1648200 Oct 22 11:12 ./guava-11.0.2.jar\n",
      "\n",
      "132923629   24 -r-xr-xr-x   1 yarn     hadoop      22715 Oct 22 12:50 ./oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "53100549    4 drwx------   2 s112380  ldapuser     4096 Feb 13 10:08 ./__spark_conf__\n",
      "\n",
      "32047138    4 -r-x------   1 s112380  ldapuser      945 Feb 13 10:08 ./__spark_conf__/taskcontroller.cfg\n",
      "\n",
      "32047119    4 -r-x------   1 s112380  ldapuser     2358 Feb 13 10:08 ./__spark_conf__/topology_script.py\n",
      "\n",
      "32047133    4 -r-x------   1 s112380  ldapuser      151 Feb 13 10:08 ./__spark_conf__/slaves\n",
      "\n",
      "32047128    4 -r-x------   1 s112380  ldapuser      661 Feb 13 10:08 ./__spark_conf__/mapred-env.sh\n",
      "\n",
      "32047110   24 -r-x------   1 s112380  ldapuser    24520 Feb 13 10:08 ./__spark_conf__/yarn-site.xml\n",
      "\n",
      "32047114    4 -r-x------   1 s112380  ldapuser     2250 Feb 13 10:08 ./__spark_conf__/yarn-env.cmd\n",
      "\n",
      "32047134    4 -r-x------   1 s112380  ldapuser      391 Feb 13 10:08 ./__spark_conf__/topology_mappings.data\n",
      "\n",
      "32047113    4 -r-x------   1 s112380  ldapuser     1631 Feb 13 10:08 ./__spark_conf__/kms-log4j.properties\n",
      "\n",
      "32047123    4 -r-x------   1 s112380  ldapuser     1020 Feb 13 10:08 ./__spark_conf__/commons-logging.properties\n",
      "\n",
      "32047137    4 -r-x------   1 s112380  ldapuser     2268 Feb 13 10:08 ./__spark_conf__/ssl-server.xml.example\n",
      "\n",
      "32047129    4 -r-x------   1 s112380  ldapuser     1602 Feb 13 10:08 ./__spark_conf__/health_check\n",
      "\n",
      "32047107    8 -r-x------   1 s112380  ldapuser     5367 Feb 13 10:08 ./__spark_conf__/hadoop-env.sh\n",
      "\n",
      "32047139    8 -r-x------   1 s112380  ldapuser     4113 Feb 13 10:08 ./__spark_conf__/mapred-queues.xml.template\n",
      "\n",
      "32047136    4 -r-x------   1 s112380  ldapuser      951 Feb 13 10:08 ./__spark_conf__/mapred-env.cmd\n",
      "\n",
      "32047125    4 -r-x------   1 s112380  ldapuser      238 Feb 13 10:08 ./__spark_conf__/yarn_jaas.conf\n",
      "\n",
      "32047106   12 -r-x------   1 s112380  ldapuser     9402 Feb 13 10:08 ./__spark_conf__/log4j.properties\n",
      "\n",
      "32047130    4 -r-x------   1 s112380  ldapuser     2316 Feb 13 10:08 ./__spark_conf__/ssl-client.xml.example\n",
      "\n",
      "32047112    0 -r-x------   1 s112380  ldapuser        0 Feb 13 10:08 ./__spark_conf__/yarn.exclude\n",
      "\n",
      "32047131    4 -r-x------   1 s112380  ldapuser     1527 Feb 13 10:08 ./__spark_conf__/kms-env.sh\n",
      "\n",
      "32047127    8 -r-x------   1 s112380  ldapuser     4221 Feb 13 10:08 ./__spark_conf__/task-log4j.properties\n",
      "\n",
      "32047117    4 -r-x------   1 s112380  ldapuser     3518 Feb 13 10:08 ./__spark_conf__/kms-acls.xml\n",
      "\n",
      "32047126    4 -r-x------   1 s112380  ldapuser     2490 Feb 13 10:08 ./__spark_conf__/hadoop-metrics.properties\n",
      "\n",
      "32047109    4 -r-x------   1 s112380  ldapuser     3979 Feb 13 10:08 ./__spark_conf__/hadoop-env.cmd\n",
      "\n",
      "32047121    8 -r-x------   1 s112380  ldapuser     5434 Feb 13 10:08 ./__spark_conf__/yarn-env.sh\n",
      "\n",
      "32047124    4 -r-x------   1 s112380  ldapuser     1124 Feb 13 10:08 ./__spark_conf__/container-executor.cfg\n",
      "\n",
      "32047116    8 -r-x------   1 s112380  ldapuser     7209 Feb 13 10:08 ./__spark_conf__/capacity-scheduler.xml\n",
      "\n",
      "32047132    4 -r-x------   1 s112380  ldapuser     1308 Feb 13 10:08 ./__spark_conf__/hadoop-policy.xml\n",
      "\n",
      "32047118    4 -r-x------   1 s112380  ldapuser      758 Feb 13 10:08 ./__spark_conf__/mapred-site.xml.template\n",
      "\n",
      "32047120    4 -r-x------   1 s112380  ldapuser     1335 Feb 13 10:08 ./__spark_conf__/configuration.xsl\n",
      "\n",
      "32047115    4 -r-x------   1 s112380  ldapuser      890 Feb 13 10:08 ./__spark_conf__/ssl-client.xml\n",
      "\n",
      "32047135    4 -r-x------   1 s112380  ldapuser     1009 Feb 13 10:08 ./__spark_conf__/ssl-server.xml\n",
      "\n",
      "32047111   28 -r-x------   1 s112380  ldapuser    27866 Feb 13 10:08 ./__spark_conf__/core-site.xml\n",
      "\n",
      "32047105   12 -r-x------   1 s112380  ldapuser     8624 Feb 13 10:08 ./__spark_conf__/mapred-site.xml\n",
      "\n",
      "32047141    4 -r-x------   1 s112380  ldapuser      721 Feb 13 10:08 ./__spark_conf__/__spark_conf__.properties\n",
      "\n",
      "32047108    4 -r-x------   1 s112380  ldapuser     2131 Feb 13 10:08 ./__spark_conf__/hadoop-metrics2.properties\n",
      "\n",
      "32047140    8 -r-x------   1 s112380  ldapuser     5511 Feb 13 10:08 ./__spark_conf__/kms-site.xml\n",
      "\n",
      "32047122   12 -r-x------   1 s112380  ldapuser    10967 Feb 13 10:08 ./__spark_conf__/hdfs-site.xml\n",
      "\n",
      "153944080  336 -r-xr-xr-x   1 yarn     hadoop     339666 Oct 22 12:50 ./datanucleus-api-jdo-3.2.6.jar\n",
      "\n",
      "132923632 1772 -r-xr-xr-x   1 yarn     hadoop    1809447 Oct 22 12:50 ./datanucleus-rdbms-3.2.9.jar\n",
      "\n",
      "524602  732 -r-xr-xr-x   1 yarn     hadoop     745325 Oct 22 11:12 ./azure-storage-4.2.0.jar\n",
      "\n",
      "172081163 1452 -r-xr-xr-x   1 yarn     hadoop    1479023 Feb 13 10:05 ./poi-ooxml-3.17.jar\n",
      "\n",
      "75603973  224 -r-xr-xr-x   1 yarn     hadoop     223737 Feb 13 10:05 ./qd_rdq_2.10-1.0.1-RC14.jar\n",
      "\n",
      "173105153  152 -r-xr-xr-x   1 yarn     hadoop     148962 Feb 13 10:05 ./univocity-parsers-1.5.1.jar\n",
      "\n",
      "38305932 186228 -r-xr-xr-x   1 yarn     hadoop   190503288 Sep 16 11:36 ./__spark__.jar\n",
      "\n",
      "524596   12 -r-xr-xr-x   1 yarn     hadoop      10092 Oct 22 11:12 ./azure-keyvault-core-0.8.0.jar\n",
      "\n",
      "75538439 2672 -r-xr-xr-x   1 yarn     hadoop    2730866 Feb 13 10:05 ./xmlbeans-2.6.0.jar\n",
      "\n",
      "170975237  348 -r-xr-xr-x   1 yarn     hadoop     349304 Feb 13 10:05 ./gxppipelinecore_2.10-2.0.13.jar\n",
      "\n",
      "173252609 1948 -r-xr-xr-x   1 yarn     hadoop    1988051 Feb 13 10:05 ./ojdbc6.jar\n",
      "\n",
      "46146258  224 -r-x------   1 s112380  ldapuser   223737 Feb 13 10:08 ./__app__.jar\n",
      "\n",
      "11567115    4 -rw-------   1 s112380  hadoop       1024 Feb 13 10:08 ./container_tokens\n",
      "\n",
      "46146251  480 -r-xr-xr-x   1 yarn     hadoop     486892 Feb 13 10:05 ./lift-json_2.10-2.6.3.jar\n",
      "\n",
      "173260801   40 -r-xr-xr-x   1 yarn     hadoop      36888 Feb 13 10:05 ./commons-csv-1.1.jar\n",
      "\n",
      "131973229   16 -r-xr-xr-x   1 yarn     hadoop      16046 Oct 22 11:12 ./json-simple-1.1.jar\n",
      "\n",
      "131973220  216 -r-xr-xr-x   1 yarn     hadoop     213154 Oct 22 11:12 ./hadoop-azure-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "131973214  168 -r-xr-xr-x   1 yarn     hadoop     165879 Oct 22 11:12 ./hadoop-aws-2.7.3.2.5.5.0-157.jar\n",
      "\n",
      "173137921 2644 -r-xr-xr-x   1 yarn     hadoop    2701171 Feb 13 10:05 ./poi-3.17.jar\n",
      "\n",
      "153969074   56 -r-xr-xr-x   1 yarn     hadoop      52413 Oct 22 11:12 ./oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\n",
      "\n",
      "153944077    4 -r-xr-xr-x   1 yarn     hadoop       1920 Oct 22 12:50 ./hive-site.xml\n",
      "\n",
      "524614   40 -r-xr-xr-x   1 yarn     hadoop      38605 Oct 22 11:12 ./jackson-annotations-2.4.0.jar\n",
      "\n",
      "11567114   24 -rwx------   1 s112380  hadoop      21897 Feb 13 10:08 ./launch_container.sh\n",
      "\n",
      "153969083 6976 -r-xr-xr-x   1 yarn     hadoop    7130772 Oct 22 11:12 ./scala-library-2.10.5.jar\n",
      "\n",
      "46146159  168 -r-xr-xr-x   1 yarn     hadoop     165361 Feb 13 10:05 ./spark-csv_2.10-1.5.0.jar\n",
      "\n",
      "153969080  432 -r-xr-xr-x   1 yarn     hadoop     434678 Oct 22 11:12 ./commons-lang3-3.4.jar\n",
      "\n",
      "153944074 1852 -r-xr-xr-x   1 yarn     hadoop    1890075 Oct 22 12:50 ./datanucleus-core-3.2.10.jar\n",
      "\n",
      "75628564  740 -r-xr-xr-x   1 yarn     hadoop     751238 Feb 13 10:05 ./commons-collections4-4.1.jar\n",
      "\n",
      "46146026 5800 -r-xr-xr-x   1 yarn     hadoop    5924600 Feb 13 10:05 ./poi-ooxml-schemas-3.17.jar\n",
      "\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:directory.info\n",
      "\n",
      "\n",
      "\n",
      "LogType:launch_container.sh\n",
      "\n",
      "Log Upload Time:Thu Feb 13 10:09:37 +0000 2020\n",
      "\n",
      "LogLength:21897\n",
      "\n",
      "Log Contents:\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "\n",
      "\n",
      "export SPARK_YARN_STAGING_DIR=\".sparkStaging/application_1568810042014_190518\"\n",
      "\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-\"/usr/hdp/current/hadoop-client/conf\"}\n",
      "\n",
      "export JAVA_HOME=${JAVA_HOME:-\"/usr/java/latest\"}\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES=\"hdfs://aaprod/hdp/apps/2.5.5.0-157/spark/spark-hdp-assembly.jar#__spark__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/qd_rdq_2.10-1.0.1-RC14.jar#__app__.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-collections4-4.1.jar#commons-collections4-4.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/commons-csv-1.1.jar#commons-csv-1.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/gxppipelinecore_2.10-2.0.13.jar#gxppipelinecore_2.10-2.0.13.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/lift-json_2.10-2.6.3.jar#lift-json_2.10-2.6.3.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/ojdbc6.jar#ojdbc6.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-3.17.jar#poi-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-3.17.jar#poi-ooxml-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/poi-ooxml-schemas-3.17.jar#poi-ooxml-schemas-3.17.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/qd_rdq_2.10-1.0.1-RC14.jar#qd_rdq_2.10-1.0.1-RC14.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/scalaj-http_2.10-2.3.0.jar#scalaj-http_2.10-2.3.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/spark-csv_2.10-1.5.0.jar#spark-csv_2.10-1.5.0.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/univocity-parsers-1.5.1.jar#univocity-parsers-1.5.1.jar,hdfs://aaprod/mc_staging/rdq_uc7_files/trackwise_oldl_auditexport/00_00_01/oozie/xml/lib/xmlbeans-2.6.0.jar#xmlbeans-2.6.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar#spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/pyspark.zip#pyspark.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-rdbms-3.2.9.jar#datanucleus-rdbms-3.2.9.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/py4j-0.9-src.zip#py4j-0.9-src.zip,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-api-jdo-3.2.6.jar#datanucleus-api-jdo-3.2.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/datanucleus-core-3.2.10.jar#datanucleus-core-3.2.10.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar#oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/spark/hive-site.xml#hive-site.xml,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-core-1.10.6.jar#aws-java-sdk-core-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-kms-1.10.6.jar#aws-java-sdk-kms-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/aws-java-sdk-s3-1.10.6.jar#aws-java-sdk-s3-1.10.6.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-keyvault-core-0.8.0.jar#azure-keyvault-core-0.8.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/azure-storage-4.2.0.jar#azure-storage-4.2.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/commons-lang3-3.4.jar#commons-lang3-3.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/guava-11.0.2.jar#guava-11.0.2.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-aws-2.7.3.2.5.5.0-157.jar#hadoop-aws-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/hadoop-azure-2.7.3.2.5.5.0-157.jar#hadoop-azure-2.7.3.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-annotations-2.4.0.jar#jackson-annotations-2.4.0.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-core-2.4.4.jar#jackson-core-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/jackson-databind-2.4.4.jar#jackson-databind-2.4.4.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/joda-time-2.5.jar#joda-time-2.5.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/json-simple-1.1.jar#json-simple-1.1.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar#oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar#oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar,hdfs://aaprod/user/oozie/share/lib/lib_20180813101500/oozie/scala-library-2.10.5.jar#scala-library-2.10.5.jar\"\n",
      "\n",
      "export SPARK_LOG_URL_STDOUT=\"https://awdex01016.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000002/s112380/stdout?start=-4096\"\n",
      "\n",
      "export NM_HOST=\"awdex01016.aws.merckcloud.com\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_FILE_SIZES=\"190503288,223737,751238,2730866,1479023,2701171,5924600,162717,486892,349304,751238,36888,349304,486892,1988051,2701171,1479023,5924600,223737,162717,165361,148962,2730866,190578782,357604,1809447,44846,339666,1890075,22715,1920,516062,258578,570101,10092,745325,434678,1648200,165879,213154,38605,225302,1076926,588001,16046,12749,52413,7130772\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS=\"1581588410189\"\n",
      "\n",
      "export LOGNAME=\"s112380\"\n",
      "\n",
      "export JVM_PID=\"$$\"\n",
      "\n",
      "export PWD=\"/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002\"\n",
      "\n",
      "export LOCAL_DIRS=\"/data2/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518,/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518\"\n",
      "\n",
      "export NM_HTTP_PORT=\"8044\"\n",
      "\n",
      "export LOG_DIRS=\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002,/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002,/data1/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002\"\n",
      "\n",
      "export NM_AUX_SERVICE_mapreduce_shuffle=\"AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n",
      "\n",
      "\"\n",
      "\n",
      "export NM_PORT=\"45454\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_TIME_STAMPS=\"1536921442593,1581588408931,1581588409037,1581588409199,1581588409299,1581588409426,1581588409654,1581588409709,1581588409779,1581588409838,1551865168471,1551865168521,1551865168577,1551865168637,1551865168745,1551865168863,1551865168953,1551865169151,1552982988554,1551865169204,1551865169256,1551865169314,1551865169434,1534148701957,1534148696849,1534148696458,1534148696706,1534148696265,1534148696362,1534148696604,1534148696504,1534148683990,1534148684047,1534148684121,1534148684206,1534148684282,1534148684346,1534148684447,1534148684511,1534148684575,1534148684624,1534148684682,1534148684764,1534148684835,1534148684923,1534148685010,1534148685067,1534148774104\"\n",
      "\n",
      "export USER=\"s112380\"\n",
      "\n",
      "export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\"/usr/hdp/current/hadoop-yarn-nodemanager\"}\n",
      "\n",
      "export CLASSPATH=\"$PWD/*:$PWD:$PWD/__spark_conf__:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.5.0-157/hadoop/lib/hadoop-lzo-0.6.0.2.5.5.0-157.jar:/etc/hadoop/conf/secure\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES=\"hdfs://aaprod/user/s112380/.sparkStaging/application_1568810042014_190518/__spark_conf__2027599449732736376.zip#__spark_conf__\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES=\"155669\"\n",
      "\n",
      "export SPARK_YARN_MODE=\"true\"\n",
      "\n",
      "export SPARK_YARN_CACHE_FILES_VISIBILITIES=\"PUBLIC,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PRIVATE,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC,PUBLIC\"\n",
      "\n",
      "export HADOOP_TOKEN_FILE_LOCATION=\"/data1/hadoop/yarn/local/usercache/s112380/appcache/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/container_tokens\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark_shuffle=\"\"\n",
      "\n",
      "export SPARK_USER=\"s112380\"\n",
      "\n",
      "export LOCAL_USER_DIRS=\"/data2/hadoop/yarn/local/usercache/s112380/,/data/hadoop/yarn/local/usercache/s112380/,/data1/hadoop/yarn/local/usercache/s112380/\"\n",
      "\n",
      "export SPARK_LOG_URL_STDERR=\"https://awdex01016.aws.merckcloud.com:8044/node/containerlogs/container_e175_1568810042014_190518_02_000002/s112380/stderr?start=-4096\"\n",
      "\n",
      "export SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES=\"PRIVATE\"\n",
      "\n",
      "export HOME=\"/home/\"\n",
      "\n",
      "export NM_AUX_SERVICE_spark2_shuffle=\"\"\n",
      "\n",
      "export CONTAINER_ID=\"container_e175_1568810042014_190518_02_000002\"\n",
      "\n",
      "export MALLOC_ARENA_MAX=\"4\"\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33274/oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\" \"oozie-hadoop-utils-hadoop-2-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33267/aws-java-sdk-core-1.10.6.jar\" \"aws-java-sdk-core-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33262/aws-java-sdk-s3-1.10.6.jar\" \"aws-java-sdk-s3-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33270/joda-time-2.5.jar\" \"joda-time-2.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33280/json-simple-1.1.jar\" \"json-simple-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33287/hive-site.xml\" \"hive-site.xml\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40069/commons-collections4-4.1.jar\" \"commons-collections4-4.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33269/oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-oozie-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40067/poi-3.17.jar\" \"poi-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33271/azure-storage-4.2.0.jar\" \"azure-storage-4.2.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33279/scala-library-2.10.5.jar\" \"scala-library-2.10.5.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33285/datanucleus-rdbms-3.2.9.jar\" \"datanucleus-rdbms-3.2.9.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40059/scalaj-http_2.10-2.3.0.jar\" \"scalaj-http_2.10-2.3.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33273/aws-java-sdk-kms-1.10.6.jar\" \"aws-java-sdk-kms-1.10.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/31211/spark-hdp-assembly.jar\" \"__spark__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33275/commons-lang3-3.4.jar\" \"commons-lang3-3.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33289/datanucleus-api-jdo-3.2.6.jar\" \"datanucleus-api-jdo-3.2.6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33282/py4j-0.9-src.zip\" \"py4j-0.9-src.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40066/qd_rdq_2.10-1.0.1-RC14.jar\" \"qd_rdq_2.10-1.0.1-RC14.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40072/lift-json_2.10-2.6.3.jar\" \"lift-json_2.10-2.6.3.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/usercache/s112380/filecache/94/__spark_conf__2027599449732736376.zip\" \"__spark_conf__\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33288/pyspark.zip\" \"pyspark.zip\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33286/datanucleus-core-3.2.10.jar\" \"datanucleus-core-3.2.10.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/40063/xmlbeans-2.6.0.jar\" \"xmlbeans-2.6.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33264/azure-keyvault-core-0.8.0.jar\" \"azure-keyvault-core-0.8.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33277/jackson-annotations-2.4.0.jar\" \"jackson-annotations-2.4.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33265/jackson-databind-2.4.4.jar\" \"jackson-databind-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40068/commons-csv-1.1.jar\" \"commons-csv-1.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40071/spark-csv_2.10-1.5.0.jar\" \"spark-csv_2.10-1.5.0.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33284/spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\" \"spark-assembly-1.6.3.2.5.5.0-157-hadoop2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/33268/guava-11.0.2.jar\" \"guava-11.0.2.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33283/oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\" \"oozie-sharelib-spark-4.2.0.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33263/hadoop-aws-2.7.3.2.5.5.0-157.jar\" \"hadoop-aws-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40061/gxppipelinecore_2.10-2.0.13.jar\" \"gxppipelinecore_2.10-2.0.13.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/33278/jackson-core-2.4.4.jar\" \"jackson-core-2.4.4.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40070/ojdbc6.jar\" \"ojdbc6.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/usercache/s112380/filecache/95/qd_rdq_2.10-1.0.1-RC14.jar\" \"__app__.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40065/poi-ooxml-3.17.jar\" \"poi-ooxml-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data2/hadoop/yarn/local/filecache/33266/hadoop-azure-2.7.3.2.5.5.0-157.jar\" \"hadoop-azure-2.7.3.2.5.5.0-157.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data/hadoop/yarn/local/filecache/40062/poi-ooxml-schemas-3.17.jar\" \"poi-ooxml-schemas-3.17.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "ln -sf \"/data1/hadoop/yarn/local/filecache/40064/univocity-parsers-1.5.1.jar\" \"univocity-parsers-1.5.1.jar\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "# Creating copy of launch script\n",
      "\n",
      "cp \"launch_container.sh\" \"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/launch_container.sh\"\n",
      "\n",
      "chmod 640 \"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/launch_container.sh\"\n",
      "\n",
      "# Determining directory contents\n",
      "\n",
      "echo \"ls -l:\" 1>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "ls -l 1>>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "echo \"find -L . -maxdepth 5 -ls:\" 1>>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -ls 1>>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "echo \"broken symlinks(find -L . -maxdepth 5 -type l -ls):\" 1>>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "find -L . -maxdepth 5 -type l -ls 1>>\"/data2/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/directory.info\"\n",
      "\n",
      "exec /bin/bash -c \"$JAVA_HOME/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms1024m -Xmx1024m '-Dlog4j.configuration=spark-log4j.properties' -Djava.io.tmpdir=$PWD/tmp '-Dspark.ui.port=0' '-Dspark.driver.port=33042' -Dspark.yarn.app.container.log.dir=/data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.31.97.178:33042 --executor-id 1 --hostname awdex01016.aws.merckcloud.com --cores 1 --app-id application_1568810042014_190518 --user-class-path file:$PWD/__app__.jar --user-class-path file:$PWD/commons-collections4-4.1.jar --user-class-path file:$PWD/xmlbeans-2.6.0.jar --user-class-path file:$PWD/poi-ooxml-3.17.jar --user-class-path file:$PWD/poi-3.17.jar --user-class-path file:$PWD/poi-ooxml-schemas-3.17.jar --user-class-path file:$PWD/scalaj-http_2.10-2.3.0.jar --user-class-path file:$PWD/lift-json_2.10-2.6.3.jar --user-class-path file:$PWD/gxppipelinecore_2.10-2.0.13.jar 1> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/stdout 2> /data/hadoop/yarn/log/application_1568810042014_190518/container_e175_1568810042014_190518_02_000002/stderr\"\n",
      "\n",
      "hadoop_shell_errorcode=$?\n",
      "\n",
      "if [ $hadoop_shell_errorcode -ne 0 ]\n",
      "\n",
      "then\n",
      "\n",
      "  exit $hadoop_shell_errorcode\n",
      "\n",
      "fi\n",
      "\n",
      "\n",
      "\n",
      "End of LogType:launch_container.sh\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='11/12/1998'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.search(r'^(\\d+/\\d+/\\d+)','11/12/1998 The date is ')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-09-21\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "s = \"Jason's birthday is on 1991-09-21 \"\n",
    "match = re.search(r'\\d{4}-\\d{2}-\\d{2}', s)\n",
    "date = datetime.datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1991-09-21'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Jason's birthday is on 1991-09-21 \"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\s+(?=\\d{2}(?:\\d{2})?-\\d{1,2}-\\d{1,2}\\b)', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-28-a09575f1a2d2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-a09575f1a2d2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    s.startswith(r'\\s+(?=\\d{2}(?:\\d{2})?-\\d{1,2}-\\d{1,2}\\b))\u001b[0m\n\u001b[1;37m                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "s.startswith(r'\\s+(?=\\d{2}(?:\\d{2})?-\\d{1,2}-\\d{1,2}\\b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
